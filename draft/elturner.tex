%\documentclass[journal]{IEEEtran}
%\documentclass[11pt,draftcls,onecolumn]{IEEEtran}
%\documentclass[10pt,twocolumn,twoside]{report}
\documentclass[12pt,onecolumn,oneside]{book}
\usepackage[margin=1.0in]{geometry}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% *** HEADER/PAGE NUMBERING RELATED PACKAGES ***
\usepackage{blindtext}% adds the blindtext
%\usepackage{showframe}% shows a framed header, type area and footer
\setlength{\headheight}{1.1\baselineskip}
%\setlength{\headsep}{10pt}

% *** GRAPHICS RELATED PACKAGES ***
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{../figures/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpg,.jpeg,.png}

% *** MATH PACKAGES ***
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\interdisplaylinepenalty=2500

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=blue
}

% Force headers and page numbers to always be on the upper right
\usepackage[nouppercase,automark]{scrpage2}
\clearscrheadfoot
\pagestyle{scrheadings}
%See the KOMA-script documentation for more information
\ihead{\rightmark}
%\ihead[]{\headmark}%helpful for two-sided printing
\ofoot[\pagemark]{\pagemark}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% begin document with roman page numbering
\begin{document}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% TITLE
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
\pagenumbering{gobble}

% the title and author page has very special requirements
%
% see:  http://grad.berkeley.edu/wp-content/uploads/title-page.pdf
%
% For all formatting requirements, see:
%
% http://grad.berkeley.edu/academic-progress/dissertation/#formatting-your-manuscript

\newcommand{\mytitle}{3D Modeling of Interior Building Environments and Objects from Noisy Sensor Suites}
\title{\mytitle}

\newcommand{\myauthor}{Eric~Lee~Turner}
\author{\myauthor}

{\centering
	{\textbf{\mytitle}}\\
	\hfill \break
	\hfill \break
	By\\
	\hfill \break
	\myauthor\\
	\hfill \break
	\hfill \break
	\hfill \break
	A dissertation submitted in partial satisfaction of the\\	
	\hfill \break
	requirements for the degree of\\
	\hfill \break
	Doctor of Philosophy\\
	\hfill \break
	in\\
	Engineering -- Electrical Engineering and Computer Sciences\\	
	\hfill \break 
	in the\\
	\hfill \break
	Graduate Division\\
	\hfill \break
	of the\\
	\hfill \break
	University of California, Berkeley\\
	\hfill \break
	\hfill \break
	\hfill \break
	Committee in charge:\\
	\hfill \break
	Professor Avideh Zakhor, Chair\\
	Professor Jonathan Shewchuk\\
	Professor Kyle Steinfeld\\
	\hfill \break	
	\hfill \break
	Spring 2015\\
}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% COPYRIGHT
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\newpage
{\centering
	{\textbf{\mytitle}}\\
	\hfill \break
	\hfill \break
	\hfill \break
	\hfill \break
	\hfill \break
	Copyright \copyright \, 2015\\
	by\\
	\myauthor\\
}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% ABSTRACT
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the abstract uses arabic page numbering and has its own counter
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

% header for abstract
{\centering
	{\textbf{Abstract}}\\
	\hfill \break
	\mytitle\\
	\hfill \break
	by\\
	\hfill \break
	\myauthor\\
	\hfill \break
	Doctor of Philosophy in Electrical Engineering and Computer Sciences\\
	\hfill \break
	University of California, Berkeley\\
	\hfill \break
	Professor Avideh Zakhor, Chair\\
	\hfill \break
}

%\begin{abstract}
In this dissertation, we present several techniques used to automatically generate virtual models of indoor building environments.  The interior environment of a building is scanned using our custom hardware system, which provides a set of data products used to develop these models.  Our modeling technqiues can be separated into two categories:  2D floor plan models and dense 3D models.  All approaches are produced automatically from the output data of our backpack-mounted ambulatory scanning system, which can scan multiple floors of a building efficiently.  We are capable of capturing the entirety of large buildings in only a few hours.  This system is capable of producing many kinds of 3D virtual building models while traversing through a GPS-denied environment, including point clouds, 2D floor plans, and 3D building models.

The novel contributions of this dissertation can be sorted into three groups.  First, we present multiple methods for producing 2D floor plans across all of the building environment.  We automatically partition these floor plans into separate building levels and separate rooms within each level.  These floor plans can also be extruded into simplified 3D models of the building environment.  Second, we present multiple technqiues to produce complex 3D models of the scanned environment, capturing all observed detail.  Third, we present several techniques that combine these two types of building models -- simple and complex -- in order to perform additional analysis of the building environment.  Such analysis includes segmenting objects and furniture of the environment as separate models, reducing noise and artifacts within the models, and demonstrating novel visualization techniques.  Additionally, we show several results of the system in real-world environments, including buildings of over 40,000 square feet.  We demonstrate how such building models are applicable to many fields of study, including architecture, building energy efficiency, virtual walk-throughs of buildings, indoor navigation, augmented and virtual reality.
%\end{abstract}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% FRONT MATTER:  dedication, table of contents, list of figures, acknowledge
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{roman}
\setcounter{page}{1}

% dedication page
\clearpage
\vspace*{\fill}
\begin{center}
To Mom.
\end{center}
\vfill % equivalent to \vspace{\fill}
\clearpage

% table of contents
\addcontentsline{toc}{chapter}{Table of Contents}
\tableofcontents{}

% list of figures
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures{}

% list of tables
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listtablename}
\listoftables{}

% acknowledgement section
\newpage
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Acknowledgements}
\chapter*{Acknowledgements}

% berkeley is an awesome place
As soon as I saw University of California, Berkeley, I knew I would love it here.  A campus cut with forrested paths and narrow streams seemed straight out of a Calvin \& Hobbes strip.  As soon as I met the people here, I immediately found that the intellectual adventure matched the aesthetics.

% Avideh and thesis committee
I first want to thank my advisor, Professor Avideh Zakhor.  The potential she sees in her students becomes manifest with each challenge she places before them.  My understanding of the field and of academics in general has become so much richer thanks to her guidance.  I also would like to thank Jonathan Shewchuk, Kyle Steinfeld, and Carlo Sequin, who served on my qualifying exam and dissertation committees.  These are some of the best professors I've ever met, and have provided me with excellent guidance both in the classroom and in my committee.

% funding
This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

% lab folks
I also want to extend my deep gratitude to all my colleagues in the Video and Image Processing Lab.  John Kua, Michael Krishnan, Rick Garcia, Shangliang Jiang, Nicholas Corso, Shicong Yang, Richard Zhang, and Joe Menke have all lent me guidance and helped me through.  I especially want to mention Nicholas, who ensured that I took a daily constitutional around campus rather than hiding away in our lab.

% family and sam
Additionally, I want to thank my family.  Even though I moved to the other side of the country, they have always been supportive.  My Dad, David Turner, has not only been my role model of an engineer, but truly comforting at every step of my way through school.  Lastly, I want to thank Samantha Shropshire, my girlfriend.  For putting up with my working at odd hours, for supporting me, and for bouncing ideas off of, her patience knows no bounds.  Sam, without you, I would never have survived graduate school.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Introduction
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{Introduction}
\label{ch:introduction}

% motivation text
Laser scanning technology is becoming a vital component of building construction and maintenance.  During building construction, laser scanning can be used to record the as-built locations of HVAC and plumbing systems before drywall is installed.  In existing buildings, blueprints are often outdated or missing, especially after several remodelings.  Such scans can be used to generate building models describing the current architecture.  Meshed triangulations allow for the efficient representation of the scanned geometry.  In addition to being useful in the fields of architecture, civil engineering, and construction, these models can be directly applied to virtual walk-throughs of environments, gaming entertainment, augmented reality, indoor navigation, and energy simulation analysis.  These applications rely on the accuracy of a model as well as its compact representation.

% figure with pointcloud and models
\begin{figure}

	% top row
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_photo}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_triangles}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_texture_compressed}}
		\centerline{(c)}\medskip
	\end{minipage}
	
	% bottom row
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_pointcloud_compressed}}
		\centerline{(d)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_triangles}}
		\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_texture_compressed}}
		\centerline{(f)}\medskip
	\end{minipage}
	
	\caption[Models generated with our techniques.]{Models generated with the techniques described in this dissertation:  (a) photograph of scanned area, academic building; (b) surface carving model of this area; (c) surface carving model with texturing; (d) point cloud of captured scans; (e) extruded floor plan model of area; (f) extruded floor plan with texturing.}
	\label{fig:coryf2}

\end{figure}

% applications
Generating an accurate model of indoor environments is an emerging challenge in the fields of architecture and construction for the purposes of verifying as-built compliance to engineering designs~\cite{Bosche10,Xiong13}.  This task is made more challenging by the GPS-deprived nature of these environments~\cite{Liang13}.  Another application that requires an exported mesh to retain as much detail as possible is historical preservation via a virtual reality model~\cite{VillageHeritage,Carving}.  Building energy efficiency simulations can use watertight meshes of the environment to estimate airflow and heat distribution~\cite{EnergyPlus}.  These simulations require simplified meshes as input, since finite element models are difficult to scale.  It is also important to be able to generate an immersive visualization and walk-through of the environment for these applications, so experts can remotely inspect the scanned environment via telepresence, a task that currently requires expensive travel and on-site visits.  Different applications require models of different complexities, both with and without furniture geometry.  The modeling approaches detailed in this report are useful for both types of applications, as shown in the examples in Figure~\ref{fig:coryf2}.  Figure~\ref{fig:coryf2}a is a photograph of the scanned area: the hallways of an academic building, encompassing about 1,000 square meters of scanned area.  Figure~\ref{fig:coryf2}d represents the captured 3D point-cloud of this area.  Figures~\ref{fig:coryf2}b and~\ref{fig:coryf2}c show a high-detail 3D mesh of 2.7 million triangles generated using the algorithm in Chapter~\ref{ch:carving}, with and without texturing, respectively.  Figures~\ref{fig:coryf2}e and~\ref{fig:coryf2}f show a low detail model of 2,644 triangles generated using the approach in Chapter~\ref{ch:floorplan}, with and without texturing~\cite{Turner14Journal}.

% brief overview of different modeling techniques presented
One of the primary challenges of indoor modeling is the sheer size of the input point-clouds.  Scans of single floors of buildings result in point-clouds that contain hundreds of millions of points, often larger than the physical memory in a personal computer.  Man-made geometry is typically composed of planar regions and sharp corners, but many conventional surface reconstruction schemes assume a certain degree of smoothness and result in rounded or blobby output if applied to these models~\cite{Powercrust,OctreeSculpting,Carving,ProgressiveMesh,Poisson,EigencrustShewchuk}.  In addition to large flat regions, building interiors also contain many small details, such as furniture.  A surface reconstruction scheme must be able to represent the large surfaces in a building with an efficient number of elements and preserve their sharp features.  The fine details of furniture are useful for some applications whereas others require furniture to be removed.  In this report, we discuss existing modeling techniques that remove fine details and those that preserve these details.  The output of both of these modeling techniques can be texture-mapped with captured camera imagery~\cite{Cheng14}.  An example of texture-mapping these two modeling processes is shown in Figs.~\ref{fig:coryf2}c and~\ref{fig:coryf2}f.

\section{Surface Reconstruction}
\label{sec:surf_recon_background}

% background on surface reconstruction techniques
This section describes common techniques for surface reconstruction from input point-cloud data.  These techniques are useful for generating meshes of objects scanned with table-top scanning systems.

% powercrust and the need for noise-reduction
Powercrust is a volumetric surface reconstruction approach that yields watertight models from point-cloud data~\cite{Powercrust}.  It computes the interior median axis of a shape using the voronoi diagram of the input scan points.  By taking the union of the inner polar balls, the elements of the Delaunay Triangulation that form the interior of the shape can be found, and are used to approximate the modeled volume.  While this approach is popular, one weakness is that it is sensitive to noise in the input scans.  Fortunately, other techniques have improved on this methodology to account for noisy input

% Accounting for noise in volumetric scans
An approach to account for the noise in the Powercrust algorithm is to remove sufficiently small polar balls from consideration, which are likely to be due to input noise~\cite{NoisyPowercrust}.  Another approach is to perform spectral clustering on the tetrahedralization of the points~\cite{EigencrustShewchuk}.  This approach, dubbed Eigencrust, allows for substantially improved robustness to noise.  An important consideration, though, is that the noise models tested are randomly positioned outlier points and randomly perturbed input points.  Both of these approaches assume that the randomness is independent and identically distributed from point-to-point.  Such characteristics are not the dominant case when dealing with noise from mis-registration of scans, which is the most common source of noise from mobile scanning systems.

% Isosurface and the reign of SDFs
Another technique used to produce watertight models is to define implicit functions on the scanned volume, and use an isosurface of this volume as the exported mesh.  Once an implicit function is defined, techniques such as Marching Cubes or Dual Contours can be used to mesh the surface~\cite{MarchingCubes,DualContouring}. A popular method for defining this implicit function is to use Signed-Distance Fields~\cite{SignedDistanceFields}.  Another popular approach is Poisson Surface Reconstruction~\cite{Poisson}.  Both of these methods are more robust at mis-registration errors than the previous algorithms.  One downside of their use in the area of modeling man-made objects is that they yield smooth, continuous surfaces.  Such results tend to look overly organic when modeling objects with flat regions or sharp corners.  This issue can be alleviated somewhat by using dual contour isosurface meshing rather than marching cubes, since it preserves sharp edges in the scalar field, but with these methods, the scalar field itself can be overly-smoothed.

% typical hardware set-up for traditional surface reconstruction
An important note is that traditional surface reconstruction techniques, as described above, are written for and tested with common point-cloud test-sets, such as the Stanford Bunny or Dragon.  These models are scanned with precision 3D scanners that yield relatively low noise and minimal mis-registration when compared to scans of larger areas such as buildings.  These approaches are also designed to model isolated objects.  The goal of my thesis is to model buildings, which are scanned at a much larger scale and produce much higher rates of noise and mis-registration.  While watertight volumetric processing is still useful for our applications, different techniques must be applied compared to isolated objects.

\section{Outdoor Building Scanning}
\label{sec:outdoor_scanning}

% motivation for discussing outdoor stuff
When discussing 3D reconstruction of building environments, it is important to note the subfields of both indoor and outdoor building modeling.  Most of this dissertation is specific to indoor building modeling, however many related techniques are used for outdoor modeling as well.

% how to scan outdoors
The current method of outdoor building scanning requires an acquisition vehicle to drive down a street while taking Light Detection And Ranging (LiDAR) scans and panoramic photographs of the surround area~\cite{Zakhor07,Chen07}.  In many applications, it is desirable to generate a triangulated geometry for a collected point-cloud.  This geometry needs to be accurate to the original architecture.  

% pointcloud of building facade
\begin{figure}
	\begin{minipage}[b]{1.0\linewidth}
	  \centering
	  \centerline{\includegraphics[width=0.7\linewidth]{icip2012/rush_pointcloud00}}
	\end{minipage}

	\caption[An example point-cloud of a fa\c{c}ade.]{An example point-cloud of a fa\c{c}ade. Note gaps near windows, ledges, and columns.}
	\label{fig:rush_points}
\end{figure}

For buildings of an appreciable height, the street-level LiDAR scans are taken at a very acute angle with respect to the surface of the building.  Any protrusions from the building cause shadows in the LiDAR and therefore the building is not collected in its entirety, as shown in Figure~\ref{fig:truck_lidar}.  For many applications, a typical solution would be to gather scans from multiple angles \cite{Tang10, Curless96}.  For the urban reconstruction problem, this tactic cannot be used since the scanner is restricted to street-level.

\begin{figure}[t]

% figure:  how to scan outdoors
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[height=4.5cm]{icip2012/truck_lidar}}
\end{minipage}

\caption[Scanning building fa\c{c}ades from a vehicle.]{Locations on the building A, B, C are occluded from the LiDAR collector on the vehicle.}
\label{fig:truck_lidar}

\end{figure}

% background on outdoor modeling
Previous attempts to model buildings have assumed that architecture takes a simple polyhedron geometry. This approach ignores minor details of a building fa\c{c}ade, such as windows and ledges when applied on a large scale \cite{Chauve10, Chen07}.  The goal of this paper is to preserve as much detail as possible in the reconstructed model geometry.  Any interpolation must preserve the sharp edges of corners that occur within these holes.  This requirement diverges from the typical assumptions of most surface completion methods, which usually result in smoothed surfaces \cite{Kazhdan06, Kawai11}.  While developing 3D models with sharp features is a well-explored topic, most current approaches require a sufficient density of points near regions of high curvature, which would not be applicable here \cite{Bernardini04, Mhatre06}.  

% our method
Our goal is to generate sharp, axis-aligned, planar features in locations where the point-cloud is sparse or not sampled at all.  We propose an approach for rectilinear surface reconstruction of building fa\c{c}ades with incomplete 3D point-clouds~\cite{Turner12outdoor}.  Specifically, our proposed algorithm generates geometry for any holes in the point-cloud while preserving flat planes and sharp corners, which are common in modern architecture.  In order to reconstruct a mesh of a building face, first we determine the dominant plane of this fa\c{c}ade, then by treating the original points as a height map on this plane, we uniformly resample these heights over the entire surface using Moving Least-Squares (MLS) smoothing in order to mitigate noise in areas of high sampling, interpolate the areas corresponding to gaps in the point-cloud, and guarantee uniformity of resulting geometry~\cite{Nealen04}.

% Some results
\begin{figure}

\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[width=0.8\linewidth]{icip2012/set2-curve-points00}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[width=0.8\linewidth]{icip2012/set2-curve-screenshot}}
  \centerline{(b)}\medskip
\end{minipage}
%
\caption[Surface reconstruction of building fa\c{c}ade.]{The curvature of the building is preserved when extrapolated; (a) building point-cloud; (b) MLS triangulation.}
\label{fig:extrapolate}
%
\end{figure}

This method results in flat strips of extrapolated fa\c{c}ade.  If the building curves outward or has other non-planar characteristics, these strips conform to the shape of the building, as shown in Figure~\ref{fig:extrapolate}.  Once a geometry has been processed, a texture is applied using panoramic photographs collected from the same location as the LiDAR.  Since these images are projected onto the geometry, any irregularities in the geometry result in dramatic disturbances in texturing.  Examples of this texturing process are shown in Figure~\ref{fig:icip2012_texture}.

\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face4_mls}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face4_texture}}
  \centerline{(b)}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face7_mls}}
  \centerline{(c)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face7_texture}}
  \centerline{(d)}\medskip
\end{minipage}
\caption[Meshes of building fa\c{c}ades with texture.]{Results of texturing geometry after sharp hole-filling; (a) sampling of flat fa\c{c}ade; (b) with texture; (c) sampling of uneven fa\c{c}ade; (d) with texture.}
\label{fig:icip2012_texture}
\end{figure}

While the techniques used for surface reconstruction of exterior building models are different than those used for interior models, both methods require the efficient processing of large amounts of scan data in the form of point clouds.  Such point clouds may represent noisy or inconsistent geometry, due to being acquired via a mobile scanning system.

\section{Indoor Building Scanning Systems}
\label{sec:indoor_scanning}

% building information models
In the architecture community, there is a continuing push for use of a consolidated Building Information Model file format~\cite{AutodeskBIM}.  Such models need to be semantically-rich for all phases of building development, including architectural, structural, mechanical, electrical, plumbing, etc.  Traditionally, each separate aspect needed to be remodeled from scratch, by hand, which introduced errors into the modeling process.

It is important to distinguish between models that are as-designed compared to as-built.  Each phase of building development will produce a design, but it is still important to compare this design to the as-is nature of the building.  3D scanning of building environments is an important technology to be able to compare the result of construction to what was desired.  With current technology, there is still a fair amount of manual intervention that is  required to convert a 3D scan of a building into a suitable BIM file that can be used for such a comparison.

% competing systems
Traditional industry-standard building scanning uses static scanners.  Such scanners are mounted on tripods, and moved from area to area in the building~\cite{RoomSegmentation,HistWallRecon,BasicPlaneFit}. This scanning process is labor intensive and slow, but results in highly accurate point clouds after stitching.  In order to automate indoor scanning, many mobile systems have been introduced. Due to cost of full 3D laser range finders, the majority of indoor modeling systems use 2D LiDAR scanners.  Wheeled platforms that carry scanning equipment and are manually pushed through the environment are popular~\cite{Carving, ProbabilisticRobotics}.  Mobility of such systems is limited, since they are unable to traverse rough terrain or stairs easily.  Others have investigated mounting laser range finders on unmanned aerial vehicles~\cite{Quadrotor,QuadrotorMIT,SpectralClustering}.  Such platforms are agile in that they can scan difficult-to-reach areas.  Such unmanned platforms are limited by short battery life and cannot scan for long durations.

% include system hardware description
% our system and WHAT WE DO DIFFERENT AND WHY IT IS BETTER!!!!!!!!!
In this dissertation, we focus on ambulatory scanning platforms, where the sensor suite is carried by a human operator as the operator moves through the building environment~\cite{Sweep,MITBackpack,VillageHeritage}.  These systems allow for rapid data acquisition and can be actively scanning for several hours at a time.  They use 2D LiDAR scanners due to the cost and weight of full 3D laser range finders.  The captured scans are used both to reconstruct the geometry of the environment and to localize the system in the environment over time.  The datasets shown in this paper were generated by a backpack-mounted system that uses 2D LiDAR scanners to estimate the 3D path of the system over time as well as multiple scanners to generate geometry for the environment~\cite{liu2010indoor,Backpack,Localization,NickJournal}.  This system also has multiple cameras collecting imagery during the data acquisition process, which allows for scanned points to be colored or for generated meshes to be textured with realistic imagery~\cite{Cheng14}. 

% This section describes building meshing techniques
\section{3D Modeling of Building Environments}
\label{sec:building_meshing}

% give a brief overview of the different types of meshing techniques
% Discuss how the below techniques typically use noise-less input from
% static scanners, and do not explicitly detail with misregistration or
% noise.
The primary thesis of this dissertation regards surface reconstruction of scan data of indoor building environments, regardless of what hardware systems are used to collect building scan data.  There exists an emerging field of techniques used to model different aspects of building geometry from captured scans.  These techniques can be classified into three main categories:  Floor-Plan Generation, Simplified 3D Modeling, and Detailed 3D Modeling.  Floor-plan generation focuses on estimating 2D positions of walls in the building.  Simplified 3D modeling similarly focuses on 3D modeling only the permanent features of a building: floors, walls, and ceilings.  Detailed 3D modeling focuses on modeling all aspects of the scanned geometry, including fine details such as furniture or objects observed in the building.  Note that most of the approaches discussed here were developed to be applied to static scans of buildings, which have very low noise and capture high detail. The focus of this dissertation is to develop modeling techniques for mobile scanning systems, which are much more likely to suffer from mis-registration noise or missing geometry.

In this section, we discuss existing techniques to generate each of these types of building models.  This dissertation also describes novel work we contributed, in Chapters~\ref{ch:floorplan},~\ref{ch:carving},~\ref{ch:better_floorplan}, and~\ref{ch:better_carving}.

% This subsubsection describes floor plan generation techniques
\subsection{Floor-Plan Modeling}
\label{ssec:background_floorplan}

Interior scanning has traditionally been used for robotic navigation in indoor environments.  The rough locations of walls are necessary to avoid collisions.  Generating floor plans for modeling, however, requires a much higher degree of precision \cite{Okorn09}.  Given these antecedent studies, techniques have been developed using a single horizontal scanner collecting about the yaw direction \cite{Weiss05}.  Constructing full 3D scans allows for more sophisticated means of identifying walls from other obstacles in a building.  In this scenario, one can compute a top-down 2D histogram of point densities across the x-y plane \cite{Okorn09}.  Areas with high density are considered likely to be wall locations.  While clutter is mitigated by being less represented in the histogram than walls, no direct measures are taken to remove outlier samples before line-fitting.  Further, each story of a building must be processed separately.

Previous approaches to floor plan modeling typically assume walls are well-fitted by straight line segments and whether these fitted models are watertight is not guaranteed \cite{Nuchter03, Okorn09, Weiss05}.  Many architectural designs incorporate curved features, which would not be modeled accurately by these approaches. The absence of any water-tightness guarantee requires extensive post-processing to be devoted to removing disconnected and outlier segments that are interpreted as noise.

Floor-plan modeling techniques are based on the idea of sampling positional information of walls within the environment captured by the scans, then using these wall samples to generate a plan composed of line segments or polygons.  The work of Weiss et al use a cart-based system with a horizontal laser scanner~\cite{Weiss05}.  The output scan points are exactly the sample positions of the walls.  The find lines in this scan map with a Hough Transform, which represent walls.  Okorn et al employ a similar method, but their input scans represent a full 3D point-cloud~\cite{Okorn09}.  The wall sample positions are found by computing a top-down histogram of the input points, and areas of high density are classified as vertical surfaces.  The approaches we discuss in Chapter~\ref{ch:floorplan} employ a similar top-down histogram.  Lastly, Mura et al's paper takes a different approach to estimating wall positions~\cite{Mura13}.  They perform region growing in the 3D point-cloud to fine planar regions, which are projected into 2D and treated as potential wall candidates.  A cell complex is then built to volumetrically identify separate rooms in the model.  This approach is the second publication that performs automatic room partitioning, where the first is our approach as discussed in Chapter~\ref{sec:room_label}~\cite{Turner14}.  There are also methods that take a floor plan as input, and use this information to generate a 3D model of the environment by extruding the defined wall information~\cite{Or05,Lewis98}.  This type of modeling yields aesthetically pleasing results with well-defined floors, walls, and ceilings.

% This subsubsection describes plane fitting for 3D modeling
\subsection{Simplified 3D Modeling}
\label{ssec:background_planefit}

Since building features are almost entirely planar, a popular approach is to explicitly fit planar elements to the input point-clouds.  Sanchez and Zakhor use PCA plane-fitting to find floors, walls, and ceilings explicitly in the point-cloud~\cite{Victors}. Since this method was applied to ambulatory data, it resulted in missing components, holes, and double-surfacing due to mis-registration.  Xiong et al also perform plane-fitting in a similar fashion, but also analyze the computed planes for the locations of windows and doors, which are represented as holes in the surface~\cite{Xiong13}.  Adan et al find planes by first generating a floor-plan, then extrude the floor-plan into a 3D model~\cite{WallFinder}.  One limitation with these methods is that they are not necessarily watertight, though floor plan extrusion can also be done in a watertight fashion~\cite{Mura14,Turner14,Cabral14}.  These approaches allow for accurate wall geometry and reduce the complexity of the output model. Extruded floor plans also allow for models to explicitly define floors, walls, and ceiling surfaces.  

Other approaches have focused to perform volumetric processing to ensure watertightness when computing simplified 3D models.  Xiao et al find horizontal cross-sections of the building, forming a sequence of 2D CSG models that are then stacked together and simplified~\cite{Museums}.  While this approach does lead to aesthetically-pleasing models, it assumes manhattan-world models, which leads to topological errors if an insufficient number of cross-sections are recovered.  Oseau et al use a voxelization approach, with a follow-up graph-cut step to remove small details in the environment, leaving only floors, walls, and ceilings~\cite{Oesau13}.  This optimization step, however, can also cause significant deformations in the final geometry depending on the input parameters. 

% This subsubsection describes dense 3D modeling
\subsection{Detailed 3D Modeling}
\label{ssec:background_3dmodeling}

One of the methods used to preserve the detail of furniture in a building scan is to explicitly search and classify for furniture models in the scan.  These techniques attempt to find locations in the input scans that match best with a stored database of known furniture.  The pre-existing model of the recognized piece of furniture is then oriented in the output model.  Nan et al employ this technique, with explicit classification of chairs and tables~\cite{SearchClassifyPointcloud}.  Kim et al also use this method, using a larger library of objects and operating on noisier scans~\cite{Kim12}.  They also discuss how search-classification can allow for change detection across scans of the same area taken at different times.  A major downside of this method is that the classification is only as good as the database.  Objects that are in unexpected orientations or are not in the database are misclassified.  For instance, a sideways chair is misclassified as a table.  One major benefit of this approach is the ability to model the objects independently of the room itself, as discussed later in Chapter~\ref{ch:better_carving}.  

% object/room detection in buildings
Recently, object detection methods from indoor scans have been proposed without the use of a training dataset~\cite{Mattausch14}.  They segment point clouds using a bottom-up approach to fit rectangular patches on to the scans, then find clusters of patches that are repeated often.  Even though this approach can be applied to large datasets, it assumes very basic building geometry in order to segment objects.  Additionally, it only detects objects of certain complexity, is unable to detect very small objects, and requires objects to be repeated often in the environment.  We expand on this work by segmenting objects volumetrically, rather than in the point cloud domain.

% general carving
There are also methods that capture fine detail of buildings by attempting to be as accurate to the input point-cloud as possible.  Holenstein et al generate a space-carving model that voxelizes the scanned environment, labeling any voxels intersected by a scan-line line to be interior~\cite{Carving}.  The boundary of the interior voxels is meshed with Marching Cubes.  The advantage of this approach is an increased robustness to mis-registration errors, but the downside is that over-carving can result in the loss of fine detail.  Lastly, a popular approach to detailed modeling of environments is Kinect Fusion~\cite{KinectFusion,Kintinuous}.  This method allows for both meshing and tracking, but is limited in that it cannot handle large areas, and the actual meshing approach is a minor extension of Signed-Distance Fields~\cite{SignedDistanceFields}. 

% this section outlines the remainder of the paper
\section{Contributions and Organization of Dissertation}
\label{sec:organization}

% organize paper
In this dissertation, we present several techniques used to automatically generate virtual models of indoor building environments.  The interior environment of a building is scanned using our custom hardware system, which provides a set of data products used to develop these models.  Our modeling technqiues can be separated into two categories:  2D floor plan models and dense 3D models.  All approaches are produced automatically from the output data of our scanning system.  These approaches are intended to be agnostic to the specific hardware used for scanning and in many cases have been applied to scans taken with other hardware systems.  Unless stated otherwise, however, any examples shown in this dissertation were generated from scans acquired using our hardware system.

First, we discuss the specifics of our hardware system in Chapter~\ref{ch:hardware}.  This system is a collection of laser scanners and other sensors, worn as a backpack by a human operator.  The operator traverses the environment at normal walking speed, allowing the system to collect data about the interior area.  We discuss specifics on the mechanics of our backpack system, how the data are logged, and special considerations needed to ensure accuracy of the resulting scans.

Next, in Chapter~\ref{ch:preprocessing}, we discuss preprocessing steps required once a data acquisition is performed.  Specifically, these steps detail how the system is localized while traversing through a GPS-denied environment and how we ensure a consistent global coordinate system.  We also discuss the first modeling technique, which is to generate a point cloud of the environment.  This point cloud can be colored using the camera imagery.  Additionally, these scans are used to determine the number of building levels covered during the acquisition, and the partitioning of those levels.

In Chapter~\ref{ch:floorplan}, we detail the techniques developed to generate 2D floor plans of the interior building environment.  These techniques use the 3D scan information to generate a consolidated 2D estimate of wall positions.  These wall position estimates are used to develop a volumetric floor plan of each level in the building environment.  We show multiple floor plan generation techniques, including methods to automatically detect and classify separate rooms in the environment.  Once a 2D floor plan is produced, its geometry can be extruded into a simplified 2.5D model, which includes height information.

In Chapter~\ref{ch:carving}, we denote multiple methods used to generate complex, fully-3D models of building environments.  We compare the complexity of these models to those generated by extruding 2D floor plans, as well as how the detail of the environment's geometry can be accurately preserved.

In Chapters~\ref{ch:better_floorplans} and~\ref{ch:better_carving}, we discuss how these two types of methods -- 2D floor plan generation and 3D dense modeling -- can be combined to provide additional analysis on building modeling.  This combination of techniques can be used to not only augment each individual model output to improve accuracy, but also provide new information about the environment.  Chapter~\ref{ch:better_floorplans} discuss how the 2D floor plan generation methods can be improved by using the 3D dense modeling results, and Chapter~\ref{ch:better_carving} iterates how the 3D dense modeling is improved by incorporating the 2D floor plan output.

In Chapter~\ref{ch:applications}, we show the applications of these techniques, and how building models at different granularity are required based on how the model is to be used.  We show several examples of use-cases for these modeling techniques.

Lastly, in Chapter~\ref{ch:conclusion}, we offer concluding remarks and directions for future work.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Hardware Description
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Hardware Description}
\label{ch:hardware}

The surface reconstruction algorithms described in this dissertation can be applied on any indoor scanning data.  For practicality purposes, we have developed our own scanning system, and all examples shown in this document are generated with our custom hardware (unless otherwise stated).

We developed a human-mounted ambulatory scanning system, worn as a backpack.  As the human operator walks through the building environment, at normal speed, this system collects data from a variety of sensors on-board.  These data products are logged and then processed offline.  Our developed software can accurately estimate the trajectory of the system over time and localize the system~\cite{NickJournal}.  This procedure allows us to rapidly move through a large environment, spending only a few seconds in each room yet capturing full geometry information.

\section{Mechanical Specification}
\label{sec:mechanical}

% this figure shows side-by-side backpacks
\begin{figure}

	\centering
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_1}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_2}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_3}}
		\centerline{(c)}\medskip
	\end{minipage}	

	\caption[The three backpack scanning systems developed by our lab.]{The three backpack scanning systems developed by our lab:  (a) First generation, weighing 80 pounds; (b) second generation, weighing 32.5 pounds, developed in 2013; (c) third generation, weighing 35 pounds, developed in 2014.}
	\label{fig:all_backpacks}

\end{figure}

% different backpack models
As shown in Figure~\ref{fig:all_backpacks}, our lab has developed three generations of the backpack hardware.  The first generation system, shown in Figure~\ref{fig:all_backpacks}a, was built as a prototype system~\cite{Backpack}.  It contains five Hokuyo laser scanners, two cameras, and an Internal Measurement Unit (IMU).  Since 2012, it has been augmented with WiFi antennas and infrared cameras.  The WiFi antennas are used to record signal-strength of nearby access points, which is useful for indoor localization techniques~\cite{Levchev14}.  The infrared cameras are useful for energy-efficiency analysis of building environments.  These data products are discussed further in Chapter~\ref{ch:applications}.

The second generation backpack, as shown in Figure~\ref{fig:all_backpacks}b, was constructed in 2013.  It contains all sensors available on the first backpack, except for the infrared cameras.  It also has a barometer and a set of 3D magnetometers.  The main modification of the second generation was to reduce the size.  The first generation is about 80 pounds, whereas the second generation is only 32.5 pounds.  The third generation, shown in Figure~\ref{fig:all_backpacks}c, backpack is almost identical to the second generation, but with the addition of infrared cameras.

\section{Sensor Characteristics}
\label{sec:sensor_specs}

% figure of backpack hardware
\begin{figure}[t]

	\begin{minipage}[c]{0.54\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/hardware/backpack_annotated}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.46\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/backpack_coord_system}}
		\centerline{(b)}\medskip
	\end{minipage}

	\caption[Annotated scanning hardware system.]{Annotated scanning hardware system, worn as a backpack.  The operator walks through the indoor building area as the system scans the surroundings.  Our method combines the LiDAR and inertial measurements in order to form a virtual 3D model for the observed space:  (a) the system worn by operator; (b) the common coordinate frame of backpack system.}
	\label{fig:backpack}
\end{figure}

% briefly describe the type of scans we use as input
Our geometry reconstruction algorithms use the retrieved laser range data as input.  These scans are taken with time-of-flight laser scanners that capture 3D geometry of the environment.  We use multiple 2D laser scanners mounted at different orientations to capture all observed geometry in the environment, as indicated in Figure~\ref{fig:backpack}a.  Since our system is mobile and ambulatory, the resulting point clouds have higher noise than traditional static scanners due to natural variability in human gait.  As such, our approach needs to be robust to motion induced scan-level noise and its associated artifacts.  We use these scans to form watertight models of the indoor building environment and the contained objects.

% sparse but fast
The primary advantage of a backpack system is speed of acquisition.  We use 2D Time-Of-Flight (TOF) laser scanners, which means we have sparser scan information per frame and traverse each area of the environment much faster than when scanning the equivalent area with a Kinect or Google Tango.  Since our system produces much sparser point clouds than Kinect-based scanning, it is not feasible to employ the same averaging techniques that allow for high-quality Kinect scans.  The advantage of these 2D TOF sensors is that they are less noisy, have a longer range, and a wider field of view.  The result is that much larger environments can be covered in significantly less time when using these sensors in a sweeping motion~\cite{Sweep}.  For instance, a backpack-mounted system allows an unskilled operator to rapidly walk through an environment to acquire data.

% the noise characteristics
Our system uses Hokuyo UTM-30LX sensors, whose intrinsic noise characterization is given in~\cite{Pomerleau12}.  Typically this noise contributes on the order of 1 to 2 centimeters to the standard deviation of the positional estimate of scan points.  This uncertainty value increases as the range of the point increases, with accurate measurements stopping at a range of $30$~meters.  We also must consider input noise from our cameras.  We use three 12-megapixel cameras with fisheye lenses, in order to collect as much of the surrounding scenery as possible.  Due to this set-up, a major factor in sensor calibration is how long of an exposure to use for each image.  Since we may be scanning in dark or narrow areas of building environments, ensuring that enough light hits the sensor is challenging, especially with fisheye lenses.  However, if we set the exposure too long, then the imagery is susceptible to motion blur, since the operator often walks or turns quickly.

% data rate from cameras, scalability of systems.
Since we collect 12~MP imagery from three cameras at 2 Hz each, the camera images represent the major of data flow during the data acquisition process, at 72~megabytes per second.  To ensure no data loss, we necessitate a fast internal drive on the backpack system.  We have also configured the system with a USB-3 external drive, with twin SSD drives in a RAID-0 configuration.  This set-up allows us to write data directly to the external drive, which can be hot-swapped for fast processing.  On the second generation system, this storage drive is 500~GB, which allows for just under two hours of consecutive scanning.  The third generation system contains a 1~TB drive, which allows for up to four hours of scanning.  At that rate, we can perform data acquisition of over 100,000 square feet of building space on a single drive.

\section{Sensor Calibration}
\label{sec:calibration}

% also discuss extrinsic calibration of sensors
Since our hardware system contains may separate sensors, it is necessary to create a common system coordinate frame, as shown in Figure~\ref{fig:backpack}b.  Each sensor has an independent coordinate frame, but they are extrinsically calibrated against one another, in order to establish the rotation and translation between any given sensor and the common coordinate frame.  Once this extrinsic calibration process is performed, we can determine the orientation of any given sensor with respect to the common system.  As discussed in Chapter~\ref{sec:localization}, after a data collection we can determine the orientation of the system within a model coordinate frame.  The combintation of the transform generated by extrinsic sensor calibration and the localization process allows for sensor readings to be placed in model coordinates.

% discuss time synchronization, both during collection and processing
In addition to geometric calibration of the individual sensors, we must also consider temporal calibration of the sensor readings to a common clock.  Since each sensor operates independely on a separate clock, it is important to compute the transformation from a given sensor's time frame to that of the common system.  For this computation, we make the assumption that there is an affine transform between any two clocks.  Such an assumption is reasonable, since two clocks can naturally have a phase offset based on when they were initialized, as well as a scalar offset due to factors such as running temperature and age.  Timestamp synchronization is acheived by recording two timestamps for every scan frame of each sensor, one from the sensor clock and one from the system computer.  This process must be done in software since not all sensors have the capacity for hardware triggering.  The linear fit between clocks is computed and typically represents a fitting error of around $2$ to $7$~milliseconds standard deviations.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Preprocessing and Conventions
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Preprocessing and Conventions}
\label{ch:preprocessing}

This chapter details processing steps that are required before any model generation can occur.  All approaches discussed in this dissertation require a reconstructed path of how the system moved through the environment.  Such a path trajectory is required in order to estimate the position of each scanner at any given time, so that the geometry readings can be placed in global coordinates.  The details of how the localization data are used are described in Sections~\ref{sec:localization} and~\ref{sec:align_path}.

Some methods discussed in this dissertation also require a consolidated 3D point cloud of the scanned environment.  Unlike most geometry products discussed, a point cloud contains no topology information.  Instead, a point cloud is a concatenation of the raw laser range data, transformed into the global coordinate frame.  Specifics on how these values are generated and what conventions are used are discussed in Section~\ref{sec:pointcloud}.

\section{Localization}
\label{sec:localization}

% Nick's localization procedure
Before models of the building geometry can be generated, we must first determine how the operator traversed the environment.  Indoor building environments are typically GPS-denied areas, so local observations are required in order to track the path walked~\cite{Backpack,Localization,NickJournal}.  The process of recovering the 3D trajectory of the system is especially difficult for our backpack hardware, since a human operator can experience six degrees of freedom in movement ($x$, $y$, $z$, $\theta$, $\phi$, $\psi$), whereas the sensors used in the system can only observe a 2D slice of the environment.  As such, multiple laser scanners are mounted at different orientations on the backpack hardware to ensure that full 3D movement can be observed~\cite{NickJournal}.  These incremental changes in pose are refined by a global graph optimization step that constrains the system path whenever an area is revisited~\cite{toro07}.  These revisits are automatically detected and constrained in order to produce an accurate 2D trajectory of the system.  The elevation of the system is computed using a similar technique with a vertically mounted scanner~\cite{Backpack}.

% coordinate systems used:  sensor, backpack, model
The localization procedure is complete, we have full mapping between three coordinate systems for each recorded frame for each hardware sensor.  The internal coordinate frame of the sensor, which is specific to the make and model of the sensor hardware, can be converted to the system common coordinate frame based on the extrinsic calibration of each sensor's rigid position and orientation on the backpack system, which is given by the coordinate frame shown in Figure~\ref{fig:backpack}b.  This rigid affine transformation for a given sensor $s$ is represented by the $3 \times 3$ rotation matrix $R_{s\rightarrow c}$ and the $3 \times 1$ translation vector $T_{s\rightarrow c}$.  The localization procedure provides an output path of the system, so that each pose of the system can be transformed from the system common coordinate frame to the world coordinate frame.  At a given time $t$, the affine transformation from the system common coordinates to the model coordinates is given by $3 \times 3$ rotation matrix $R_{c\rightarrow m}(t)$ and $3 \times 1$ translation vector $T_{c\rightarrow m}(t)$.  With these transformations, any sensor reading can be placed in global coordinates.

% discuss output model frame of localization, and units
The localization output path provides the output model coordinate frame in a locally consistent coordinate frame based on the starting position of the operator during a scan.  This starting position is used as the origin point of the model.  By convention, the heading vector of the first pose is used as the $+X$-direction of the model coordinate frame.  The direction of gravity is used as the $-Z$-direction of the model coordinate frame.  The $Y$-direction of the model coordinate frame is chosen such that the coordinate system is right-handed.  Although this procedure produces a common set of coordinates for the model, we apply an additional step in order to align the model with the global coordinate frame, as discussed in Section~\ref{sec:align_path}.  Once this step is complete, the model uses East-North-Up (ENU) coordinates.  For the rest of this dissertation, all output models are expressed in units of meters.

% show figure of double surfacing.
\begin{figure}
	\centerline{\includegraphics[width=0.7\linewidth]{quals/misregistration_example}}
	\caption{An example of misregistration or ``double surfacing'' in a point cloud.}
	\label{fig:double_surface}
\end{figure}

% noise models of localization output (e.g. double-surfacing)
Since the localization procedure used is performed in an integrative fashion, the expected error in the reported position and orientation of the system at any pose is higher than in competing technologies, such as static scan systems~\cite{NickJournal}.  Static systems often utilize markers or control points within the environment to ensure manual alignment of scans, allowing for positional accuracy of within $1~mm$~\cite{Li97,Karimi00}.  By contrast, the mean positional accuracy of the output poses in the backpack localization procedure is typically $10~cm$.  Additionally, the errors in successive poses are highly correlated.  Typically a local set of scan points captured within a few seconds of one another are highly accurate, yet if the same features in the environment are scanned twice with a long interval between scans, then the scans are likely to be mis-matched.  This behavior leads to the presence of ``double-surfacing'' in the output scans, which can be observed in the generated point cloud of the environment.  Figure~\ref{fig:double_surface} shows an example of such misregistration.  The chair in the depicted point cloud is represented by two copies, slightly offset from one another.  Note that the point cloud shown is generated using a static scanning system, which can also suffer from misregistration issues.

In order to provide consistent output geometry, the modeling techniques described in this dissertation are designed to prevent such ``double-surfacing'' from occurring in the output meshes.  This trait is accomplished by performing volumetric analysis of the input scans, as discussed later in this document. 

\section{Aligning Model Coordinates}
\label{sec:align_path}

% show color pointcloud compared to camera image
\begin{figure}
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{align_path/compass_readings}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{align_path/aligned_path}}
		\centerline{(b)}
	\end{minipage}

	\caption[Aligning model coordinate system with North.]{An example of aligning model coordinates to magnetic north:  (a) individual compass estimates of North (red) are compiled from each pose of the path (blue) and represented in world coordinates; (b) the model coordinates are transformed so that the mean estimate of magnetic North across all readings is oriented in the $+Y$-direction.}
	\label{fig:align_path}
\end{figure}

As discussed in Section~\ref{sec:localization}, the system path generated by the localization procedure is initially oriented using the heading vector of the first pose.  In order to keep the orientation of model coordinate frames consistent between separate scans of the same environment, and to ensure that output geometry is easily recognizable, we perform an additional orientation step that aligns the path to the East-North-Up (ENU) coordinate frame.

One of the on-board sensors of the backpack hardware system is a 3D magnetometer, which is built into the Inertial Measurement Unit (IMU).  Individual readings from a 3D compass are typically noisy when indoors, due to nearby ferrous building elements that interfere with hard and soft iron calibration~\cite{Caruso00,Guo08}.  As such, these readings are not reliable during the localization process.  Once the final, consistent path is generated we are able to generate a least-squares estimate of compass north and reorient the path so that North is aligned with the $+Y$-direction.

First, the 3D magnetometer reading is recorded at each pose during the data acquisition.  As shown in the example in Figure~\ref{fig:align_path}a, these vector readings represent the magnetic field at that location.  The direction of each reading is used as a noisy estimate of the direction of magnetic South.  The magnetic field flows South, so the reverse direction, as shown in Figure~\ref{fig:align_path}a, is an estimate of North.  Let $\vec{m}_i \in \mathbb{R}^3$ be the magnetometer reading at pose $i$, in the coordinate frame of the magnetometer sensor $\texttt{mag}$.  The rotation required to orient the model coordinate frame to ENU coordinates is given by:

\begin{equation}
	\label{eq:align_path}
	\texttt{argmin}_{\theta \in [0,2\pi]} \; \sum \limits_{i} 
		\; \left| \left( R_{z}(\theta) \, R_{c\rightarrow m} (t_i) \,
			R_{\texttt{mag} \rightarrow c} \, m_i \right) - \left[ 
				\begin{array}{c} 0 \\ -1 \\ 0 \end{array} 
					\right] \right|^2
\end{equation}

where $t_i$ is the timestamp of pose $i$, and $R_{z}(\theta)$ is the rotation matrix:

\begin{equation}
	\label{eq:rotation_matrix}
	R_{z}(\theta) = \left[ \begin{array}{ccc}
				\texttt{cos}(\theta) & -\texttt{sin}(\theta) & 0 \\
				\texttt{sin}(\theta) & \texttt{cos}(\theta) & 0 \\
				0 & 0 & 1 \end{array} \right]
\end{equation}

This optimization rigidly rotates the model coordinate frame about the $z$-axis in order to best align all magnetometer readings with the direction we wish to denote south:  the $-Y$-direction.  An example result of this procedure is shown in Figure~\ref{fig:align_path}b.  This procedure ensures that all output models are aligned with magnetic North, in ENU coordinates.  While not applied in the current configuration, it is possible to also align the model to true North if given the latitude and longitude coordinates of the scanning location, by performing a look-up of magnetic declination~\cite{MagDec}.

\section{Point Cloud Generation}
\label{sec:pointcloud}

When a finalized path and coordinate frame is produced, we can immediately generate a point cloud of the raw scans taken in the environment.  This point cloud allows visualization of observed building geometry.  Each point can be computed independently by transforming its position from its sensor frame to the model coordinate frame.  An example point cloud of an indoor environment is shown in Figure~\ref{fig:pointcloud_nocolor}.  This scene shows a corner of a room, with a lamp to the left of a TV on a table.

% show example pointcloud
\begin{figure}
	\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_nocolor}}
	\caption{An example point cloud of an indoor building environment.}
	\label{fig:pointcloud_nocolor}
\end{figure}

We also have the capability to export point clouds in color.  The coloration is taken from camera imagery taken temporally close to when the range points were acquired.  For a given point $\vec{x}$ taken at time $t$, the point color is determined by querying all available cameras for any images within $\Delta t$ seconds of time $t$.  Typically, we use a window size of $\Delta t = 2$~seconds.  For each reported camera, we project the position $\vec{x}$ onto the camera image plane, and determine the pixel color at that position.  This projection is demonstrated in Figure~\ref{fig:pointcloud_color_diagram}.  In addition to color, we also compute a quality measure $q_x = (\vec{x} - \vec{c})^T \vec{n}$, where $\vec{c}$ is the camera position and $\vec{n}$ is the optical axis of the camera.  The quality $q_x$ is $1.0$ if $\vec{x}$ projects into the center of the image, and decreases as the projected location moves away from the center of image.  For all images within the time window $[t - \Delta t, t + \Delta t]$, the pixel color associated with the highest quality meaure is used to color the point.

The resulting point cloud is colored based on all available imagery in the dataset.  Figure~\ref{fig:pointcloud_color}a shows a camera image of an example seen and Figure~\ref{fig:pointcloud_color}b shows the colored point cloud of the same environment.

% figure showing projection of points onto images
\begin{figure}
	\centerline{\includegraphics[width=0.5\linewidth]{pointcloud/gradlounge/pointcloud_color_diagram}}
	\caption{Determining the color of point $x$ based on camera imagery.}
	\label{fig:pointcloud_color_diagram}
\end{figure}

% show color pointcloud compared to camera image
\begin{figure}
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/camera}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_color}}
		\centerline{(b)}
	\end{minipage}

	\caption{An example point cloud colored by nearby camera imagery.}
	\label{fig:pointcloud_color}
\end{figure}

% partitioning pointclouds into separate levels
\section{Partitioning Point Cloud by Building Levels}
\label{sec:pointcloud_level_split}

Many methods described in this dissertation require explicit detection and partitioning of the different levels, or stories, in a scanned building.  Since our hardware system is human-mounted, the operator can walk up and down stairs during the data acquistion process, facilitating scans of multiple stories at once.  

Some localization systems that rely on 2D grid maps of wall samples are capable of detecting when the operator moves from one level to another and automatically partition their output grid maps accordingly~\cite{MITBackpack}.  We do not just want the elevation of each level, but the vertical extent as well.  In order to detect and separate building levels, we identify the primary floor and ceiling surfaces for each level.  This identification can be done either in the point cloud domain or after a full model has been generated.  The latter method is discussed in Chapter~\ref{ch:better_floorplans}.  Here, we discuss how level splitting can be accomplished via point clouds, which offers a faster if less accurate partitioning.

A histogram approach can be used to separate the point cloud by levels~\cite{Turner12,Turner14Journal}. Figure~\ref{fig:heighthist}a shows an example point cloud, colored by height, which contains multiple levels.  By computing a histogram along the vertical-axis of the point-cloud, it is possible to find heights with high point density, which indicates the presence of a large horizontal surface.  An example of this process is shown in Figure~\ref{fig:heighthist}b, where peaks in the histogram correspond to the floors and ceilings of each scanned level in the building.  Points scanned from above, in a downward direction, are used to populate a histogram to estimate the position of each floor, and the histogram used to estimate the position of each ceiling is populated by points scanned from below.  The local maxima of these two histograms show locations of likely candidates for floor and ceiling positions, which are used to estimate the number of scanned levels and the vertical extent of each level.  The candidate floor and ceiling heights are pruned by first taking the lowest floor maxima as the elevation of the first level's floor.  The first level's ceiling elevation is determined by the most populated ceiling maxima position that resides below the next maximal floor elevation.  This process is repeated for all levels, which allows for detection of both number of levels and their range extents.  Once levels are separated, they can be processed and analyzed separately.  Figure~\ref{fig:heighthist}c shows the final extruded mesh with all three scanned levels.

% show example histogram and partitioning
\begin{figure}

	\centerline{\begin{minipage}[c]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/points_LEFT}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.35\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/hist}}
		\centerline{(b)}\medskip
	\end{minipage}}
	\begin{minipage}[c]{0.9\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/mesh}}
		\centerline{(c)}
	\end{minipage}
	
	\caption[An example point-cloud partitioning by height.]{An example point-cloud partitioning by height: (a) the input point-cloud, showing geometry for three levels; (b) the vertical histogram showing estimates of each building level height; (c) the produced mesh of this building scan using method from Chapter~\ref{ch:floorplan}.}
	\label{fig:heighthist}

\end{figure}


%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Floorplan Generation
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Floor Plan Generation}
\label{ch:floorplan}

In this chapter, we discuss multiple methods to produce 2D floor plans of indoor building scans.  Both methods generate floor plan geometry by first generating a set of 2D wall samples.  Wall samples are represented by a set of points that locate areas of high likelihood of being large vertical surfaces.  We list the evolving techniques we use to generate wall samples in Section~\ref{sec:wall_sampling}.

Once wall sampling is performed, we have developed two different floor plan generation techniques.  In Section~\ref{sec:eigencrust}, we discuss a global optimization approach to generate watertight floor plan geometry based off of the Eigencrust algorithm~\cite{EigencrustShewchuk,Turner12}.  This method explicitly fits both straight and curved features into the floor plan geometry, in order to preserve curved building features.  In Section~\ref{sec:visigrapp}, we discuss our second floor plan generation approach.  This approach is adapted from our original algorithm, but no longer requires a global optimization step, allowing for faster processing~\cite{Turner14}.  Additionally, it explicitly partitions the floor plan geometry into individual rooms.  This method also introduces simplfication mechanisms that allow for minimal elements to represent the geometry while still preserving the watertight, volumetric topology.

Lastly, in Section~\ref{sec:fp_results}, we show several results comparing each of these methods, including multi-story models showing how these methods can operate on datasets taken over several levels of a building environment.  We also show comparisons of our automatically generated results to the ground-truth blueprints of the scanned environments.

% wall sampling technqiues
\section{2D Wall Sampling}
\label{sec:wall_sampling}

% overview of wall sampling: motivation
The input data used during floor plan generation consist of points in the ($x$,$y$) horizontal plane, which we call wall samples.  These points depict locations of walls or vertical objects in the environment.  We assume that interior environments satisfy ``2.5-Dimensional'' geometry:  all walls are vertically aligned, while floors and ceilings are perfectly horizontal.  In many application scenarios only 2D scanners operating in one plane are used, so this assumption is needed to extract 3D information about the environment.  Many mapping systems use a horizontal LiDAR scanner to estimate a map of the area as a set of wall sample positions, while refining estimates for scanner poses.  These mobile mapping systems often have additional sensors capable of estimating floor and ceiling heights at each pose~\cite{Backpack,Quadrotor}.  The input to our algorithm is a set of 2D wall samples, where each sample is associated with the scanner pose that observed it, as well as estimates of the floor and ceiling heights at the wall sample location.

% methods of wall sampling
Wall samples can be derived in three different ways.  First, histogram analysis can be performed on the 3D point cloud generated for a building environment as discussed in Chapter~\ref{sec:pointcloud}, in order to generate a 2D sampling of wall positions~\cite{Turner12}.  This technique is discussed in Section~\ref{ssec:ws_from_pc}.  Second, wall samples can be exported directly from the grid-map generated alongside a particle filter during the localization step of scan processing~\cite{NickJournal,Turner14}.  This method is described in Section~\ref{ssec:ws_from_pf}.  Third, wall samples can be generated from a processed 3D volumetric model of the environment.  This last option is described in detail in Chapter~\ref{ch:better_floorplan}.

% first wall sampling technique:  from a point cloud
\subsection{Generating Wall Samples from Point Clouds}
\label{ssec:ws_from_pc}

% describe the basic idea of histogramming to find wall samples
Given a 3D input point set, $P$, we wish to generate a floor plan for each story represented.  The coordinate system is defined so that the $z$-component represents height.  First, we compute how many stories are present in $P$ and partition $P$ into separate point sets $P_0$, $P_1$, ..., $P_f$ for each level by height, as discussed in Chapter~\ref{sec:pointcloud_level_split}.

For each story $P_k$, the method for computing wall samples is to subsample the full 3D point-cloud to a set of representative 2D points~\cite{Turner12,Turner14,Turner14Journal,Okorn09}.  This process cannot be done in a streaming fashion, but can provide accurate estimates for wall positions.  Such an approach is useful when representing dense, highly complex point clouds with simple geometry.  Under the 2.5D assumption of the environment, wall samples can be detected by projecting 3D points onto the horizontal plane.  Horizontal areas with a high density of projected points are likely to correspond to vertical surfaces.  Our goal is to find these wall locations with a sparse sampling of points $S_k \subseteq P_k$.  By defining a grid along the x-y plane, we can find coordinate locations that are likely to represent walls.  The width of each grid cell is denoted by $d_s$, which should be smaller than the minimum feature size expected in a floor plan.  A typical value for this parameter is $5$~cm.  This grid partitions $P_k$ laterally.  For a grid cell $g$, let the neighborhood set $N_g$ be the subset of points of $P_k$ that are within a horizontal distance of $d_s$ from the center of $g$, as shown in Figure~\ref{fig:grid_filtering}.  Thus a given point can be in the neighborhood of multiple grid cells.

% figure showing wall sampling process
\begin{figure}

\begin{minipage}{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=0.6\linewidth]{3dimpvt2012/wall_sampling/grid_filtering}}
\end{minipage}

\caption[Generating wall samples from 3D point clouds.]{A point set $P_k$ (small dots in blue) is partitioned by a grid in the $x$-$y$ plane (lines in black).  For each grid cell, a neighborhood of points (medium dots in red) is computed and the median position of the points (large dot in green) is taken as a wall sample.  The normal of this neighborhood (vector in dark green) is computed using PCA.}
\label{fig:grid_filtering}

\end{figure}

% real-world example of wall sampling
\begin{figure}

\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/wall_sampling/BHH-Lobby_topdown}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/wall_sampling/BHH-Lobby_wall_sampling}}
  \centerline{(b)}\medskip
\end{minipage}

\caption[Wall sampling from static point cloud.]{(a) Point cloud of scanned area, viewed from above and colored by elevation; (b) wall sample locations generated from point cloud.  Clutter such as furniture or plants do not affect position of wall samples.}
\label{fig:bhh_wall_sampling_example}

\end{figure}

We perform the following checks on $N_g$ to determine whether this neighborhood represents a portion of a wall.  The first assertion is that a wall must be densely sampled, so if $|N_g|$ is less than a threshold, it is rejected. Our scanners create point-clouds with thousands of points per square meter of surface, so a threshold of $|N_g| \geq 50$ points is sufficient.  The second check is to verify the height of a wall.  The vertical support of $N_g$ must be at least the minimum wall height threshold. The value of this height threshold may vary depending on application, but a length of $2$ meters works well to capture permanent wall features while ignoring furniture and other interior clutter.  We also wish to verify that the distribution of points is uniform vertically, which signifies that a flat surface was captured.  For this uniformity check, we compute the histogram of the heights of $N_g$ with 16 bins and enforce that at least 12 of these must be non-empty.  Additionally, at least three of the highest four bins must be non-empty, which ensures a neighborhood extends all the way to the ceiling.  If these requirements are fulfilled, $g$ is said to represent a wall.  The median horizontal position for the elements of $N_g$ is computed.  If this median lies within the bounds of $g$, then it is taken as a ``wall sample'' for $g$.

The normal vector for this wall sample is calculated using Principal Components Analysis (PCA) \cite{PCA}.  Let $C$ be the $2 \times 2$ covariance matrix of the $x$-$y$ positions of the elements of $N_g$ and $e_1, e_2$ be the eigenvectors of $C$ with corresponding eigenvalues $\lambda_1, \lambda_2$.  
Let $\lambda_1 \leq \lambda_2$.  The normal vector is chosen to be $\vec{n} = e_1$, as shown in Figure~\ref{fig:grid_filtering}.  The confidence of this normal estimate is computed as $c = 1 - \frac{2 \lambda_1}{\lambda_1 + \lambda_2}$.  If $c$ is less than 0.15, then the neighborhood points are not well-aligned and rejected as the location for a wall.

If pose information for the scanner is known for each point, this normal vector's direction may be flipped to guarantee that it points towards the laser scanner.  Any wall samples captured far away from the scanner are thrown away as outliers.  This distance is typically 5-10 meters.  If a feature is not scanned within this distance, it is only partially captured and cannot be accurately represented. Performing the above operation for all grid cells results in a set of wall sample points $S_k \subseteq P_k$.  

% houston wall samples figure
\begin{figure}
  \centering
  \includegraphics[width=0.98\linewidth]{visigrapp/algorithm/input_points/houston_wall_samples_with_labels}
  \caption[Wall samples from backpack point cloud.]{Example input wall samples of hotel hallways and lobby generated from a particle filter system. (a) Wall samples of full model; (b) close up of wall in model.}
  \label{fig:backpack_wall_sample_example}
\end{figure}

The result is a set of wall samples, where each wall sample is represented by its 2D position, the minimum and maximum height values of the points that sample represents, and the poses of the scanners that observed the sample location.  As we discuss later, these scanner poses provide crucial line-of-sight information that facilitate floor plan reconstruction.  Figure~\ref{fig:bhh_wall_sampling_example} shows an example of collecting wall samples from a point-cloud of a hotel lobby.  While most examples in this dissertation will be generated using our backpack-mounted scanning system, this point cloud was created with static scanners, to showcase the versitility of our system. An example of wall samples generated from backpack scans for a hotel hallway is shown in Figure~\ref{fig:backpack_wall_sample_example}.  As shown, even though the walls are well sampled, noise in the localization estimate causes noisy wall samples with outliers.

% second wall sampling technique:  from a particle filter
\subsection{Generating Wall Samples from Particle Filter Grid Maps}
\label{ssec:ws_from_pf}

% using grid map
Many mapping systems use a horizontal LiDAR scanner to estimate a map of the area as a set of wall sample positions, while refining estimates for scanner poses.  Such systems perform Simultaneous Localization and Mapping (SLAM) by defining a rough grid-map of the area while also estimating the pose of the system via a particle filter~\cite{NickJournal,Quadrotor}.  This grid-map is often represented by a set of 2D position samples that indicate the presence of walls or other obstacles~\cite{ProbabilisticRobotics}.  In such particle filtering approaches, the grid map can be used direction as a set of wall samples for floor plan model generation~\cite{Turner14}.  As discussed in Section~\ref{sec:visigrapp}, we have developed a technique for floor plan generation that is fast enough to work in a streaming, real-time fashion on this type of input data.  Particle filtering approaches to localization typically result in real-time mapping~\cite{fastslam03,toro07} and can therefore benefit from a real-time floor plan generation algorithm that delivers a live map of the environment. 

These mobile mapping systems often have additional sensors capable of estimating floor and ceiling heights at each pose~\cite{Backpack,Quadrotor}.  The input to our algorithm is a set of 2D wall samples, where each sample is associated with the scanner pose that observed it, as well as estimates of the floor and ceiling heights at the wall sample location.  In Section~\ref{sec:fp_results}, we show comparisons of floor plans generated with both types of wall samples:  those generated from point cloud data and those generated from particle filter grid maps.

% eigencrust floor plan reconstruction
\section{Eigencrust Floor Plan Reconstruction}
\label{sec:eigencrust}

Given a point cloud representing a subset of the interior of a building that can cover multiple stories, we propose a three-step process to generate a watertight floor plan for each story in the point cloud by fitting line segments and curves to model features~\cite{Turner12}.  First, The output wall samples generated using the method described in Section~\ref{sec:wall_sampling} are smoothed to prevent double-surfacing effects, as discussed in Section~\ref{ssec:eigencrust_smoothing}.  Second, the Delaunay Triangulation of the generated wall samples is computed and each triangle is labeled as ``inside'' or ``outside'' using a 2D version of the Eigencrust algorithm~\cite{EigencrustShewchuk}.  This process is discussed in Section~\ref{ssec:eigencrust_triangulation}.  The edges that border these two labels are output as wall locations, as detailed in Section~\ref{ssec:eigencrust_boundary}.  Third, noise reduction is achieved by fitting these facets to line segments and curved components using Random Sample Consensus (RANSAC)~\cite{Ransac}, discussed in Section~\ref{ssec:eigencrust_fitting}.

\subsection{Smoothing Wall Samples}
\label{ssec:eigencrust_smoothing}

\begin{figure}[t]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/sample_smoothing_results}}
\end{minipage}

\caption[Smoothing wall samplings.]{Average filtering reduces occurrence of registration errors exposed in wall sampling.}
\label{fig:sample_smoothing_results}

\end{figure}

A crucial step in generating a point cloud is to register the poses of separate scan positions to a common coordinate system.  Any error in this process may result in duplicate instances of a scanned wall with slight misalignments.  A given wall position may be scanned from multiple disjoint poses, causing it to appear multiple times in the set of generated wall samples.  If the registration error is significant smoothing may be required to force these wall instances to become aligned~\cite{Turner12}.

For a model with wall samples set $S_k$, a given sample $s \in S_k$ with normal vector $\vec{n}$ has a smoothing set that is a subset of $S_k$ within an anisometric neighborhood of $s$.  The position of $s$ is translated to be the mean position of this smoothing set. The smoothing set contains all samples that are (a) within some smoothing distance of $s$ along the $\pm \vec{n}$ direction, (b) no more than $3 d_s$ away from $s$ in the direction orthogonal to $\vec{n}$, and (c) whose normals are aligned to within $\frac{\pi}{8}$ radians of $\vec{n}$.  The smoothing distance must be less than the minimum allowable spacing between parallel walls, but at least as large as the expected registration error.  This step is not necessary if registration error is smaller than $d_s$.  An example of this smoothing process is shown in Figure~\ref{fig:sample_smoothing_results}.

\subsection{Labeling Triangulation of Wall Samples}
\label{ssec:eigencrust_triangulation}

We wish to determine the topology of the 2D sample set $S_k$ for each story $k \in \{ 0, 1, ..., f-1 \}$.  This task is accomplished using a 2D variant of the Eigencrust algorithm \cite{EigencrustShewchuk}.  This will partition 2D space into regions labeled ``inside'' and ``outside'' and export the borders between these regions.  These borders form the faces of a 2D simplicial complex, which are guaranteed to be watertight and not self-intersecting.  Eigencrust has been shown to be more robust to outlier samples than similar algorithms, such as Powercrust \cite{EigencrustShewchuk, Powercrust}.

\begin{figure}[t]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{3dimpvt2012/eigencrust/intersect_angles}}
\end{minipage}

\caption[Computing weighting between triangle pairs.]{The angles of intersection between two triangles $u$ and $v$, which share the edge $\overline{s_1 s_2}$. These values are used in computing weights between triangle pairs.}
\label{fig:intersect_angles}

\end{figure}

The Eigencrust algorithm first computes the 2D Delaunay Triangulation, $T$, for the sample set $S_k$ plus four bounding box corners.  Eigencrust constructs a sparse graph where the nodes represent triangles and edges are placed between the nodes with weights corresponding to the relative geometry of the triangles.  Edges with positive weights indicate that triangles should have the same labeling, while negative weights indicate that the triangles connected should be labeled oppositely.  Generalized eigensystems are solved in order to determine the best triangle labelings to fit these connections.  For our 2D varient of Eigencrust, we keep the same criteria for placing edge weights as in \cite{EigencrustShewchuk}, but modify the negative edge weight values.  We have additional information of normal vectors for each sample location. If a negative edge is placed between two triangles $u$ and $v$, we use the weighting:

\begin{equation}
w_{u,v} = - e ^ {4 + 4 cos \phi + 2 sin \theta_1 + 2 sin \theta_2}
\label{neg_edge_weight}
\end{equation}

where these parameters are shown in Figure~\ref{fig:intersect_angles}.  This weighting is very close to the one used in \cite{EigencrustShewchuk}.  The value $\phi$ is defined as the angle at which the circumcircles of $u$ and $v$ intersect.  The intersection of the triangles $u$ and $v$ is a line segment whose endpoints are samples $s_1$ and $s_2$, which have normal vectors $\vec{n}_1$ and $\vec{n}_2$ respectively.  The values of $\theta_1$, $\theta_2$ are defined to be the angles between $\overline{s_1 s_2}$ and $\vec{n}_1$, $\vec{n}_2$, respectively.  This weight is defined to have a large magnitude when both the normal vectors are perpendicular to the line $\overline{s_1 s_2}$, which is a strong indication of a wall in the original point cloud $P_k$.

\begin{figure}[t]

\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{3dimpvt2012/eigencrust/BHH-Lobby_wall_sampling_zoom}}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{3dimpvt2012/eigencrust/BHH-Lobby_triangulation_zoom}}
  \centerline{(b)}
\end{minipage}

\caption[Example triangulation of wall samples.]{An example triangulation of wall samples: (a) a set of wall samples for a column; (b) each triangle is denoted as ``inside'' (yellow) or ``outside'' (green).}
\label{fig:eigencrust_triangulation}

\end{figure}

For each pair of samples $s$,$s' \in S_k$ such that ($s$,$s'$) is an edge of the Delaunay Triangulation $T$, let $u$ and $v$ be the poles of $s$, and $u'$ and $v'$ be the poles of $s'$.  Each of the pairs ($u$, $u'$), ($u$, $v'$), ($v$, $u'$), and ($v$, $v'$) are edges in $E$.  If a negative edge is not already defined, each of these pairs has positive edge weight:

\begin{equation}
w_{u,u'} = e ^ {4 - 4 cos \phi}
\label{pos_edge_weight}
\end{equation}

where $\phi$ is again defined as the angle at which the circumcircles of $u$ and $u'$ intersect~\cite{EigencrustShewchuk}.  Next, we can constrain some triangles to be interior or exterior based on a priori information.  The label ``inside'' refers to locations of open area within a building where a person can exist.  The term ``outside'' refers to all other areas, which include both the exterior of the building as well as the areas inside walls or other solid spaces.  We can immediately force the label of ``outside'' to any triangles connected to the bounding box corner vertices.  If the pose information for the scanner is available, we can force the label of ``inside'' to a subset of triangles.  First, we investigate whether the 2D line-of-sight from a scanner pose to a scan sample crosses triangles.  If the center 50\% of this laser travel distance crosses a triangle, that triangle is intersected by that scan line.  If a triangle is intersected by 10 or more scan lines, it is assumed to be ``inside''.  Second, all triangles that contain a pose position of a scanner are marked as interior.  If a mobile scanning system is used, then the path traversed by that system can also be used to mark interior triangles.  Each pair of adjacent pose positions represents a line segment in 2D space.  If both of these poses contributed scans to the wall sample set $S_k$, then any triangles that are intersected by this line segment are also constrained to be interior.

The remaining triangles are labeled inside or outside by solving generalized eigenvalue systems that minimizes the total positive edge weights that connect oppositely labeled triangles and negative edge weights that connect triangles with the same labeling~\cite{EigencrustShewchuk}.

The non-pole triangles are then put into a corresponding graph.  Since the edge-weights of this non-pole graph in the 3D Eigencrust algorithm use the aspect ratio of bounding triangular faces, there is not a direct translation of these weight calculations to 2D.  Thus the same weights are used as for the pole graph $G$, defined in Equations~\ref{neg_edge_weight} and~\ref{pos_edge_weight}.  A corresponding generalized eigenvalue system is solved for this non-pole graph, which provides a labeling for all triangles in $T$.  An example output of this triangulation labeling process is shown in Figure~\ref{fig:eigencrust_triangulation}.

All the ``outside'' nodes are collapsed into one outside super-node and all ``inside'' nodes are collapsed into one inside super-node, creating graph $G'$.  The nodes and edges of $G'$ are then converted into a matrix $L \in \mathbb{R}^{|G'| \times |G'|}$.  Each element is defined as $L_{ij} = L_{ji} = -w_{i,j}$ with the diagonal $L_{ii} = \sum_{j \neq i} |L_{ij}|$.  The diagonal matrix $D$ is the same size as $L$ and has the same diagonal elements as $L$.  These values are used to find the solution to the generalized eigensystem $L x = \lambda D x$.  The eigenvector $x$ associated with the smallest non-zero eigenvalue $\lambda$ is taken as the result~\cite{EigencrustShewchuk}.

Each element of $x$ corresponds to a pole triangle in $T$.  The $i$'th triangle is labeled as ``inside'' or ``outside'' based on the sign of $x_i$.  This step partitions the poles into the inside/outside categories.  To label all the non-poles, we construct another graph.  All the ``outside'' poles in $G'$ are collapsed into one outside super-node and all the ``inside'' poles of $G'$ are collapsed into one inside super-node.  All the non-pole triangles are also nodes of this non-pole graph, and edges are defined between every pair of touching triangles, assuming both triangles are not part of the same super-node. Any edge between the two super-nodes are negatively weighted according to Equation~\ref{neg_edge_weight}.  Any edge involving at least one non-pole is positively weighted according to Equation~\ref{pos_edge_weight}.  The non-pole graph is then converted to the matrix $H$ in the same manner as described above for $G'$.  Let $D_H$ be the diagonal component of the matrix $H$.  Solving the concordant generalized eigensystem $H y = \mu D_H y$ will yield the eigenvector $y$, which gives us the inside/outside labelings for each of the non-pole triangles as well~\cite{EigencrustShewchuk}. An example output of this triangulation process is shown in Figure~\ref{fig:eigencrust_triangulation}.

\subsection{Forming Floor Plan Boundary Edges}
\label{ssec:eigencrust_boundary}

\begin{figure}[t]

\begin{minipage}[b]{0.49\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/eigencrust/eigencrust_cleaning_1}}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.49\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/eigencrust/eigencrust_cleaning_3}}
  \centerline{(b)}
\end{minipage}

\caption[Post-processing floor plan triangulation.]{Original triangulation labeling (a) is cleaned by removing small unions and sharp protrusions (b).}
\label{fig:eigencrust_postprocessing}

\end{figure}

We have found the following post-processing clean-up techniques useful for increasing the quality of the output model.  First, we represent this topology of triangles as a set of unions.  Any subset of triangles with the same label that forms a connected component is represented as a single union using the Union-Find algorithm \cite{Unionfind}.  Unions that are composed of a small number of triangles are most likely mislabeled, since the smallest building features should still be sufficiently sampled.  The appropriate cut-off value depends on the value of $d_s$ used to form samples, but a value of 30 triangles is typically used.  All inside unions that have fewer than this many triangles are relabeled as outside. The set of unions is recomputed and then all outside unions that meet this criterion are relabeled as inside. Another post-processing smoothing process is to remove jagged edges from the boundaries between labelings.  Every triangle has three neighboring triangles, each sharing one of its edges.  If a triangle $t \in T$ has only one neighbor that shares the same labeling and that neighbor is connected to $t$'s shortest edge, then $t$ is considered to be a protrusion.  It is unlikely for building geometry to be correctly represented by such a protrusion, so all protrusions labeled as inside are relabeled as outside. After this relabeling, all outside protrusions are found and relabeled as inside.  An example of this processing is shown in Figure~\ref{fig:eigencrust_postprocessing}.

The set of edges in $T$ that are shared by an ``inside'' triangle and an ``outside'' triangle are exported as boundary edge set, $B$.  Each element in $B$ is a line segment, which in total constitute a water-tight floor plan of the scanned area.

\subsection{Fitting Edges With Lines and Curves}
\label{ssec:eigencrust_fitting}

Since there may exist areas in the building geometry that are insufficiently scanned or errors incurred during processing, it is important to utilize common architectural patterns to reduce these errors.  Additionally, applications may require a floor plan to be composed of parametric lines and curves.

Local model-fitting approaches such as region growing are sub-optimal because unconnected architectural features may use the same geometric model.  For example, the back walls in a row of offices may lie on the same plane, even though they are in different rooms. Since we need to fit both curves and straight lines simultaneously, a reasonable technique for this situation is one that is non-local and flexible, such as Random Sample Consensus (RANSAC)~\cite{Ransac}.

We apply RANSAC to the subset of samples used as vertices in the boundary edges $B$.  Each iteration randomly picks three samples from this set that have not yet been associated with a model.  These three points uniquely define a circle.  A line of best fit can also be obtained for these points.  Both the circle and the line models are compared to the subset of samples still unassigned.  Inlier sets for both of these models are computed.  An inlier is an unassigned sample that is both within a threshold distance of a model and whose normal vector is within a threshold angle to the model's normal vector at that location.  Only models that exceed a specified minimum number of inliers are considered.  The line or circle model found with the smallest average error is returned as a valid model and its inlier vertices are no longer considered for subsequent models.

\begin{figure}[t]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[height=5.5cm]{3dimpvt2012/curvefit/circle_segments_example_highlighted}}
\end{minipage}

\caption[Fitting curves to floor plan walls.]{The walls of a circular room are fit to the same circle model (shown highlighted in teal), even though they are comprised of several connected components. }
\label{fig:eigencrust_circle_segments_example}

\end{figure}

\begin{figure}[t]

\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/curvefit/backpack_cory_three_1_final_precurvefit}}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/curvefit/backpack_cory_three_1_final_curvefit}}
  \centerline{(b)}
\end{minipage}

\caption[Sharpening floor plan corners with RANASC.]{(a) Watertight wall edges; (b) curve-fitting via RANSAC reduces sample location error and sharpens corners.}
\label{fig:eigencrust_wall_straightening}

\end{figure}

This process continues until no new parametric model can be found.  The result is a set of models, where each model has a set of inlier samples and a parametric representation of either a line or a circle.  The topology defined in $B$ has not been altered, so its edge elements can be used to partition the inlier vertices of each model into a set of connected components, again with Union-Find~\cite{Unionfind}.  Any of these connected components composed of too few samples is most likely a misclassification.  Thus the elements of any of these components with fewer than 15 samples are reset to be unassigned.  Additionally, we wish to encourage models to extend along the edges defined in $B$.  If an unassigned sample is within 15 edge hops to samples belonging to a model, that sample is associated with the closest model.  These two steps encourages outlier samples to belong to models that are topologically close.  These steps also grow models to be adjacent, encouraging sharp corners.

Once the revised parametric models and their respective inliers are computed, the inlier samples and edges in $B$ are replaced by edges that conform exactly to their models' locations.  This process reduces the overall number of samples and removes small perturbation errors from the floor plan.  Figure~\ref{fig:eigencrust_circle_segments_example} demonstrates the results of this process, which fits a circle to the walls of a round room with several entrances. Figure~\ref{fig:eigencrust_wall_straightening} shows how this process can also be applied to straight walls.

\subsection{Conclusion}
\label{ssec:eigencrust_conclusion}

This section details a technique to generate 2D architectural floor plans.  By projecting each story's point set to two dimensions and performing a density analysis, we can yield an increase in wall location accuracy than by using a direct 3D approach. In Section~\ref{sec:fp_results}, we show several example models generated using this technique, and compare these models both to the ground-truth blueprints of the building as well as to other automatic as-built floor plan generation techniques.

\section{Floor Plan Generation with Room Partitioning}
\label{sec:visigrapp}

In this section, we present a technique to automatically generate accurate floor plan models at real-time speeds for indoor building environments~\cite{Turner14}.  In Section~\ref{ssec:visigrapp_carving}, we discuss how we compute the interior space of the 2D floor-plan, which defines the resultant building geometry.  The partitioning of this interior space into separate rooms is then considered in Section~\ref{ssec:room_label}.  Using these room labels, the geometry can be simplified, as described in Section~\ref{ssec:fp_simplify}.  In Section~\ref{ssec:fp_extrusion}, the final 2D geometry is extruded into an exported 3D model.

\subsection{Defining Interior Area}
\label{ssec:visigrapp_carving}

% carving a floor plan
\begin{figure}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{visigrapp/algorithm/carving/carving_full_figure}
  \caption[Carving process to find interior triangles.]{Example of carving process to find interior triangles:  (a) wall samples (in blue) with path of scanner (in green); (b) Delaunay Triangulation of wall samples; (c) laser scans from each pose (in red); (d) triangles that intersect with laser scans (in pink), used as interior triangles, with building model border (in blue).}
  \label{fig:visigrapp_floorplan_creation}
\end{figure}

% introduction
We generate a floor plan by partitioning space into {\it interior} and {\it exterior} domains.  The interior space of the floor plan is computed using the input wall samples.  The desired output consists of not only the architectural features such as walls, but also a volumetric representation of the environment such as rooms and hallways.  The exterior represents all space outside of the building, space occupied by solid objects, or space that is unobservable.  Once this partitioning is completed, as described below, the boundary lines between the interior and exterior are used to represent the exported walls of the floor plan.

% carving process using pose positions  
The input samples are used to define a volumetric representation by generating a Delaunay Triangulation in the plane.  Each triangle is labeled either interior or exterior by analyzing the line-of-sight information of each wall sample.  Initially, all triangles are considered exterior.  Each input wall sample, $p \in P$, is viewed by a set of scanner positions, $S_p \subseteq \mathbb{R}^2$.  For every scanner position $s \in S_p$, the line segment $(s,p)$ denotes the line-of-sight occurring from the scanner to the scanned point during data collection.  No solid object can possibly intersect this line, since otherwise the scan would have been occluded.  Thus, all triangles intersected by the line segment $(s,p)$ are relabeled to be interior.

In order to prevent fine details from being removed, we check for occlusions when carving each line segment $(s,p)$.  If another wall sample $p'$ is located in between the positions of $s$ and $p$, then the line segment is truncated to $(s,p')$.  Thus, no features captured by wall samples are ever fully carved away, preserving environment details.  This process carves away the interior triangles with each captured scan.  Since these scans are captured on a mobile scanner, the scanner poses are ordered in time.  In order for the system to traverse the environment, the line segment between adjacent scanner poses must also intersect only interior space.  In addition to carving via scanner-to-scan lines, the same carving process is performed with scanner-to-scanner line segments.

Figure~\ref{fig:visigrapp_floorplan_creation} demonstrates an example of this process.  Figure~\ref{fig:visigrapp_floorplan_creation}a shows the input wall samples, in blue, as well as the path of the mobile mapping system, in green.  These points are triangulated, as shown in Figure~\ref{fig:visigrapp_floorplan_creation}b.  The line-of-sight information is analyzed from each pose of the system, demonstrated by the laser scans from each pose to its observed wall samples in Figure~\ref{fig:visigrapp_floorplan_creation}c.  The subset of triangles that are intersected by these laser scans are considered interior.  The interior triangles are shown in pink in Figure~\ref{fig:visigrapp_floorplan_creation}d, denoting the interior volume of the reconstructed building model.  The border of this building model is shown in blue, denoting the estimated walls of the floor plan.

% checking for occludiing wall samples
The above method may result in errors if there is misregistration in the localization of the poses of the scanners during the data collection.  For instance, if opposite sides of the same wall are scanned, this wall appears in the output with some thickness, $t$.  If the estimated positions of the scanners on either side of the wall are inaccurate, the observed wall thickness varies by the component of this localization error orthogonal to the wall plane.  If this error is greater than $t$, then every triangle associated with the wall may be labeled interior, and the wall is carved away entirely from the output.  In order to prevent these topological errors, we check for occlusions when carving each line segment $(s,p)$.  If another wall sample $p'$ is located in between the positions of $s$ and $p$, then the line segment is truncated to $(s,p')$.  Thus, no features captured by wall samples are ever fully carved away, which preserves the details of the environment.

% room labeling section
\subsection{Labeling Rooms Within the Floor Plan}
\label{ssec:room_label}

A novel contribution of this technique is the use of room labeling to enhance building models, e.g.\ for thermal simulations of interior environments~\cite{Turner14,EnergyPlus}.  One motivation for existing work has been to capture line-of-sight information for fast rendering of building environments~\cite{WalkthroughRendering}.  This technique requires axis-aligned rectilinear building geometry, which often is not a valid assumption.  Others have partitioned building environments into submap segments with the goal of efficient localization and tracking~\cite{SpectralClustering}.  This approach is meant to create easily recognizable subsections of the environment, whereas our proposed room labeling technique uses geometric features to capture semantic room definitions for both architectural and building energy simulation applications.

% figure showing room seed identification
\begin{figure}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{visigrapp/algorithm/room_seeds/room_seeds_full_figure}
  \caption[Example room seed partitioning.]{Example room seed partitioning: (a) interior triangulation; (b) the room seed triangles, and their corresponding circumcircles; (c) room labels propagated to all other triangles.}
  \label{fig:roomlabeling}
\end{figure}

Once the volume has been partitioned into interior and exterior domains, the boundary between these domains can be exported as a valid floor plan of the environment.  Keeping volumetric information can also yield useful information, such as a partitioning of the interior into separate rooms.

% definition of a room
We define a {\it room} to be a connected subset of the interior triangles in the building model.  Ideally, a room is a large open space with small shared boundaries to the rest of the model.  Detected rooms should match with real-world architecture, where separations between labeled rooms are located at doorways in the building.  Since doors are often difficult to detect, or not even present, there is no strict mathematical definition for a room, so this labeling is heuristic in nature.

We model room labeling as a graph-cut problem.  First, a rough estimate for the number of rooms and a seed triangle for each room is computed.  A seed triangle is representative of a room, where every room to be modeled has one seed triangle.  These seeds are used to partition the remainder of interior triangles into rooms.  This process typically over-estimates the number of rooms, so prior knowledge of architectural compliance standards is used to evaluate each estimated room geometry.  Using this analysis, the number of ill-formed rooms is reduced, providing an update on the original seed points.  This process is repeated until the set of room seeds converges.

%\subsubsection{Forming Room Seeds}
%\label{sssec:room_seeds}

We use the Delaunay property of the triangulation to identify likely seed triangle locations for room labels, which states that no vertex is strictly inside the circumcircle of any triangle in this triangulation.  If we assume that the input wall samples represent a dense sampling of the building geometry, this property implies that the circumcircles of none of the interior triangles intersect the boundary walls of the carved floor plan, forcing these circles to represent only interior area.  This make-up allows each triangle's circumradius to provide an estimate of the local feature size at its location on the floor plan boundary polygon.  Given the example interior triangulation shown in Figure~\ref{fig:roomlabeling}a, the highlighted triangles in Figure~\ref{fig:roomlabeling}b show the chosen seed locations.

Triangles with larger circumradii are likely to be more representative of their rooms than those with smaller circumradii.  We form the initial set of room seeds by finding all triangles whose circumcircles are local maxima.  Specifically, given the set of interior triangles $T$, each triangle $t \in T$ has circumcircle $c_t$, which is tested against every other circumcircle in $T$ that is intersected by $c_t$.  If $c_t$ has the largest radius of any intersecting circumcircle, then $t$ is considered a seed for the room labeling.  This process selects the largest triangles that encompass the space of rooms as the seeds for room labeling.  Figure~\ref{fig:roomlabeling}b shows example seed triangles and their corresponding circumcircles.  The result is an estimate of the number of rooms and a rough location for each room.

%\subsubsection{Partitioning Room Labels}

% refining and combining rooms
\begin{figure}
  \centering
  \includegraphics[width=0.75\linewidth]{visigrapp/algorithm/room_merging/room_merging_full_figure}
  \caption[Room labeling refinement example.]{Room labeling refinement example: (a) initial room labels; (b) converged room labels}
  \label{fig:roommerging}
\end{figure}

Let $K$ be the number of room seeds found, with the seed triangles denoted as $t_1,\,t_2,\,...,\,t_K$.  We wish to partition all triangles in $T$ into $K$ rooms.  This step can be performed as a graph-cut on the dual of the triangulation.  Specifically, each triangle $t \in T$ is a node in the graph, and the edge weight between two abutting triangles is the length of their shared side.  Performing a min-cut on this graph partitions rooms to minimize inter-room boundary length.  In other words, rooms are defined to minimize the size of doors.  This process propagates the room labels to every triangle, and the boundaries between rooms are composed of only the smallest edges in the triangulation $T$.  The result of this process is shown in Figure~\ref{fig:roomlabeling}c.

A greedy algorithm can alternatively be employed to flood-fill rooms.  To do so, we initialize the labeling so that each seed triangle $t_i$ has a unique label, while all non-seed triangles are unlabeled.  We then construct a priority queue of all edges in $T$ that separate a labeled triangle from an unlabeled triangle, sorted by decreasing edge length.  Iterating through this queue, the top value represents the longest edge in the queue, adjoining a labeled triangle $u \in T$ and unlabeled triangle $v \in T$.  We then pop this edge, propagate the label from $u$ to $v$, then push all edges of $v$ adjoining unlabeled triangles onto the queue.  

%\subsection{Refining Rooms}
%\label{sec:refiningrooms}

Room labels partition $T$ into a set of rooms $R = \{R_1,\,R_2,\,...,\,R_K\}$, where each room $R_i$ contains a disjoint subset of $T$ and has seed triangle $t_i$.  The initial room seeds over-estimate the number of rooms, since a room may have multiple local maxima.  This case is especially true for long hallways, where the assumption that one triangle dominates the area of the room is invalid.  An example is shown in Figure~\ref{fig:roomlabeling}c, where two lower rooms, shown in green and purple, are properly labeled, but their adjoining hallway is broken into three subsections.  The solution is to selectively remove room seeds and redefine the partition.

A room is considered a candidate for merging if it shares a large perimeter with another room.  Ideally, two rooms sharing a border too large to be a door should be considered the same room.  By Americans with Disabilities Act Compliance Standards, a swinging door cannot exceed 48 inches in width~\cite{ADACompliance}.  Accounting for the possibility of double-doors, we use a threshold of 2.44 meters, or 96 inches, when considering boundaries between rooms.  If two rooms share a border greater than this threshold, then the seed triangle with the smaller circumradius is discarded.  This process reduces the value of $K$, the number of rooms, while keeping the interior triangulation $T$ unchanged.  With a reduced set of room seeds, existing room labels are discarded and the process of room partitioning is repeated.  This iteration repeats until the room labeling converges.

Another way room labels are refined is by comparing the path of the mobile mapping system to the current room labeling for each iteration.  The mobile scanning system does not necessarily traverse every room, and may only take superficial scans of room geometry passing by a room's open doorway.  Since the room is not actually entered, the model is unlikely to capture sufficient geometry, and so only a small handful of wall samples are acquired for such a room.  It is desirable to remove this poorly scanned area from the model rather than keeping it as part of the output.  After each round of room partitioning, if none of the triangles in a room $R_i$ are intersected by the scanner's path, then we infer that room has not been entered.  The elements of $R_i$ are removed from the interior triangulation $T$.  Since the topology of the building model is changed, the set of room seeds is recomputed in this event and room labeling is restarted.  This process will also remove areas that are falsely identified as rooms, such as ghost geometry generated by windows and reflective surfaces, which cause rooms to be replicated outside the actual model.

Figure~\ref{fig:roommerging} shows an example of the room refinement process for the hallways and classrooms in an academic building. Figure~\ref{fig:roommerging}a shows the initial room seeds that were found based on circumcircle analysis.  The hallways of this building are represented by several room labels, but after room label refinement as shown in Figure~\ref{fig:roommerging}b, the hallways are appropriately classified.  Additionally, rooms that are insufficiently scanned and represented with triangulation artifacts are removed from the model in the manner described above.

% simplification of floor plan geometry
\subsection{Simplification of Floor Plan Geometry}
\label{ssec:fp_simplify}

The interior building model is represented as a triangulation of wall samples, which densely represent the building geometry.  In many applications, it is useful to reduce the complexity of this representation, so that each wall is represented by a single line segment.  This step is often desirable in order to attenuate noise in the input wall samples or to classify the walls of a room for application-specific purposes.  The goal is to simplify the wall geometry while preserving the general shape and features of the building model.

We opt to simplify walls using a variant of QEM~\cite{QEM,Turner14}.  Since this mesh is in the plane, only vertices incident to the model boundary are considered for simplification.  The error matrix $Q_v$ of each boundary vertex $v$ is used to compute the sum of squared displacement error from each adjoining line along the boundary polygon.  Since error is measured via distance away from a line in 2D, each $Q_v$ has size $3 \times 3$, and is defined as:

\begin{equation}
Q_v = \sum_{l \in lines(v)} E_l
\end{equation}

where $E_l$ is defined from the line equation $ax + by + c = 0$, with $a^2 + b^2 = 1$:

\begin{equation}
E_l = \left[ \begin{array}{c c c}
a^2 & ab & ac \\
ab & b^2 & bc \\
ac & bc & c^2 \end{array} \right]
\end{equation}

The simplification of the boundary proceeds in a similar manner to QEM, but if a wall vertex $v$ is contained in multiple rooms or if it is connected by an edge to a vertex that is contained in multiple rooms, then it is not simplified.  This constraint is used to preserve the fine details of doorways between rooms, while freely simplifying walls that are fully contained within one room.  Wall edges are iteratively simplified until no simplification produces error of less than the original wall sampling resolution, $r$.  Thus, walls are simplified while preserving any geometry features of the building interior.

Since we are interested in preserving the 2D triangulation $T$ of the building model, in addition to the boundary polygon, every edge simplification is performed by collapsing an interior triangle.  This computation simplifies the boundary polygon of the model while still preserving the room labeling of the model's volume.  These triangle collapses do not preserve the Delaunay property of the triangulation, but do preserve the boundaries between room volumes, which is more desirable in the output.

We can compare this method of simplification to the RANSAC fitting described in the previous method, in Section~\ref{sec:eigencrust}.  By using local simplification, each modification to the floor plan geometry only requires access to a small number of elements, allowing for rapid simplification.  With the RANSAC method, each adjustment is very expensive, since it must iterate several times over the entire set of input wall samples.  The one limitation to the QEM simplification method is that it does not support circular or curved surfaces.  This limitation, however, is reasonable, since curved features can be approximated with a set of piecewise-planar walls.

% floor plan height extrusion
\subsection{Extruding Floor Plan Geometry to 2.5D Models}
\label{ssec:fp_extrusion}

% example simplification and height extrusion
\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{visigrapp/algorithm/height_extrusion/height_extrusion_full}
  \caption[Example of creating a 3D extruded mesh from 2D wall samples.]{Example of creating a 3D extruded mesh from 2D wall samples:  (a) walls of generated floor plan with estimated height ranges; (b) floor and ceiling heights are grouped by room; (c) simplification performed on walls; (d) floor and ceiling triangles added to create a watertight mesh.}
  \label{fig:heightextrusion}
\end{figure}

As mentioned in Section~\ref{sec:wall_sampling}, each input wall sample also references the vertical extent for the observed scans at that location.  This information can be used to convert the labeled 2D interior building model to a 2.5D extruded model, by using the minimum and maximum height values for each scan as an estimate of the floor and ceiling heights, respectively~\cite{Turner14}.

Since these wall samples are collected using 2D planar scanners in an environment containing clutter, the minimum and maximum heights associated with each point are noisy.  Figure~\ref{fig:heightextrusion}a shows an example room with these initial heights.  To produce aesthetically-pleasing models, each room uses a single floor height and a single ceiling height.  This assumption is reasonable since the goal of this processing is to produce a simplified building mesh.  This step demonstrates the utility of room labeling to modeling.  The height range for each room is computed from the median floor and ceiling height values of that room's vertices.  An example is shown in Figure~\ref{fig:heightextrusion}b and the corresponding result from the simplification process from Section~\ref{ssec:fp_simplify} is demonstrated in Figure~\ref{fig:heightextrusion}c.

The 2D triangulation of a room is then used to create the floor and ceiling mesh for that room, with the boundary edges of the triangulation extruded to create rectangular vertical wall segments.  The result is a watertight 3D mesh of the building, capturing the permanent geometry in an efficient number of triangles.  Figure~\ref{fig:heightextrusion}d shows an example of this watertight extruded geometry, including the effects of wall boundary simplification on the resulting extruded mesh.

\section{Results}
\label{sec:fp_results}

% TODO add results for eigencrust
% TODO add results for visigrapp
% TODO compare pf and pc floorplans

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Complex 3D Model Generation
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Complex 3D Model Generation}
\label{ch:carving}

% TODO chapter on carving

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Improving floorplans with carving
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Improvements of 2D Floor Plan Models using Complex 3D Models}
\label{ch:better_floorplans}

% TODO chapter on better floorplans

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Improving carving with floorplans
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Improvements of 3D Complex Models using 2D Floor Plan Models}
\label{ch:better_floorplans}

% TODO chapter on better carving

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Applications
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Applications of Building Models}
\label{ch:applications}

% TODO chapter on applications

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Conclusion
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Conclusions}
\label{ch:conclusion}

% TODO chapter on conclusion



% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
\bibliography{elturner}
\vfill

% that's all folks
\end{document}
