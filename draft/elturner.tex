%\documentclass[journal]{IEEEtran}
%\documentclass[11pt,draftcls,onecolumn]{IEEEtran}
%\documentclass[10pt,twocolumn,twoside]{report}
\documentclass[12pt,onecolumn,oneside]{book}
\usepackage[margin=1.0in]{geometry}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% *** HEADER/PAGE NUMBERING RELATED PACKAGES ***
\usepackage{blindtext}% adds the blindtext
%\usepackage{showframe}% shows a framed header, type area and footer
\setlength{\headheight}{1.1\baselineskip}
%\setlength{\headsep}{10pt}

% *** GRAPHICS RELATED PACKAGES ***
\usepackage{float}
\usepackage{placeins}
%\usepackage[demo,pdftex]{graphicx}  % compiles really fast, but show figs as black
\usepackage[pdftex]{graphicx}  % actually include full figures

% declare the path(s) where your graphic files are
\graphicspath{{../figures/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpg,.jpeg,.png}

% *** MATH PACKAGES ***
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\interdisplaylinepenalty=2500

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=blue
}

% Force headers and page numbers to always be on the upper right
\usepackage[nouppercase,automark]{scrpage2}
\clearscrheadfoot
\pagestyle{scrheadings}
%See the KOMA-script documentation for more information
\ihead{\rightmark}
%\ihead[]{\headmark}%helpful for two-sided printing
\ofoot[\pagemark]{\pagemark}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% begin document with roman page numbering
\begin{document}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% TITLE
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
\pagenumbering{gobble}

% the title and author page has very special requirements
%
% see:  http://grad.berkeley.edu/wp-content/uploads/title-page.pdf
%
% For all formatting requirements, see:
%
% http://grad.berkeley.edu/academic-progress/dissertation/#formatting-your-manuscript

\newcommand{\mytitle}{3D Modeling of Interior Building Environments and Objects from Noisy Sensor Suites}
\title{\mytitle}

\newcommand{\myauthor}{Eric~Lee~Turner}
\author{\myauthor}

{\centering
	{\textbf{\mytitle}}\\
	\hfill \break
	\hfill \break
	By\\
	\hfill \break
	\myauthor\\
	\hfill \break
	\hfill \break
	\hfill \break
	A dissertation submitted in partial satisfaction of the\\	
	\hfill \break
	requirements for the degree of\\
	\hfill \break
	Doctor of Philosophy\\
	\hfill \break
	in\\
	Engineering -- Electrical Engineering and Computer Sciences\\	
	\hfill \break 
	in the\\
	\hfill \break
	Graduate Division\\
	\hfill \break
	of the\\
	\hfill \break
	University of California, Berkeley\\
	\hfill \break
	\hfill \break
	\hfill \break
	Committee in charge:\\
	\hfill \break
	Professor Avideh Zakhor, Chair\\
	Professor Jonathan Shewchuk\\
	Professor Kyle Steinfeld\\
	\hfill \break	
	\hfill \break
	Spring 2015\\
}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% COPYRIGHT
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\newpage
{\centering
	{\textbf{\mytitle}}\\
	\hfill \break
	\hfill \break
	\hfill \break
	\hfill \break
	\hfill \break
	Copyright \copyright \, 2015\\
	by\\
	\myauthor\\
}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% ABSTRACT
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the abstract uses arabic page numbering and has its own counter
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

% header for abstract
{\centering
	{\textbf{Abstract}}\\
	\hfill \break
	\mytitle\\
	\hfill \break
	by\\
	\hfill \break
	\myauthor\\
	\hfill \break
	Doctor of Philosophy in Electrical Engineering and Computer Sciences\\
	\hfill \break
	University of California, Berkeley\\
	\hfill \break
	Professor Avideh Zakhor, Chair\\
	\hfill \break
}

%\begin{abstract}
In this dissertation, we present several techniques used to automatically generate virtual models of indoor building environments.  The interior environment of a building is scanned using our custom hardware system, which provides a set of data products used to develop these models.  Our modeling techniques can be separated into three categories:  2D floor plan models, simplified 2.5D extruded models, and fully complex and detailed 3D models.  All approaches are produced automatically from the output data of our backpack-mounted ambulatory scanning system, which can scan multiple floors of a building efficiently.  We are capable of capturing the entirety of large buildings in only a few hours.  This system is capable of producing many kinds of 3D virtual building models while traversing through a GPS-denied environment, including point clouds, 2D floor plans, and 3D building models.

The novel contributions of this dissertation can be sorted into three groups.  First, we present multiple methods for producing 2D floor plans across all of the building environment.  We automatically partition these floor plans into separate building levels and separate rooms within each level.  These floor plans can also be extruded into simplified 2.5D models of the building environment.  Second, we present multiple techniques to produce complex 3D models of the scanned environment, capturing all observed detail.  Third, we present several techniques that combine these two types of building models -- simple and complex -- in order to perform additional analysis of the building environment.  Such analysis includes segmenting objects and furniture of the environment as separate models, reducing noise and artifacts within the models, and demonstrating novel visualization techniques.  Additionally, we show several results of the system in real-world environments, including buildings of over 40,000 square feet.  We demonstrate how such building models are applicable to many fields of study, including architecture, building energy efficiency, virtual walk-throughs of buildings, indoor navigation, augmented and virtual reality.
%\end{abstract}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% FRONT MATTER:  dedication, table of contents, list of figures, acknowledge
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{roman}
\setcounter{page}{1}

% dedication page
\clearpage
\vspace*{\fill}
\begin{center}
To Mom.
\end{center}
\vfill % equivalent to \vspace{\fill}
\clearpage

% table of contents
\addcontentsline{toc}{chapter}{Table of Contents}
\tableofcontents{}

% list of figures
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures{}

% list of tables
%\cleardoublepage
%\phantomsection
%\addcontentsline{toc}{chapter}{\listtablename}
%\listoftables{}

% acknowledgement section
\newpage
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Acknowledgements}
\chapter*{Acknowledgements}

% Berkeley is an awesome place
As soon as I saw University of California, Berkeley, I knew I would love it here.  A campus cut with forested paths and narrow streams seemed straight out of a Calvin \& Hobbes strip.  As soon as I met the people here, I immediately found that the intellectual adventure matched the aesthetics.

% Avideh and thesis committee
I first want to thank my advisor, Professor Avideh Zakhor.  The potential she sees in her students becomes manifest with each challenge she places before them.  My understanding of the field and of academics in general has become so much richer thanks to her guidance.  I also would like to thank Jonathan Shewchuk, Kyle Steinfeld, and Carlo Sequin, who served on my qualifying exam and dissertation committees.  These are some of the best professors I've ever met, and have provided me with excellent guidance both in the classroom and in my committee.

% funding
This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

% lab folks
I also want to extend my deep gratitude to all my colleagues in the Video and Image Processing Lab.  John Kua, Michael Krishnan, Rick Garcia, Shangliang Jiang, Nicholas Corso, Shicong Yang, Richard Zhang, and Joe Menke have all lent me guidance and helped me through.  I especially want to mention Nicholas, who ensured that I took a daily constitutional around campus rather than hiding away in our lab.

% family and sam
Additionally, I want to thank my family.  Even though I moved to the other side of the country, they have always been supportive.  My Dad, David Turner, has not only been my role model of an engineer, but truly comforting at every step of my way through school.  Lastly, I want to thank Samantha Shropshire, my girlfriend.  For putting up with my working at odd hours, for supporting me, and for bouncing ideas off of, her patience knows no bounds.  Sam, without you, I would never have survived graduate school.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Introduction
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{Introduction}
\label{ch:introduction}

% motivation text
Laser scanning technology is becoming a vital component of building construction and maintenance.  During initial construction, laser scanning can be used to record the as-built locations of HVAC and plumbing systems before drywall is installed.  In existing finished buildings, blueprints are often outdated or missing, especially after several remodelings.  Such scans can be used to generate building models describing the current architecture.  Meshed triangulations allow for the efficient representation of the scanned geometry.  In addition to being useful in the fields of architecture, civil engineering, and construction, these models can be directly applied to virtual walk-throughs of environments, gaming entertainment, augmented reality, indoor navigation, and energy simulation analysis.  These applications rely on the accuracy of a model as well as its compact representation.

% figure with pointcloud and models
\begin{figure}

	% top row
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_photo}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_triangles}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_texture_compressed}}
		\centerline{(c)}\medskip
	\end{minipage}
	
	% bottom row
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_pointcloud_compressed}}
		\centerline{(d)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_triangles}}
		\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_texture_compressed}}
		\centerline{(f)}\medskip
	\end{minipage}
	
	\caption[Models generated with our techniques.]{Models generated with the techniques described in this dissertation:  (a) photograph of scanned area, academic building; (b) surface carving model of this area; (c) surface carving model with texturing; (d) point cloud of captured scans; (e) extruded floor plan model of area; (f) extruded floor plan with texturing.}
	\label{fig:coryf2}

\end{figure}

% applications
Generating an accurate model of indoor environments is an emerging challenge in the fields of architecture and construction for the purposes of verifying as-built compliance to engineering designs~\cite{Bosche10,Xiong13}.  This task is made more challenging by the GPS-deprived nature of these environments~\cite{Liang13}.  Another application that requires an exported mesh to retain as much detail as possible is historical preservation via a virtual reality model~\cite{VillageHeritage,Carving}.  Building energy efficiency simulations can use watertight meshes of the environment to estimate airflow and heat distribution~\cite{EnergyPlus}.  These simulations require simplified meshes as input, since finite element models are difficult to scale.  It is also important to be able to generate an immersive visualization and walk-through of the environment for these applications, so experts can remotely inspect the scanned environment via tele-presence, a task that currently requires expensive travel and on-site visits.  Different applications require models of different complexities, both with and without furniture geometry.  The modeling approaches detailed in this report are useful for both types of applications, as shown in the examples in Figure~\ref{fig:coryf2}.  Figure~\ref{fig:coryf2}a is a photograph of the scanned area: the hallways of an academic building, encompassing over 10,000 square feet of scanned area.  Figure~\ref{fig:coryf2}d represents the captured 3D point-cloud of this area, which is discussed further in Chapter~\ref{ch:preprocessing}.  Figures~\ref{fig:coryf2}b and~\ref{fig:coryf2}c show a high-detail 3D mesh of 2.7 million triangles generated using the algorithm in Chapter~\ref{ch:carving}, with and without texturing, respectively.  Figures~\ref{fig:coryf2}e and~\ref{fig:coryf2}f show a low detail model of 2,644 triangles generated using the approach in Chapter~\ref{ch:floorplan}, with and without texturing~\cite{Turner14Journal}.

% brief overview of different modeling techniques presented
One of the primary challenges of indoor modeling is the sheer size of the input point-clouds.  Scans of single floors of buildings result in point-clouds that contain hundreds of millions of points, often larger than the physical memory in a personal computer.  Man-made geometry is typically composed of planar regions and sharp corners, but many conventional surface reconstruction schemes assume a certain degree of smoothness and result in rounded or blobby output if applied to these models~\cite{Powercrust,OctreeSculpting,Carving,ProgressiveMesh,Poisson,EigencrustShewchuk}.  In addition to large flat regions, building interiors also contain many small details, such as furniture.  A surface reconstruction scheme must be able to represent the large surfaces in a building with an efficient number of elements and preserve their sharp features.  The fine details of furniture are useful for some applications whereas others require furniture to be removed.  In this dissertation, we discuss existing modeling techniques that remove fine details and those that preserve these details.  The output of both of these modeling techniques can be texture-mapped with captured camera imagery~\cite{Cheng14}.  An example of texture-mapping these two modeling processes is shown in Figs.~\ref{fig:coryf2}c and~\ref{fig:coryf2}f.

\section{Surface Reconstruction}
\label{sec:surf_recon_background}

% background on surface reconstruction techniques
This section describes common background techniques for surface reconstruction from input point-cloud data.  These techniques are useful for generating meshes of objects scanned with table-top scanning systems.

% powercrust and the need for noise-reduction
Powercrust is a volumetric surface reconstruction approach that yields watertight models from point-cloud data~\cite{Powercrust}.  It computes the interior median axis of a shape using the voronoi diagram of the input scan points.  By taking the union of the inner polar balls, the elements of the Delaunay Triangulation that form the interior of the shape can be found, and are used to approximate the modeled volume.  While this approach is popular, one weakness is that it is sensitive to noise in the input scans.  Fortunately, other techniques have improved on this methodology to account for noisy input

% Accounting for noise in volumetric scans
An approach to account for the noise in the Powercrust algorithm is to remove sufficiently small polar balls from consideration, which are likely to be due to input noise~\cite{NoisyPowercrust}.  Another approach is to perform spectral clustering on the tetrahedralization of the points~\cite{EigencrustShewchuk}.  This approach, dubbed Eigencrust, allows for substantially improved robustness to noise.  An important consideration, though, is that the noise models tested are randomly positioned outlier points and randomly perturbed input points.  Both of these approaches assume that the randomness is independent and identically distributed from point-to-point.  Such characteristics are not the dominant case when dealing with noise from mis-registration of scans, which is the most common source of noise from mobile scanning systems.  Further, these methods assume that the point-clouds are modeling smooth and continuous surfaces, which is not the case in building modeling.  These algorithms may also require a global optimization step~\cite{EigencrustShewchuk}.  While advancements have been made to perform these computations in an efficient and out-of-core manner~\cite{RealTimeEigenCrust,StreamingDelaunay}, the resulting models are too large to be practical for graphical or simulation applications.

% Isosurface and the reign of SDFs
Another technique used to produce watertight models is to define implicit functions on the scanned volume, and use an isosurface of this volume as the exported mesh.  Once an implicit function is defined, techniques such as Marching Cubes or Dual Contours can be used to mesh the surface~\cite{MarchingCubes,DualContouring}. A popular method for defining this implicit function is to use Signed-Distance Fields~\cite{SignedDistanceFields}.  Other popular approaches such as Poisson Surface Reconstruction allow the user to specify a resolution parameter for the generation of more compact models~\cite{Poisson}.  These schemes guarantee watertightness by using an implicit surface to model the point-cloud~\cite{UnorganizedPoints}.  While these approaches can be applied to large models using distributed computing techniques~\cite{OutOfCorePoisson,ParallelPoisson}, they are unsuited for modeling man-made architecture.  The output models of these methods lack sharp features because they generate implicit surfaces using Gaussian basis functions.  Additionally, many common triangulation schemes for implicit surfaces result in uniform elements~\cite{DualContouring,MarchingCubes}, which are undesirable for large, flat surfaces that can be modeled just as accurately with fewer elements.  If these approaches are used on a discretized voxel grid, undesirable artifacts of the discretization are preserved, requiring the final mesh to be smoothed, thus reducing accuracy~\cite{Carving}.  Algorithms that adaptively mesh an isosurface or simplify an existing mesh rely on the local feature size of a model~\cite{QEM,ProgressiveMesh,Isostuffing,AdaptiveMeshing}.  Models with flat regions or sharp corners, where the curvature approaches zero or infinity, can become degenerate or have poor quality.  Since building models are composed almost entirely of such areas, these techniques are not appropriate.

Models of building interiors are rich with flat surfaces and right angles.  This prior knowledge supports the use of primitives that have these same aspects.  Examples include voxel and octree structures, which are used in many carving techniques~\cite{OctreeSculpting,Carving,SpaceTime,VoxelSurfaceArea,Yang05}.  The advantage to such approaches is that they are robust to noise and registration errors in the input point-cloud.  One of the challenges with voxel representations is memory and computational intensity, thus becoming tractable only when performed in a distributed or parallel fashion~\cite{ParallelOctree}.  Some voxel carving approaches can also inadvertently remove small details~\cite{Carving}.  In Chapter~\ref{ch:carving}, we modify voxel carving to address these issues and introduce memory-efficient data structures that produce models that preserve fine details with an efficient number of elements.

% typical hardware set-up for traditional surface reconstruction
An important note is that traditional surface reconstruction techniques, as described above, are written for and tested with common point-cloud test-sets, such as the Stanford Bunny or Dragon.  These models are scanned with precision 3D scanners that yield relatively low noise and minimal mis-registration when compared to scans of larger areas such as buildings.  These approaches are also designed to model isolated objects.  The goal of this thesis is to model buildings, which are scanned at a much larger scale and produce much higher rates of noise and mis-registration.  While watertight volumetric processing is still useful for our applications, different techniques must be applied compared to isolated objects.

\section{Outdoor Building Scanning}
\label{sec:outdoor_scanning}

% motivation for discussing outdoor stuff
In the previous section, we discussed general surface reconstruction techniques.  When discussing 3D reconstruction specifically of building environments, it is important to note the subfields of both indoor and outdoor building modeling.  Most of this dissertation is specific to indoor building modeling, however many related techniques are used for outdoor modeling as well.

% how to scan outdoors
The current method of outdoor building scanning requires an acquisition vehicle to drive down a street while taking Light Detection And Ranging (LiDAR) scans and panoramic photographs of the surround area~\cite{Zakhor07,Chen07,Karimi00,Li97}.  In many applications, it is desirable to generate a triangulated geometry for a collected point-cloud.  This geometry needs to be accurate to the original architecture.  

% pointcloud of building facade
\begin{figure}
	\begin{minipage}[b]{1.0\linewidth}
	  \centering
	  \centerline{\includegraphics[width=0.7\linewidth]{icip2012/rush_pointcloud00}}
	\end{minipage}

	\caption[An example point-cloud of a building fa\c{c}ade.]{An example point-cloud of a buidling fa\c{c}ade. Note gaps near windows, ledges, and columns due to line-of-sight occlusions of the scanning system.}
	\label{fig:rush_points}
\end{figure}

For buildings of an appreciable height, the street-level LiDAR scans are taken at a very acute angle with respect to the surface of the building.  Any protrusions from the building cause shadows in the LiDAR and therefore the building is not collected in its entirety, as shown in Figure~\ref{fig:truck_lidar}.  A typical solution would be to gather scans from multiple angles \cite{Tang10, Curless96}.  For the urban reconstruction problem, this tactic cannot be used since the scanner is restricted to street-level.

\begin{figure}[t]

% figure:  how to scan outdoors
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[height=4.5cm]{icip2012/truck_lidar}}
\end{minipage}

\caption[Scanning building fa\c{c}ades from a vehicle.]{Locations on the building A, B, C are occluded from the LiDAR collector on the vehicle.}
\label{fig:truck_lidar}

\end{figure}

% background on outdoor modeling
Previous attempts to model buildings have assumed that architecture takes a simple polyhedron geometry. This approach ignores minor details of a building fa\c{c}ade, such as windows and ledges when applied on a large scale \cite{Chauve10, Chen07}.  Our goal is to preserve as much detail as possible in the reconstructed model geometry.  Any interpolation must preserve the sharp edges of corners that occur within these holes.  This requirement diverges from the typical assumptions of most surface completion methods, which usually result in smoothed surfaces~\cite{Poisson, Kawai11}.  While developing 3D models with sharp features is a well-explored topic, most current approaches require a sufficient density of points near regions of high curvature, which would not be applicable here \cite{Bernardini04, Mhatre06}.  

% our method
Our goal is to generate sharp, axis-aligned, planar features in locations where the point-cloud is sparse or not sampled at all.  We propose an approach for rectilinear surface reconstruction of building fa\c{c}ades with incomplete 3D point-clouds~\cite{Turner12outdoor}.  Specifically, our proposed algorithm generates geometry for any holes in the point-cloud while preserving flat planes and sharp corners, which are common in modern architecture.  In order to reconstruct a mesh of a building face, first we determine the dominant plane of this fa\c{c}ade, then by treating the original points as a height map on this plane, we uniformly resample these heights over the entire surface using Moving Least-Squares (MLS) smoothing in order to mitigate noise in areas of high sampling, interpolate the areas corresponding to gaps in the point-cloud, and guarantee uniformity of resulting geometry~\cite{Nealen04}.

% Some results
\begin{figure}[t]

\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[width=0.8\linewidth]{icip2012/set2-curve-points00}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[width=0.8\linewidth]{icip2012/set2-curve-screenshot}}
  \centerline{(b)}\medskip
\end{minipage}
%
\caption[Surface reconstruction of building fa\c{c}ade.]{The curvature of the building is preserved when extrapolated; (a) building point-cloud; (b) MLS triangulation.}
\label{fig:extrapolate}
%
\end{figure}

This method results in flat strips of extrapolated fa\c{c}ade.  If the building curves outward or has other non-planar characteristics, these strips conform to the shape of the building, as shown in Figure~\ref{fig:extrapolate}.  Once a geometry has been processed, a texture is applied using panoramic photographs collected from the same location as the LiDAR.  Since these images are projected onto the geometry, any irregularities in the geometry result in dramatic disturbances in texturing.  Examples of this texturing process are shown in Figure~\ref{fig:icip2012_texture}.

\begin{figure}[b]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face4_mls}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face4_texture}}
  \centerline{(b)}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face7_mls}}
  \centerline{(c)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face7_texture}}
  \centerline{(d)}\medskip
\end{minipage}
\caption[Meshes of building fa\c{c}ades with texture.]{Results of texturing geometry after sharp hole-filling; (a) sampling of flat fa\c{c}ade; (b) with texture; (c) sampling of uneven fa\c{c}ade; (d) with texture.}
\label{fig:icip2012_texture}
\end{figure}

While the techniques used for surface reconstruction of exterior building models are different than those used for interior models, both methods require the efficient processing of large amounts of scan data in the form of point clouds.  Such point clouds may represent noisy or inconsistent geometry, due to being acquired via a mobile scanning system.

\FloatBarrier
\section{Indoor Building Scanning Systems}
\label{sec:indoor_scanning}

% building information models
For the remainder of this dissertation, we discuss prior work performed to reconstruct models of the interiors of buildings.  In the architecture community, there is a continuing push for use of a consolidated Building Information Model file format~\cite{AutodeskBIM}.  Such models need to be semantically-rich for all phases of building development, including architectural, structural, mechanical, electrical, plumbing, etc.  Traditionally, each separate aspect needed to be remodeled from scratch, by hand, which introduced errors into the modeling process.

It is important to distinguish between models that are as-designed compared to as-built.  Each phase of building development will produce a design, but it is still important to compare this design to the as-is nature of the building.  Scanning of building environments is an important technology to be able to compare the result of construction to what was desired.  With current technology, there is still a fair amount of manual intervention that is required to convert a 3D scan of a building into a suitable BIM file that can be used for such a comparison.

% competing systems
Traditional industry-standard building scanning uses static scanners.  Such scanners are mounted on tripods, and moved from area to area in the building~\cite{RoomSegmentation,HistWallRecon,BasicPlaneFit}. This scanning process is labor intensive and slow, but results in highly accurate point clouds after stitching.  In order to automate indoor scanning, many mobile systems have been introduced. Due to cost of full 3D laser range finders, the majority of indoor modeling systems use 2D LiDAR scanners.  Wheeled platforms that carry scanning equipment and are manually pushed through the environment are popular~\cite{Carving, ProbabilisticRobotics}.  Mobility of such systems is limited, since they are unable to traverse rough terrain or stairs easily.  Others have investigated mounting laser range finders on unmanned aerial vehicles~\cite{Quadrotor,QuadrotorMIT,SpectralClustering}.  Such platforms are agile in that they can scan difficult-to-reach areas, but are limited by short battery life and cannot scan for long durations.

% include system hardware description
% our system and WHAT WE DO DIFFERENT AND WHY IT IS BETTER!!!!!!!!!
In this dissertation, we focus on ambulatory scanning platforms, where the sensor suite is carried by a human operator as the operator moves through the building environment~\cite{Sweep,MITBackpack,VillageHeritage}.  These systems allow for rapid data acquisition and can be actively scanning for several hours at a time.  They use 2D LiDAR scanners due to the cost and weight of full 3D laser range finders.  The captured scans are used both to reconstruct the geometry of the environment and to localize the system in the environment over time, in Simultaneous Localization and Mapping (SLAM).  The datasets shown in this paper were generated by a backpack-mounted system that uses 2D LiDAR scanners to estimate the 3D path of the system over time as well as multiple scanners to generate geometry for the environment~\cite{liu2010indoor,Backpack,Localization,NickJournal}.  This system also has multiple cameras collecting imagery during the data acquisition process, which allows for scanned points to be colored or for generated meshes to be textured with realistic imagery~\cite{Cheng14}. 

% This section describes building meshing techniques
\section{Modeling of Indoor Building Environments}
\label{sec:building_meshing}

% give a brief overview of the different types of meshing techniques
% Discuss how the below techniques typically use noise-less input from
% static scanners, and do not explicitly detail with misregistration or
% noise.
The primary thesis of this dissertation regards surface reconstruction of scan data of indoor building environments, regardless of what hardware systems are used to collect building scan data.  There exists an emerging field of techniques used to model different aspects of building geometry from captured scans.  These techniques can be classified into three main categories:  Floor-Plan Generation, Simplified 3D Modeling, and Detailed 3D Modeling.  Floor-plan generation focuses on estimating 2D positions of walls in the building.  Simplified 3D modeling similarly focuses on 3D modeling only the permanent features of a building: floors, walls, and ceilings.  Detailed 3D modeling focuses on modeling all aspects of the scanned geometry, including fine details such as furniture or objects observed in the building.  Note that most of the pre-existing approaches were developed to be applied to static scans of buildings, which have very low noise and capture high detail. The focus of this dissertation is to develop modeling techniques for mobile scanning systems, which are much more likely to suffer from mis-registration noise or missing geometry.

In this section, we discuss existing techniques to generate each of these types of building models.  This dissertation also describes novel work we contributed in each of these areas, in Chapters~\ref{ch:floorplan},~\ref{ch:carving},~\ref{ch:better_floorplans}, and~\ref{ch:better_carving}.

% This subsection describes floor plan generation techniques
\subsection{Floor-Plan Modeling}
\label{ssec:background_floorplan}

Interior scanning is traditionally used for robotic navigation in indoor environments.  The rough locations of walls are necessary to avoid collisions.  Generating floor plans for modeling, however, requires a much higher degree of precision \cite{Okorn09}.  Given these antecedent studies, techniques have been developed using a single horizontal scanner collecting a 2D cross-section of the environment~\cite{Weiss05}.  Constructing full 3D scans allows for more sophisticated means of identifying walls from other obstacles in a building.  In this scenario, one can compute a top-down 2D histogram of point densities across the $xy$-plane~\cite{Okorn09}.  Areas with high density are considered likely to be wall locations.  While clutter is mitigated by being less represented in the histogram than walls, no direct measures are taken to remove outlier samples before line-fitting.  Further, each story of a building must be processed separately.

Previous approaches to floor plan modeling typically assume walls are well-fitted by straight line segments and whether these fitted models are watertight is not guaranteed~\cite{Nuchter03, Okorn09, Weiss05}.  Many architectural designs incorporate curved features, which would not be modeled accurately by these approaches~\cite{Castles07,Turner12}. The absence of any water-tightness guarantee requires extensive post-processing to be devoted to removing disconnected and outlier segments that are interpreted as noise.

Floor-plan modeling techniques are based on the idea of sampling positional information of walls within the environment captured by the scans, then using these wall samples to generate a plan composed of line segments or polygons.  The work of Weiss et al use a cart-based system with a horizontal laser scanner~\cite{Weiss05}.  The output scan points are exactly the sample positions of the walls.  The find lines in this scan map with a Hough Transform, which represent walls.  Okorn et al employ a similar method, but their input scans represent a full 3D point-cloud~\cite{Okorn09}.  The wall sample positions are found by computing a top-down histogram of the input points, and areas of high density are classified as vertical surfaces.  The approaches we discuss in Chapter~\ref{ch:floorplan} employ a similar top-down histogram.  Lastly, Mura et al's paper takes a different approach to estimating wall positions~\cite{Mura13}.  They perform region growing in the 3D point-cloud to fine planar regions, which are projected into 2D and treated as potential wall candidates.  A cell complex is then built to volumetrically identify separate rooms in the model.  This approach is the second publication that performs automatic room partitioning, where the first is our approach as discussed in Chapter~\ref{ssec:room_label}~\cite{Turner14}.  There are also methods that take a floor plan as input, and use this information to generate a 3D model of the environment by extruding the defined wall information~\cite{Or05,Lewis98}.  This type of modeling yields aesthetically pleasing results with well-defined floors, walls, and ceilings.

% This subsubsection describes plane fitting for 3D modeling
\subsection{Simplified 3D Modeling}
\label{ssec:background_planefit}

Since building features are almost entirely planar, a popular approach is to explicitly fit planar elements to the input point-clouds.  This assumption allows for plane-fitting to be performed on the input point-cloud, either by a histogram approach or random consensus~\cite{HistWallRecon,BasicPlaneFit}.  Such approaches do not guarantee watertightness of the resulting mesh and can require substantial post-processing.  Sanchez and Zakhor use PCA plane-fitting to find floors, walls, and ceilings explicitly in the point-cloud, as well as explicitly fitting staircases~\cite{Victors}. Since this method was applied to ambulatory data, it resulted in missing components, holes, and double-surfacing due to mis-registration.  Xiong et al also perform plane-fitting in a similar fashion, but also analyze the computed planes for the locations of windows and doors, which are represented as holes in the surface~\cite{Xiong13}.  Adan et al find planes by first generating a floor-plan, then extrude the floor-plan into a 3D model~\cite{WallFinder}.  One limitation with these methods is that they are not necessarily watertight, though floor plan extrusion can also be done in a watertight fashion~\cite{Mura14,Turner14,Cabral14}.  These approaches allow for accurate wall geometry and reduce the complexity of the output model. Extruded floor plans also allow for models to explicitly define floors, walls, and ceiling surfaces.  Being able to make assumptions about the planarity of an environment has been used successfully to model only the major features of scanned objects even in the presence of high noise~\cite{Lafarge13}.

Other approaches have focused to perform volumetric processing to ensure watertightness when computing simplified 3D models.  Xiao et al find horizontal cross-sections of the building, forming a sequence of 2D CSG models that are then stacked together and simplified~\cite{Museums}.  While this approach does lead to aesthetically-pleasing models, it assumes Manhattan-world models, which leads to topological errors if an insufficient number of cross-sections are recovered.  Oseau et al use a voxelization approach, with a follow-up graph-cut step to remove small details in the environment, leaving only floors, walls, and ceilings~\cite{Oesau13}.  This optimization step, however, can also cause significant deformations in the final geometry depending on the input parameters. 

% This subsubsection describes dense 3D modeling
\subsection{Detailed 3D Modeling}
\label{ssec:background_3dmodeling}

One of the methods used to preserve the detail of furniture in a building scan is to explicitly search and classify for furniture models in the scan.  These techniques attempt to find locations in the input scans that match best with a stored database of known furniture.  The pre-existing model of the recognized piece of furniture is then oriented in the output model.  Nan et al employ this technique, with explicit classification of chairs and tables~\cite{SearchClassifyPointcloud}.  Kim et al also use this method, using a larger library of objects and operating on noisier scans~\cite{Kim12}.  They also discuss how search-classification can allow for change detection across scans of the same area taken at different times.  A major downside of this method is that the classification is only as good as the database.  Objects that are in unexpected orientations or are not in the database are misclassified.  For instance, a sideways chair is misclassified as a table.  One major benefit of this approach is the ability to model the objects independently of the room itself, as discussed later in Chapter~\ref{ch:better_carving}.  

% object/room detection in buildings
Recently, object detection methods from indoor scans have been proposed without the use of a training dataset~\cite{Mattausch14}.  They segment point clouds using a bottom-up approach to fit rectangular patches on to the scans, then find clusters of patches that are repeated often.  Even though this approach can be applied to large datasets, it assumes very basic building geometry in order to segment objects.  Additionally, it only detects objects of certain complexity, is unable to detect very small objects, and requires objects to be repeated often in the environment.  We expand on this work by segmenting objects volumetrically, rather than in the point cloud domain.

% general carving
There are also methods that capture fine detail of buildings by attempting to be as accurate to the input point-cloud as possible.  Holenstein et al generate a space-carving model that voxelizes the scanned environment, labeling any voxels intersected by a scan-line line to be interior~\cite{Carving}.  The boundary of the interior voxels is meshed with Marching Cubes.  The advantage of this approach is an increased robustness to mis-registration errors, but the downside is that over-carving can result in the loss of fine detail.  Lastly, a popular approach to detailed modeling of environments is Kinect Fusion~\cite{KinectFusion,Kintinuous}.  This method allows for both meshing and tracking, but is limited in that it cannot handle large areas, and the actual meshing approach is a minor extension of Signed-Distance Fields~\cite{SignedDistanceFields}. 

% this section outlines the remainder of the paper
\section{Contributions and Organization of Dissertation}
\label{sec:organization}

% organize paper
In this dissertation, we present several techniques used to automatically generate virtual models of indoor building environments.  The interior environment of a building is scanned using our custom hardware system, which provides a set of data products used to develop these models.  Our modeling techniques can be separated into three categories:  2D floor plan models, 2.5D extruded simplified models, and dense 3D models.  All approaches are produced automatically from the output data of our scanning system.  These approaches are intended to be agnostic to the specific hardware used for scanning and in many cases have been applied to scans taken with other hardware systems.  Unless stated otherwise, however, any examples shown in this dissertation were generated from scans acquired using our hardware system.

First, we discuss the specifics of our hardware system in Chapter~\ref{ch:hardware}.  This system is a collection of laser scanners and other sensors, worn as a backpack by a human operator.  The operator traverses the environment at normal walking speed, allowing the system to collect data about the interior area.  We discuss specifics on the mechanics of our backpack system, how the data are logged, and special considerations needed to ensure accuracy of the resulting scans.

Next, in Chapter~\ref{ch:preprocessing}, we discuss preprocessing steps required once a data acquisition is performed.  Specifically, these steps detail how the system is localized while traversing through a GPS-denied environment and how we ensure a consistent global coordinate system.  We also discuss the first modeling technique, which is to generate a point cloud of the environment.  This point cloud can be colored using the camera imagery.  Additionally, these scans are used to determine the number of building levels covered during the acquisition, and the partitioning of those levels.

In Chapter~\ref{ch:floorplan}, we detail the techniques developed to generate 2D floor plans of the interior building environment.  These techniques use the 3D scan information to generate a consolidated 2D estimate of wall positions.  These wall position estimates are used to develop a volumetric floor plan of each level in the building environment.  We show multiple floor plan generation techniques, including methods to automatically detect and classify separate rooms in the environment.  Once a 2D floor plan is produced, its geometry can be extruded into a simplified 2.5D model, which includes height information.

In Chapter~\ref{ch:carving}, we denote multiple methods used to generate complex, fully-3D models of building environments.  Such models include not only building elements, but also furniture and other objects within the building environment.  In Chapter~\ref{ch:fp_carving_compare}, we compare the complexity of these models to those generated by extruding 2D floor plans, as well as how the detail of the environment's geometry can be accurately preserved.

In Chapters~\ref{ch:better_floorplans} and~\ref{ch:better_carving}, we discuss how these two types of methods -- 2D floor plan generation and 3D dense modeling -- can be combined to provide additional analysis on building modeling.  This combination of techniques can be used to not only augment each individual model output to improve accuracy, but also provide new information about the environment.  Chapter~\ref{ch:better_floorplans} discuss how the 2D floor plan generation methods can be improved by using the 3D dense modeling results, and Chapter~\ref{ch:better_carving} iterates how the 3D dense modeling is improved by incorporating the 2D floor plan output.

In Chapter~\ref{ch:applications}, we show the applications of these techniques, and how building models at different granularity are required based on how the model is to be used.  We show several examples of use-cases for these modeling techniques.

Lastly, in Chapter~\ref{ch:conclusion}, we offer concluding remarks and directions for future work.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Hardware Description
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Hardware Description}
\label{ch:hardware}

The surface reconstruction algorithms described in this dissertation can be applied on any indoor scanning data.  For practicality purposes, we have developed our own scanning system, and all examples shown in this document are generated with our custom hardware (unless otherwise stated).

We developed a human-mounted ambulatory scanning system, worn as a backpack.  As the human operator walks through the building environment, at normal speed, this system collects data from a variety of sensors on-board.  These data products are logged and then processed off-line.  Our developed software can accurately estimate the trajectory of the system over time and localize the system~\cite{NickJournal}.  This procedure allows us to rapidly move through a large environment, spending only a few seconds in each room yet capturing full geometry information.

\section{Mechanical Specification}
\label{sec:mechanical}

% this figure shows side-by-side backpacks
\begin{figure}

	\centering
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_1}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_2}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_3}}
		\centerline{(c)}\medskip
	\end{minipage}	

	\caption[The three backpack scanning systems developed by our lab.]{The three backpack scanning systems developed by our lab:  (a) First generation, weighing 80 pounds; (b) second generation, weighing 32.5 pounds, developed in 2013; (c) third generation, weighing 35 pounds, developed in 2014.}
	\label{fig:all_backpacks}

\end{figure}

% different backpack models
As shown in Figure~\ref{fig:all_backpacks}, our lab has developed three generations of the backpack hardware.  The first generation system, shown in Figure~\ref{fig:all_backpacks}a, was built as a prototype~\cite{Backpack}.  It contains five Hokuyo laser scanners, two cameras, and an Internal Measurement Unit (IMU).  Since 2012, it has been augmented with Wi-Fi antennas and infrared cameras.  The Wi-Fi antennas are used to record signal-strength of nearby access points, which is useful for indoor localization techniques~\cite{Levchev14}.  The infrared cameras are useful for energy-efficiency analysis of building environments.  These data products are discussed further in Chapter~\ref{ch:applications}.

The second generation backpack, as shown in Figure~\ref{fig:all_backpacks}b, was constructed in 2013.  It contains all sensors available on the first backpack, except for the infrared cameras.  It also has a barometer and a set of 3D magnetometers.  The main modification of the second generation was to reduce the size.  The first generation is about $80$~pounds, whereas the second generation is only $32.5$~pounds.  The third generation, shown in Figure~\ref{fig:all_backpacks}c, backpack is almost identical to the second generation, but with the addition of infrared cameras, resulting in a weight of $35$~pounds.

\section{Sensor Characteristics}
\label{sec:sensor_specs}

% figure of backpack hardware
\begin{figure}[t]

	\begin{minipage}[c]{0.54\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/hardware/backpack_annotated}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.46\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/backpack_coord_system}}
		\centerline{(b)}\medskip
	\end{minipage}

	\caption[Annotated scanning hardware system.]{Annotated scanning hardware system, worn as a backpack.  The operator walks through the indoor building area as the system scans the surroundings.  Our method combines the LiDAR and inertial measurements in order to form a virtual 3D model for the observed space:  (a) the system worn by operator; (b) the common coordinate frame of backpack system.}
	\label{fig:backpack}
\end{figure}

% briefly describe the type of scans we use as input
Our geometry reconstruction algorithms use the retrieved laser range data as input.  These scans are taken with time-of-flight laser scanners that capture 3D geometry of the environment.  We use multiple 2D laser scanners mounted at different orientations to capture all observed geometry in the environment, as indicated in Figure~\ref{fig:backpack}a.  Since our system is mobile and ambulatory, the resulting point clouds have higher noise than traditional static scanners due to natural variability in human gait.  As such, our approach needs to be robust to motion induced scan-level noise and its associated artifacts.  We use these scans to form watertight models of the indoor building environment and the contained objects.

% sparse but fast
The primary advantage of a backpack system is speed of acquisition.  We use 2D Time-Of-Flight (TOF) laser scanners, which means we have sparser scan information per frame and traverse each area of the environment much faster than when scanning the equivalent area with a Kinect or Google Tango.  Since our system produces much sparser point clouds than Kinect-based scanning, it is not feasible to employ the same averaging techniques that allow for high-quality Kinect scans.  The advantage of these 2D TOF sensors is that they are less noisy, have a longer range, and a wider field of view.  The result is that much larger environments can be covered in significantly less time when using these sensors in a sweeping motion~\cite{Sweep}.  For instance, a backpack-mounted system allows an unskilled operator to rapidly walk, or even power walk, through an environment to acquire data.

% the noise characteristics
Our system uses Hokuyo UTM-30LX sensors, whose intrinsic noise characterization is given in~\cite{Pomerleau12,Wong11}.  Typically this noise contributes on the order of 1 to 2 centimeters to the standard deviation of the positional estimate of scan points.  This uncertainty value increases as the range of the point increases, with accurate measurements stopping at a range of $30$~meters.  We also must consider input noise from our cameras.  We use three 12-megapixel cameras with fisheye lenses, in order to collect as much of the surrounding scenery as possible.  Due to this set-up, a major factor in sensor calibration is how long of an exposure to use for each image.  Since we may be scanning in dark or narrow areas of building environments, ensuring that enough light hits the sensor is challenging, especially with fisheye lenses.  However, if we set the exposure too long, then the imagery is susceptible to motion blur, since the operator often walks or turns quickly.

% data rate from cameras, scalability of systems.
Since we collect 12~MP imagery from three cameras at 2 Hz each, the camera images represent the major of data flow during the data acquisition process, at 72~megabytes per second.  To ensure no data loss, we necessitate a fast internal drive on the backpack system.  We have also configured the system with a USB-3 external drive, with twin solid-state drives in a RAID-0 configuration.  This set-up allows us to write data directly to the external drive, which can be hot-swapped for fast processing.  On the second generation system, this storage drive is 500~GB, which allows for just under two hours of consecutive scanning.  The third generation system contains a 1~TB drive, which allows for up to four hours of scanning.  At that rate, we can perform data acquisition of over 100,000 square feet of building space on a single drive.

\section{Sensor Calibration}
\label{sec:calibration}

% also discuss extrinsic calibration of sensors
Since our hardware system contains may separate sensors, it is necessary to create a common system coordinate frame, as shown in Figure~\ref{fig:backpack}b.  Each sensor has an independent coordinate frame, but they are extrinsically calibrated against one another, in order to establish the rotation and translation between any given sensor and the common coordinate frame.  Once this extrinsic calibration process is performed, we can determine the orientation of any given sensor with respect to the common system.  As discussed in Chapter~\ref{sec:localization}, after a data collection we can determine the orientation of the system within a model coordinate frame.  The combination of the transform generated by extrinsic sensor calibration and the localization process allows for sensor readings to be placed in model coordinates.

% discuss time synchronization, both during collection and processing
In addition to geometric calibration of the individual sensors, we must also consider temporal calibration of the sensor readings to a common clock.  Since each sensor operates independently on a separate clock, it is important to compute the transformation from a given sensor's time frame to that of the common system.  For this computation, we make the assumption that there is an affine transform between any two clocks.  Such an assumption is reasonable, since two clocks can naturally have a phase offset based on when they were initialized, as well as a scalar offset due to factors such as running temperature and age.  Timestamp synchronization is achieved by recording two timestamps for every scan frame of each sensor, one from the sensor clock and one from the system computer.  This process must be done in software since not all sensors have the capacity for hardware triggering.  The linear fit between clocks is computed and typically represents a fitting error of around $2$ to $7$~millisecond standard deviations.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Preprocessing and Conventions
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Preprocessing and Conventions}
\label{ch:preprocessing}

This chapter details processing steps that are required before any model generation can occur.  All approaches discussed in this dissertation require a reconstructed path of how the system moved through the environment.  Such a path trajectory is required in order to estimate the position of each scanner at any given time, so that the geometry readings can be placed in global coordinates.  The details of how the localization data are used are described in Sections~\ref{sec:localization} and~\ref{sec:align_path}.

Some methods discussed in this dissertation also require a consolidated 3D point cloud of the scanned environment.  Unlike most geometry products discussed, a point cloud contains no topology information.  Instead, a point cloud is a concatenation of the raw laser range data, transformed into the global coordinate frame.  Specifics on how these values are generated and what conventions are used are discussed in Section~\ref{sec:pointcloud}.

\section{Localization}
\label{sec:localization}

% Nick's localization procedure
Before models of the building geometry can be generated, we must first determine how the operator traversed the environment.  Indoor building environments are typically GPS-denied areas, so local observations are required in order to track the path walked~\cite{Backpack,Localization,NickJournal}.  The process of recovering the 3D trajectory of the system is especially difficult for our backpack hardware, since a human operator can experience six degrees of freedom in movement ($x$, $y$, $z$, $\theta$, $\phi$, $\psi$), whereas the sensors used in the system can only observe a 2D slice of the environment.  As such, multiple laser scanners are mounted at different orientations on the backpack hardware to ensure that full 3D movement can be observed~\cite{NickJournal}.  These incremental changes in pose are refined by a global graph optimization step that constrains the system path whenever an area is revisited~\cite{toro07}.  These revisits are automatically detected and constrained in order to produce an accurate 2D trajectory of the system.  The elevation of the system is computed using a similar technique with a vertically-mounted scanner~\cite{Backpack}.

% coordinate systems used:  sensor, backpack, model
The localization procedure is complete, we have full mapping between three coordinate systems for each recorded frame for each hardware sensor.  The internal coordinate frame of the sensor, which is specific to the make and model of the sensor hardware, can be converted to the system common coordinate frame based on the extrinsic calibration of each sensor's rigid position and orientation on the backpack system, which is given by the coordinate frame shown in Figure~\ref{fig:backpack}b.  This rigid affine transformation for a given sensor $s$ is represented by the $3 \times 3$ rotation matrix $R_{s\rightarrow c}$ and the $3 \times 1$ translation vector $T_{s\rightarrow c}$.  The localization procedure provides an output path of the system, so that each pose of the system can be transformed from the system common coordinate frame to the model coordinate frame.  At a given time $t$, the affine transformation from the system common coordinates to the model coordinates is given by $3 \times 3$ rotation matrix $R_{c\rightarrow m}(t)$ and $3 \times 1$ translation vector $T_{c\rightarrow m}(t)$.  With these transformations, any sensor reading can be placed in global coordinates.

% discuss output model frame of localization, and units
The localization output path provides the output model coordinate frame in a locally consistent coordinate frame based on the starting position of the operator during a scan.  This starting position is used as the origin point of the model.  By convention, the heading vector of the first pose is used as the $+X$-direction of the model coordinate frame.  The direction of gravity is used as the $-Z$-direction of the model coordinate frame.  The $Y$-direction of the model coordinate frame is chosen such that the coordinate system is right-handed.  Although this procedure produces a common set of coordinates for the model, we apply an additional step in order to align the model with the global coordinate frame, as discussed in Section~\ref{sec:align_path}.  Once this step is complete, the model uses East-North-Up (ENU) coordinates.  For the rest of this dissertation, all output models are expressed in units of meters.

% show figure of double surfacing.
\begin{figure}
	\centerline{\includegraphics[width=0.7\linewidth]{quals/misregistration_example}}
	\caption[An example of mis-registration or ``double-surfacing'' in a point cloud.]{An example of mis-registration or ``double-surfacing'' in a point cloud.   The chair shown in the figure was scanned at two different times, causing a duplicate chair to be shown at a slight offset.}
	\label{fig:double_surface}
\end{figure}

% noise models of localization output (e.g. double-surfacing)
Since the localization procedure used is performed in an integrative fashion, the expected error in the reported position and orientation of the system at any pose is higher than in competing technologies, such as static scan systems~\cite{NickJournal}.  Static systems often utilize markers or control points within the environment to ensure manual alignment of scans, allowing for positional accuracy of within $1~mm$~\cite{Li97,Karimi00}.  By contrast, the mean positional accuracy of the output poses in the backpack localization procedure is typically $10~cm$.  Additionally, the errors in successive poses are highly correlated.  Typically a local set of scan points captured within a few seconds of one another are highly accurate, yet if the same features in the environment are scanned twice with a long interval between scans, then the scans are likely to be mis-matched.  This behavior leads to the presence of ``double-surfacing'' in the output scans, which can be observed in the generated point cloud of the environment.  Figure~\ref{fig:double_surface} shows an example of such mis-registration.  The chair in the depicted point cloud is represented by two copies, slightly offset from one another.  Note that the point cloud shown is generated using a static scanning system, which can also suffer from mis-registration issues.

In order to provide consistent output geometry, the modeling techniques described in this dissertation are designed to prevent such ``double-surfacing'' from occurring in the output meshes.  This trait is accomplished by performing volumetric analysis of the input scans, as discussed later in this document. 

\section{Aligning Model Coordinates}
\label{sec:align_path}

% show color pointcloud compared to camera image
\begin{figure}
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{align_path/compass_readings}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{align_path/aligned_path}}
		\centerline{(b)}
	\end{minipage}

	\caption[Aligning model coordinate system with North.]{An example of aligning model coordinates to magnetic north:  (a) individual compass estimates of North (red) are compiled from each pose of the path (blue) and represented in world coordinates; (b) the model coordinates are transformed so that the mean estimate of magnetic North across all readings is oriented in the $+Y$-direction.}
	\label{fig:align_path}
\end{figure}

As discussed in Section~\ref{sec:localization}, the system path generated by the localization procedure is initially oriented using the heading vector of the first pose.  In order to keep the orientation of model coordinate frames consistent between separate scans of the same environment, and to ensure that output geometry is easily recognizable, we perform an additional orientation step that aligns the path to the East-North-Up (ENU) coordinate frame.

One of the on-board sensors of the backpack hardware system is a 3D magnetometer, which is built into the Inertial Measurement Unit (IMU).  Individual readings from a 3D compass are typically noisy when indoors, due to nearby ferrous building elements that interfere with hard and soft iron calibration~\cite{Caruso00,Guo08}.  As such, these readings are not reliable during the localization process.  Once the final, consistent path is generated we are able to generate a least-squares estimate of compass north and reorient the path so that North is aligned with the $+Y$-direction.

First, the 3D magnetometer reading is recorded at each pose during the data acquisition.  As shown in the example in Figure~\ref{fig:align_path}a, these vector readings represent the magnetic field at that location.  The direction of each reading is used as a noisy estimate of the direction of magnetic South.  The magnetic field flows South, so the reverse direction, as shown in Figure~\ref{fig:align_path}a, is an estimate of North.  Let $\vec{m}_i \in \mathbb{R}^3$ be the magnetometer reading at pose $i$, in the coordinate frame of the magnetometer sensor $\texttt{mag}$.  The rotation required to orient the model coordinate frame to ENU coordinates is given by:

\begin{equation}
	\label{eq:align_path}
	\texttt{argmin}_{\theta \in [0,2\pi]} \; \sum \limits_{i} 
		\; \left| \left( R_{z}(\theta) \, R_{c\rightarrow m} (t_i) \,
			R_{\texttt{mag} \rightarrow c} \, \vec{m}_i \right) - \left[ 
				\begin{array}{c} 0 \\ -1 \\ 0 \end{array} 
					\right] \right|^2
\end{equation}

where $t_i$ is the timestamp of pose $i$, and $R_{z}(\theta)$ is the rotation matrix:

\begin{equation}
	\label{eq:rotation_matrix}
	R_{z}(\theta) = \left[ \begin{array}{ccc}
				\texttt{cos}(\theta) & -\texttt{sin}(\theta) & 0 \\
				\texttt{sin}(\theta) & \texttt{cos}(\theta) & 0 \\
				0 & 0 & 1 \end{array} \right]
\end{equation}

This optimization rigidly rotates the model coordinate frame about the $z$-axis in order to best align all magnetometer readings with the direction we wish to denote south:  the $-Y$-direction.  An example result of this procedure is shown in Figure~\ref{fig:align_path}b.  This procedure ensures that all output models are aligned with magnetic North, in ENU coordinates.  While not applied in the current configuration, it is possible to also align the model to true North if given the latitude and longitude coordinates of the scanning location, by performing a look-up of magnetic declination~\cite{MagDec}.

\section{Point Cloud Generation}
\label{sec:pointcloud}

When a finalized path and coordinate frame is produced, we can immediately generate a point cloud of the raw scans taken in the environment.  This point cloud allows visualization of observed building geometry.  Each point can be computed independently by transforming its position from its sensor frame to the model coordinate frame.  An example point cloud of an indoor environment is shown in Figure~\ref{fig:pointcloud_nocolor}.  This scene shows a corner of a room, with a lamp to the left of a TV on a table.

% show example pointcloud
\begin{figure}
	\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_nocolor}}
	\caption{An example point cloud of an indoor building environment.}
	\label{fig:pointcloud_nocolor}
\end{figure}

We also have the capability to export point clouds in color.  The coloration is taken from camera imagery taken temporally close to when the range points were acquired.  For a given point $\vec{x}$ taken at time $t$, the point color is determined by querying all available cameras for any images within $\Delta t$ seconds of time $t$.  Typically, we use a window size of $\Delta t = 2$~seconds.  For each reported camera, we project the position $\vec{x}$ onto the camera image plane, and determine the pixel color at that position.  This projection is demonstrated in Figure~\ref{fig:pointcloud_color_diagram}.  In addition to color, we also compute a quality measure $q_x = (\vec{x} - \vec{c})^T \vec{n}$, where $\vec{c}$ is the camera position and $\vec{n}$ is the optical axis of the camera.  The quality $q_x$ is $1.0$ if $\vec{x}$ projects into the center of the image, and decreases as the projected location moves away from the center of image.  For all images within the time window $[t - \Delta t, t + \Delta t]$, the pixel color associated with the highest quality measure is used to color the point.

The resulting point cloud is colored based on all available imagery in the dataset.  Figure~\ref{fig:pointcloud_color}a shows a camera image of an example seen and Figure~\ref{fig:pointcloud_color}b shows the colored point cloud of the same environment.

% figure showing projection of points onto images
\begin{figure}
	\centerline{\includegraphics[width=0.5\linewidth]{pointcloud/gradlounge/pointcloud_color_diagram}}
	\caption{Determining the color of point $x$ based on camera imagery.}
	\label{fig:pointcloud_color_diagram}
\end{figure}

% show color pointcloud compared to camera image
\begin{figure}
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/camera}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_color}}
		\centerline{(b)}
	\end{minipage}

	\caption{An example point cloud colored by nearby camera imagery.}
	\label{fig:pointcloud_color}
\end{figure}

% partitioning pointclouds into separate levels
\section{Partitioning Point Cloud by Building Levels}
\label{sec:pointcloud_level_split}

Many methods described in this dissertation require explicit detection and partitioning of the different levels, or stories, in a scanned building.  Since our hardware system is human-mounted, the operator can walk up and down stairs during the data acquisition process, facilitating scans of multiple stories at once.  

Some localization systems that rely on 2D grid-maps of wall samples are capable of detecting when the operator moves from one level to another and automatically partition their output grid-maps accordingly~\cite{MITBackpack}.  We do not just want the elevation of each level, but the vertical extent as well.  In order to detect and separate building levels, we identify the primary floor and ceiling surfaces for each level.  This identification can be done either in the point cloud domain or after a full model has been generated.  The latter method is discussed in Chapter~\ref{ch:better_floorplans}.  Here, we discuss how level splitting can be accomplished via point clouds, which offers a faster if less accurate partitioning.

A histogram approach can be used to separate the point cloud by levels~\cite{Turner12,Turner14Journal}. Figure~\ref{fig:heighthist}a shows an example point cloud, colored by height, which contains multiple levels.  By computing a histogram along the vertical-axis of the point-cloud, it is possible to find heights with high point density, which indicates the presence of a large horizontal surface.  An example of this process is shown in Figure~\ref{fig:heighthist}b, where peaks in the histogram correspond to the floors and ceilings of each scanned level in the building.  Points scanned from above, in a downward direction, are used to populate a histogram to estimate the position of each floor, and the histogram used to estimate the position of each ceiling is populated by points scanned from below.  The local maxima of these two histograms show locations of likely candidates for floor and ceiling positions, which are used to estimate the number of scanned levels and the vertical extent of each level.  The candidate floor and ceiling heights are pruned by first taking the lowest floor maxima as the elevation of the first level's floor.  The first level's ceiling elevation is determined by the most populated ceiling maxima position that resides below the next maximal floor elevation.  This process is repeated for all levels, which allows for detection of both number of levels and their range extents.  Once levels are separated, they can be processed and analyzed separately.  Figure~\ref{fig:heighthist}c shows the final extruded mesh with all three scanned levels.

% show example histogram and partitioning
\begin{figure}

	\centerline{\begin{minipage}[c]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/points_LEFT}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.35\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/hist}}
		\centerline{(b)}\medskip
	\end{minipage}}
	\begin{minipage}[c]{0.9\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/mesh}}
		\centerline{(c)}
	\end{minipage}
	
	\caption[An example point-cloud partitioning by height.]{An example point-cloud partitioning by height: (a) the input point-cloud, showing geometry for three levels; (b) the vertical histogram showing estimates of each building level height; (c) the produced mesh of this building scan using method from Chapter~\ref{ch:floorplan}.}
	\label{fig:heighthist}

\end{figure}


%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Floorplan Generation
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Floor Plan Generation}
\label{ch:floorplan}

In this chapter, we discuss two methods to produce 2D floor plans of indoor building scans.  Both methods generate floor plan geometry by first generating a set of 2D wall samples.  Wall samples are represented by a set of points that locate areas of high likelihood of being large vertical surfaces.  We list the evolving techniques we use to generate wall samples in Section~\ref{sec:wall_sampling}.

Once wall sampling is performed, we have developed two different floor plan generation techniques.  In Section~\ref{sec:eigencrust}, we discuss a global optimization approach to generate watertight floor plan geometry based off of the Eigencrust algorithm~\cite{EigencrustShewchuk,Turner12}.  This method explicitly fits both straight and curved features into the floor plan geometry, in order to preserve curved building features.  In Section~\ref{sec:visigrapp}, we discuss our second floor plan generation approach.  This approach is adapted from our original algorithm, but no longer requires a global optimization step, allowing for faster processing~\cite{Turner14}.  Additionally, it explicitly partitions the floor plan geometry into individual rooms.  This method also introduces simplification mechanisms that allow for minimal elements to represent the geometry while still preserving the watertight, volumetric topology.

Lastly, in Section~\ref{sec:fp_results}, we show several results comparing each of these methods, including multi-story models showing how these methods can operate on datasets taken over several levels of a building environment.  We also show comparisons of our automatically generated results to the ground-truth blueprints of the scanned environments.

% wall sampling techniques
\section{2D Wall Sampling}
\label{sec:wall_sampling}

% overview of wall sampling: motivation
The input data used during floor plan generation consist of points in the ($x$,$y$) horizontal plane, which we call wall samples.  These points depict locations of walls or vertical objects in the environment.  We assume that interior environments satisfy ``2.5-Dimensional'' geometry:  all walls are vertically aligned, while floors and ceilings are perfectly horizontal.  In many application scenarios only 2D scanners operating in one plane are used, so this assumption is needed to extract 3D information about the environment.  Many mapping systems use a horizontal LiDAR scanner to estimate a map of the area as a set of wall sample positions, while refining estimates for scanner poses.  These mobile mapping systems often have additional sensors capable of estimating floor and ceiling heights at each pose~\cite{Backpack,Quadrotor}.  The input to our algorithm is a set of 2D wall samples, where each sample is associated with the scanner pose that observed it, as well as estimates of the floor and ceiling heights at the wall sample location.

% methods of wall sampling
Wall samples can be derived in three different ways.  First, histogram analysis can be performed on the 3D point cloud generated for a building environment as discussed in Chapter~\ref{sec:pointcloud}, in order to generate a 2D sampling of wall positions~\cite{Turner12}.  This technique is discussed in Section~\ref{ssec:ws_from_pc}.  Second, wall samples can be exported directly from the grid-map generated alongside a particle filter during the localization step of scan processing~\cite{NickJournal,Turner14}.  This method is described in Section~\ref{ssec:ws_from_pf}.  Third, wall samples can be generated from a processed 3D volumetric model of the environment.  This last option is described in detail in Chapter~\ref{ch:better_floorplans}.

% first wall sampling technique:  from a point cloud
\subsection{Generating Wall Samples from Point Clouds}
\label{ssec:ws_from_pc}

% describe the basic idea of histogramming to find wall samples
Given a 3D input point set, $P$, we wish to generate a floor plan for each story represented.  The coordinate system is defined so that the $z$-component represents height.  First, we compute how many stories are present in $P$ and partition $P$ into separate point sets $P_0$,~$P_1$,~...,~$P_f$ for each level by height, as discussed in Chapter~\ref{sec:pointcloud_level_split}.

For each story $P_k$, the method for computing wall samples is to subsample the full 3D point-cloud to a set of representative 2D points~\cite{Turner12,Turner14,Turner14Journal,Okorn09}.  This process cannot be done in a streaming fashion, but can provide accurate estimates for wall positions.  Such an approach is useful when representing dense, highly complex point clouds with simple geometry.  Under the 2.5D assumption of the environment, wall samples can be detected by projecting 3D points onto the horizontal plane.  Horizontal areas with a high density of projected points are likely to correspond to vertical surfaces.  Our goal is to find these wall locations with a sparse sampling of points $S_k \subseteq P_k$.  By defining a grid along the x-y plane, we can find coordinate locations that are likely to represent walls.  The width of each grid cell is denoted by $d_s$, which should be smaller than the minimum feature size expected in a floor plan.  A typical value for this parameter is $5$~cm.  This grid partitions $P_k$ laterally.  For a grid cell $g$, let the neighborhood set $N_g$ be the subset of points of $P_k$ that are within a horizontal distance of $d_s$ from the center of $g$, as shown in Figure~\ref{fig:grid_filtering}.  Thus a given point can be in the neighborhood of multiple grid cells.

% figure showing wall sampling process
\begin{figure}

\begin{minipage}{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=0.6\linewidth]{3dimpvt2012/wall_sampling/grid_filtering}}
\end{minipage}

\caption[Generating wall samples from 3D point clouds.]{A point set $P_k$ (small dots in blue) is partitioned by a grid in the $xy$-plane (lines in black).  For each grid cell, a neighborhood of points (medium dots in red) is computed and the median position of the points (large dot in green) is taken as a wall sample.  The normal of this neighborhood (vector in dark green) is computed using PCA.}
\label{fig:grid_filtering}

\end{figure}

% real-world example of wall sampling
\begin{figure}

\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/wall_sampling/BHH-Lobby_topdown}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/wall_sampling/BHH-Lobby_wall_sampling}}
  \centerline{(b)}\medskip
\end{minipage}

\caption[Wall sampling from static point cloud.]{(a) Point cloud of scanned area, viewed from above and colored by elevation; (b) wall sample locations generated from point cloud.  Clutter such as furniture or plants do not affect position of wall samples.}
\label{fig:bhh_wall_sampling_example}

\end{figure}

We perform the following checks on $N_g$ to determine whether this neighborhood represents a portion of a wall.  The first assertion is that a wall must be densely sampled, so if $|N_g|$ is less than a threshold, it is rejected. Our scanners create point-clouds with thousands of points per square meter of surface, so a threshold of $|N_g| \geq 50$ points is sufficient.  The second check is to verify the height of a wall.  The vertical support of $N_g$ must be at least the minimum wall height threshold. The value of this height threshold may vary depending on application, but a length of $2$ meters works well to capture permanent wall features while ignoring furniture and other interior clutter.  We also wish to verify that the distribution of points is uniform vertically, which signifies that a flat surface was captured.  For this uniformity check, we compute the histogram of the heights of $N_g$ with 16 bins and enforce that at least 12 of these must be non-empty.  Additionally, at least three of the highest four bins must be non-empty, which ensures a neighborhood extends all the way to the ceiling.  If these requirements are fulfilled, $g$ is said to represent a wall.  The median horizontal position for the elements of $N_g$ is computed.  If this median lies within the bounds of $g$, then it is taken as a ``wall sample'' for $g$.

The normal vector for this wall sample is calculated using Principal Components Analysis (PCA) \cite{PCA}.  Let $C$ be the $2 \times 2$ covariance matrix of the $xy$-positions of the elements of $N_g$ and $e_1, e_2$ be the eigenvectors of $C$ with corresponding eigenvalues $\lambda_1, \lambda_2$.  
Let $\lambda_1 \leq \lambda_2$.  The normal vector is chosen to be $\vec{n} = e_1$, as shown in Figure~\ref{fig:grid_filtering}.  The confidence of this normal estimate is computed as $c = 1 - \frac{2 \lambda_1}{\lambda_1 + \lambda_2}$.  If $c$ is less than 0.15, then the neighborhood points are not well-aligned and rejected as the location for a wall.

If pose information for the scanner is known for each point, this normal vector's direction may be flipped to guarantee that it points towards the laser scanner.  Any wall samples captured far away from the scanner are thrown away as outliers.  This distance is typically 5-10 meters.  If a feature is not scanned within this distance, it is only partially captured and cannot be accurately represented. Performing the above operation for all grid cells results in a set of wall sample points $S_k \subseteq P_k$.  

% houston wall samples figure
\begin{figure}
  \centering
  \includegraphics[width=0.98\linewidth]{visigrapp/algorithm/input_points/houston_wall_samples_with_labels}
  \caption[Wall samples from backpack point cloud.]{Example input wall samples of hotel hallways and lobby generated from a particle filter system. (a) Wall samples of full model; (b) close up of wall in model.}
  \label{fig:backpack_wall_sample_example}
\end{figure}

The result is a set of wall samples, where each wall sample is represented by its 2D position, the minimum and maximum height values of the points that sample represents, and the poses of the scanners that observed the sample location.  As we discuss later, these scanner poses provide crucial line-of-sight information that facilitate floor plan reconstruction.  Figure~\ref{fig:bhh_wall_sampling_example} shows an example of collecting wall samples from a point-cloud of a hotel lobby.  While most examples in this dissertation will be generated using our backpack-mounted scanning system, this point cloud was created with static scanners, to showcase the versatility of our system. An example of wall samples generated from backpack scans for a hotel hallway is shown in Figure~\ref{fig:backpack_wall_sample_example}.  As shown, even though the walls are well sampled, noise in the localization estimate causes noisy wall samples with outliers.

% second wall sampling technique:  from a particle filter
\subsection{Generating Wall Samples from Particle Filter Grid-Maps}
\label{ssec:ws_from_pf}

% using grid map
Many mapping systems use a horizontal LiDAR scanner to estimate a map of the area as a set of wall sample positions, while refining estimates for scanner poses.  Such systems perform Simultaneous Localization and Mapping (SLAM) by defining a rough grid-map of the area while also estimating the pose of the system via a particle filter~\cite{NickJournal,Quadrotor}.  This grid-map is often represented by a set of 2D position samples that indicate the presence of walls or other obstacles~\cite{ProbabilisticRobotics}.  In such particle filtering approaches, the grid-map can be used direction as a set of wall samples for floor plan model generation~\cite{Turner14}.  As discussed in Section~\ref{sec:visigrapp}, we have developed a technique for floor plan generation that is fast enough to work in a streaming, real-time fashion on this type of input data.  Particle filtering approaches to localization typically result in real-time mapping~\cite{fastslam03,toro07} and can therefore benefit from a real-time floor plan generation algorithm that delivers a live map of the environment. 

These mobile mapping systems often have additional sensors capable of estimating floor and ceiling heights at each pose~\cite{Backpack,Quadrotor}.  The input to our algorithm is a set of 2D wall samples, where each sample is associated with the scanner pose that observed it, as well as estimates of the floor and ceiling heights at the wall sample location.  In Section~\ref{sec:fp_results}, we show comparisons of floor plans generated with both types of wall samples:  those generated from point cloud data and those generated from particle filter grid-maps.

% eigencrust floor plan reconstruction
\section{Eigencrust Floor Plan Reconstruction}
\label{sec:eigencrust}

Given a point cloud representing a subset of the interior of a building that can cover multiple stories, we propose a three-step process to generate a watertight floor plan for each story in the point cloud by fitting line segments and curves to model features~\cite{Turner12}.  First, The output wall samples generated using the method described in Section~\ref{sec:wall_sampling} are smoothed to prevent double-surfacing effects, as discussed in Section~\ref{ssec:eigencrust_smoothing}.  Second, the Delaunay Triangulation of the generated wall samples is computed and each triangle is labeled as ``inside'' or ``outside'' using a 2D version of the Eigencrust algorithm~\cite{EigencrustShewchuk}.  This process is discussed in Section~\ref{ssec:eigencrust_triangulation}.  The edges that border these two labels are output as wall locations, as detailed in Section~\ref{ssec:eigencrust_boundary}.  Third, noise reduction is achieved by fitting these facets to line segments and curved components using Random Sample Consensus (RANSAC)~\cite{Ransac}, discussed in Section~\ref{ssec:eigencrust_fitting}.

\subsection{Smoothing Wall Samples}
\label{ssec:eigencrust_smoothing}

\begin{figure}[t]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/sample_smoothing_results}}
\end{minipage}

\caption[Smoothing wall samplings.]{Average filtering reduces occurrence of registration errors exposed in wall sampling.}
\label{fig:sample_smoothing_results}

\end{figure}

A crucial step in generating a point cloud is to register the poses of separate scan positions to a common coordinate system.  Any error in this process may result in duplicate instances of a scanned wall with slight misalignments.  A given wall position may be scanned from multiple disjoint poses, causing it to appear multiple times in the set of generated wall samples.  If the registration error is significant smoothing may be required to force these wall instances to become aligned~\cite{Turner12}.

For a model with wall samples set $S_k$, a given sample $s \in S_k$ with normal vector $\vec{n}$ has a smoothing set that is a subset of $S_k$ within an an-isometric neighborhood of $s$.  The position of $s$ is translated to be the mean position of this smoothing set. The smoothing set contains all samples that are (a) within some smoothing distance of $s$ along the $\pm \vec{n}$ direction, (b) no more than $3 d_s$ away from $s$ in the direction orthogonal to $\vec{n}$, and (c) whose normals are aligned to within $\frac{\pi}{8}$ radians of $\vec{n}$.  The smoothing distance must be less than the minimum allowable spacing between parallel walls, but at least as large as the expected registration error.  This step is not necessary if registration error is smaller than $d_s$.  An example of this smoothing process is shown in Figure~\ref{fig:sample_smoothing_results}.

\subsection{Labeling Triangulation of Wall Samples}
\label{ssec:eigencrust_triangulation}

We wish to determine the topology of the 2D sample set $S_k$ for each story $k \in \{ 0, 1, ..., f-1 \}$.  This task is accomplished using a 2D variant of the Eigencrust algorithm \cite{EigencrustShewchuk}.  This will partition 2D space into regions labeled ``inside'' and ``outside'' and export the borders between these regions.  These borders form the faces of a 2D simplicial complex, which are guaranteed to be watertight and not self-intersecting.  Eigencrust has been shown to be more robust to outlier samples than similar algorithms, such as Powercrust \cite{EigencrustShewchuk, Powercrust}.

\begin{figure}[t]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{3dimpvt2012/eigencrust/intersect_angles}}
\end{minipage}

\caption[Computing weighting between triangle pairs.]{The angles of intersection between two triangles $u$ and $v$, which share the edge $\overline{s_1 s_2}$. These values are used in computing weights between triangle pairs.}
\label{fig:intersect_angles}

\end{figure}

The Eigencrust algorithm first computes the 2D Delaunay Triangulation, $T$, for the sample set $S_k$ plus four bounding box corners.  Eigencrust constructs a sparse graph where the nodes represent triangles and edges are placed between the nodes with weights corresponding to the relative geometry of the triangles.  Edges with positive weights indicate that triangles should have the same labeling, while negative weights indicate that the triangles connected should be labeled oppositely.  Generalized eigensystems are solved in order to determine the best triangle labelings to fit these connections.  For our 2D variant of Eigencrust, we keep the same criteria for placing edge weights as in \cite{EigencrustShewchuk}, but modify the negative edge weight values.  We have additional information of normal vectors for each sample location. If a negative edge is placed between two triangles $u$ and $v$, we use the weighting:

\begin{equation}
w_{u,v} = - e ^ {4 + 4 cos \phi + 2 sin \theta_1 + 2 sin \theta_2}
\label{neg_edge_weight}
\end{equation}

where these parameters are shown in Figure~\ref{fig:intersect_angles}.  This weighting is very close to the one used in \cite{EigencrustShewchuk}.  The value $\phi$ is defined as the angle at which the circumcircles of $u$ and $v$ intersect.  The intersection of the triangles $u$ and $v$ is a line segment whose endpoints are samples $s_1$ and $s_2$, which have normal vectors $\vec{n}_1$ and $\vec{n}_2$ respectively.  The values of $\theta_1$, $\theta_2$ are defined to be the angles between $\overline{s_1 s_2}$ and $\vec{n}_1$, $\vec{n}_2$, respectively.  This weight is defined to have a large magnitude when both the normal vectors are perpendicular to the line $\overline{s_1 s_2}$, which is a strong indication of a wall in the original point cloud $P_k$.

\begin{figure}[t]

\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{3dimpvt2012/eigencrust/BHH-Lobby_wall_sampling_zoom}}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[height=4.2cm]{3dimpvt2012/eigencrust/BHH-Lobby_triangulation_zoom}}
  \centerline{(b)}
\end{minipage}

\caption[Example triangulation of wall samples.]{An example triangulation of wall samples: (a) a set of wall samples for a column; (b) each triangle is denoted as ``inside'' (yellow) or ``outside'' (green).}
\label{fig:eigencrust_triangulation}

\end{figure}

For each pair of samples $s$,$s' \in S_k$ such that ($s$,$s'$) is an edge of the Delaunay Triangulation $T$, let $u$ and $v$ be the poles of $s$, and $u'$ and $v'$ be the poles of $s'$.  Each of the pairs ($u$, $u'$), ($u$, $v'$), ($v$, $u'$), and ($v$, $v'$) are edges in $E$.  If a negative edge is not already defined, each of these pairs has positive edge weight:

\begin{equation}
w_{u,u'} = e ^ {4 - 4 cos \phi}
\label{pos_edge_weight}
\end{equation}

where $\phi$ is again defined as the angle at which the circumcircles of $u$ and $u'$ intersect~\cite{EigencrustShewchuk}.  Next, we can constrain some triangles to be interior or exterior based on a priori information.  The label ``inside'' refers to locations of open area within a building where a person can exist.  The term ``outside'' refers to all other areas, which include both the exterior of the building as well as the areas inside walls or other solid spaces.  We can immediately force the label of ``outside'' to any triangles connected to the bounding box corner vertices.  If the pose information for the scanner is available, we can force the label of ``inside'' to a subset of triangles.  First, we investigate whether the 2D line-of-sight from a scanner pose to a scan sample crosses triangles.  If the center 50\% of this laser travel distance crosses a triangle, that triangle is intersected by that scan line.  If a triangle is intersected by 10 or more scan lines, it is assumed to be ``inside''.  Second, all triangles that contain a pose position of a scanner are marked as interior.  If a mobile scanning system is used, then the path traversed by that system can also be used to mark interior triangles.  Each pair of adjacent pose positions represents a line segment in 2D space.  If both of these poses contributed scans to the wall sample set $S_k$, then any triangles that are intersected by this line segment are also constrained to be interior.

The remaining triangles are labeled inside or outside by solving generalized eigenvalue systems that minimizes the total positive edge weights that connect oppositely labeled triangles and negative edge weights that connect triangles with the same labeling~\cite{EigencrustShewchuk}.

The non-pole triangles are then put into a corresponding graph.  Since the edge-weights of this non-pole graph in the 3D Eigencrust algorithm use the aspect ratio of bounding triangular faces, there is not a direct translation of these weight calculations to 2D.  Thus the same weights are used as for the pole graph $G$, defined in Equations~\ref{neg_edge_weight} and~\ref{pos_edge_weight}.  A corresponding generalized eigenvalue system is solved for this non-pole graph, which provides a labeling for all triangles in $T$.  An example output of this triangulation labeling process is shown in Figure~\ref{fig:eigencrust_triangulation}.

All the ``outside'' nodes are collapsed into one outside super-node and all ``inside'' nodes are collapsed into one inside super-node, creating graph $G'$.  The nodes and edges of $G'$ are then converted into a matrix $L \in \mathbb{R}^{|G'| \times |G'|}$.  Each element is defined as $L_{ij} = L_{ji} = -w_{i,j}$ with the diagonal $L_{ii} = \sum_{j \neq i} |L_{ij}|$.  The diagonal matrix $D$ is the same size as $L$ and has the same diagonal elements as $L$.  These values are used to find the solution to the generalized eigensystem $L x = \lambda D x$.  The eigenvector $x$ associated with the smallest non-zero eigenvalue $\lambda$ is taken as the result~\cite{EigencrustShewchuk}.

Each element of $x$ corresponds to a pole triangle in $T$.  The $i$'th triangle is labeled as ``inside'' or ``outside'' based on the sign of $x_i$.  This step partitions the poles into the inside/outside categories.  To label all the non-poles, we construct another graph.  All the ``outside'' poles in $G'$ are collapsed into one outside super-node and all the ``inside'' poles of $G'$ are collapsed into one inside super-node.  All the non-pole triangles are also nodes of this non-pole graph, and edges are defined between every pair of touching triangles, assuming both triangles are not part of the same super-node. Any edge between the two super-nodes are negatively weighted according to Equation~\ref{neg_edge_weight}.  Any edge involving at least one non-pole is positively weighted according to Equation~\ref{pos_edge_weight}.  The non-pole graph is then converted to the matrix $H$ in the same manner as described above for $G'$.  Let $D_H$ be the diagonal component of the matrix $H$.  Solving the concordant generalized eigensystem $H y = \mu D_H y$ will yield the eigenvector $y$, which gives us the inside/outside labelings for each of the non-pole triangles as well~\cite{EigencrustShewchuk}. An example output of this triangulation process is shown in Figure~\ref{fig:eigencrust_triangulation}.

\subsection{Forming Floor Plan Boundary Edges}
\label{ssec:eigencrust_boundary}

\begin{figure}[t]

\begin{minipage}[b]{0.49\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/eigencrust/eigencrust_cleaning_1}}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.49\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/eigencrust/eigencrust_cleaning_3}}
  \centerline{(b)}
\end{minipage}

\caption[Post-processing floor plan triangulation.]{Original triangulation labeling (a) is cleaned by removing small unions and sharp protrusions (b).}
\label{fig:eigencrust_postprocessing}

\end{figure}

We have found the following post-processing clean-up techniques useful for increasing the quality of the output model.  First, we represent this topology of triangles as a set of unions.  Any subset of triangles with the same label that forms a connected component is represented as a single union using the Union-Find algorithm \cite{Unionfind}.  Unions that are composed of a small number of triangles are most likely mislabeled, since the smallest building features should still be sufficiently sampled.  The appropriate cut-off value depends on the value of $d_s$ used to form samples, but a value of 30 triangles is typically used.  All inside unions that have fewer than this many triangles are relabeled as outside. The set of unions is recomputed and then all outside unions that meet this criterion are relabeled as inside. Another post-processing smoothing process is to remove jagged edges from the boundaries between labelings.  Every triangle has three neighboring triangles, each sharing one of its edges.  If a triangle $t \in T$ has only one neighbor that shares the same labeling and that neighbor is connected to $t$'s shortest edge, then $t$ is considered to be a protrusion.  It is unlikely for building geometry to be correctly represented by such a protrusion, so all protrusions labeled as inside are relabeled as outside. After this relabeling, all outside protrusions are found and relabeled as inside.  An example of this processing is shown in Figure~\ref{fig:eigencrust_postprocessing}.

The set of edges in $T$ that are shared by an ``inside'' triangle and an ``outside'' triangle are exported as boundary edge set, $B$.  Each element in $B$ is a line segment, which in total constitute a water-tight floor plan of the scanned area.

\subsection{Fitting Edges With Lines and Curves}
\label{ssec:eigencrust_fitting}

Since there may exist areas in the building geometry that are insufficiently scanned or errors incurred during processing, it is important to utilize common architectural patterns to reduce these errors.  Additionally, applications may require a floor plan to be composed of parametric lines and curves.

Local model-fitting approaches such as region growing are sub-optimal because unconnected architectural features may use the same geometric model.  For example, the back walls in a row of offices may lie on the same plane, even though they are in different rooms. Since we need to fit both curves and straight lines simultaneously, a reasonable technique for this situation is one that is non-local and flexible, such as Random Sample Consensus (RANSAC)~\cite{Ransac}.

We apply RANSAC to the subset of samples used as vertices in the boundary edges $B$.  Each iteration randomly picks three samples from this set that have not yet been associated with a model.  These three points uniquely define a circle.  A line of best fit can also be obtained for these points.  Both the circle and the line models are compared to the subset of samples still unassigned.  Inlier sets for both of these models are computed.  An inlier is an unassigned sample that is both within a threshold distance of a model and whose normal vector is within a threshold angle to the model's normal vector at that location.  Only models that exceed a specified minimum number of inliers are considered.  The line or circle model found with the smallest average error is returned as a valid model and its inlier vertices are no longer considered for subsequent models.

\begin{figure}[t]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[height=5.5cm]{3dimpvt2012/curvefit/circle_segments_example_highlighted}}
\end{minipage}

\caption[Fitting curves to floor plan walls.]{The walls of a circular room are fit to the same circle model (shown highlighted in teal), even though they are comprised of several connected components. }
\label{fig:eigencrust_circle_segments_example}

\end{figure}

\begin{figure}[t]

\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/curvefit/backpack_cory_three_1_final_precurvefit}}
  \centerline{(a)}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/curvefit/backpack_cory_three_1_final_curvefit}}
  \centerline{(b)}
\end{minipage}

\caption[Sharpening floor plan corners with RANSAC.]{(a) Watertight wall edges; (b) curve-fitting via RANSAC reduces sample location error and sharpens corners.}
\label{fig:eigencrust_wall_straightening}

\end{figure}

This process continues until no new parametric model can be found.  The result is a set of models, where each model has a set of inlier samples and a parametric representation of either a line or a circle.  The topology defined in $B$ has not been altered, so its edge elements can be used to partition the inlier vertices of each model into a set of connected components, again with Union-Find~\cite{Unionfind}.  Any of these connected components composed of too few samples is most likely a misclassification.  Thus the elements of any of these components with fewer than 15 samples are reset to be unassigned.  Additionally, we wish to encourage models to extend along the edges defined in $B$.  If an unassigned sample is within 15 edge hops to samples belonging to a model, that sample is associated with the closest model.  These two steps encourages outlier samples to belong to models that are topologically close.  These steps also grow models to be adjacent, encouraging sharp corners.

Once the revised parametric models and their respective inliers are computed, the inlier samples and edges in $B$ are replaced by edges that conform exactly to their models' locations.  This process reduces the overall number of samples and removes small perturbation errors from the floor plan.  Figure~\ref{fig:eigencrust_circle_segments_example} demonstrates the results of this process, which fits a circle to the walls of a round room with several entrances. Figure~\ref{fig:eigencrust_wall_straightening} shows how this process can also be applied to straight walls.

\subsection{Conclusion}
\label{ssec:eigencrust_conclusion}

This section details a technique to generate 2D architectural floor plans.  By projecting each story's point set to two dimensions and performing a density analysis, we can yield an increase in wall location accuracy than by using a direct 3D approach. In Section~\ref{sec:fp_results}, we show several example models generated using this technique, and compare these models both to the ground-truth blueprints of the building as well as to other automatic as-built floor plan generation techniques.

\section{Floor Plan Generation with Room Partitioning}
\label{sec:visigrapp}

In this section, we present a technique to automatically generate accurate floor plan models at real-time speeds for indoor building environments~\cite{Turner14}.  This technique provides an expansion on the method discussed in the previous section.  In Section~\ref{ssec:visigrapp_carving}, we discuss how we compute the interior space of the 2D floor-plan, which defines the resultant building geometry.  The partitioning of this interior space into separate rooms is then considered in Section~\ref{ssec:room_label}.  Using these room labels, the geometry can be simplified, as described in Section~\ref{ssec:fp_simplify}.  In Section~\ref{ssec:fp_extrusion}, the final 2D geometry is extruded into an exported 3D model.

\subsection{Defining Interior Area}
\label{ssec:visigrapp_carving}

% carving a floor plan
\begin{figure}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{visigrapp/algorithm/carving/carving_full_figure}
  \caption[Carving process to find interior triangles.]{Example of carving process to find interior triangles:  (a) wall samples (in blue) with path of scanner (in green); (b) Delaunay Triangulation of wall samples; (c) laser scans from each pose (in red); (d) triangles that intersect with laser scans (in pink), used as interior triangles, with building model border (in blue).}
  \label{fig:visigrapp_floorplan_creation}
\end{figure}

% introduction
We generate a floor plan by partitioning space into {\it interior} and {\it exterior} domains.  The interior space of the floor plan is computed using the input wall samples.  The desired output consists of not only the architectural features such as walls, but also a volumetric representation of the environment such as rooms and hallways.  The exterior represents all space outside of the building, space occupied by solid objects, or space that is unobservable.  Once this partitioning is completed, as described below, the boundary lines between the interior and exterior are used to represent the exported walls of the floor plan.

% carving process using pose positions  
The input samples are used to define a volumetric representation by generating a Delaunay Triangulation in the plane.  Each triangle is labeled either interior or exterior by analyzing the line-of-sight information of each wall sample.  Initially, all triangles are considered exterior.  Each input wall sample, $p \in P$, is viewed by a set of scanner positions, $S_p \subseteq \mathbb{R}^2$.  For every scanner position $s \in S_p$, the line segment $(s,p)$ denotes the line-of-sight occurring from the scanner to the scanned point during data collection.  No solid object can possibly intersect this line, since otherwise the scan would have been occluded.  Thus, all triangles intersected by the line segment $(s,p)$ are relabeled to be interior.

This process carves away the interior triangles with each captured scan.  Since these scans are captured on a mobile scanner, the scanner poses are ordered in time.  In order for the system to traverse the environment, the line segment between adjacent scanner poses must also intersect only interior space.  In addition to carving via scanner-to-scan lines, the same carving process is performed with scanner-to-scanner line segments.

Figure~\ref{fig:visigrapp_floorplan_creation} demonstrates an example of this process.  Figure~\ref{fig:visigrapp_floorplan_creation}a shows the input wall samples, in blue, as well as the path of the mobile mapping system, in green.  These points are triangulated, as shown in Figure~\ref{fig:visigrapp_floorplan_creation}b.  The line-of-sight information is analyzed from each pose of the system, demonstrated by the laser scans from each pose to its observed wall samples in Figure~\ref{fig:visigrapp_floorplan_creation}c.  The subset of triangles that are intersected by these laser scans are considered interior.  The interior triangles are shown in pink in Figure~\ref{fig:visigrapp_floorplan_creation}d, denoting the interior volume of the reconstructed building model.  The border of this building model is shown in blue, denoting the estimated walls of the floor plan.

% checking for occluding wall samples
The above method may result in errors if there is mis-registration in the localization of the poses of the scanners during the data collection.  For instance, if opposite sides of the same wall are scanned, this wall appears in the output with some thickness, $t$.  If the estimated positions of the scanners on either side of the wall are inaccurate, the observed wall thickness varies by the component of this localization error orthogonal to the wall plane.  If this error is greater than $t$, then every triangle associated with the wall may be labeled interior, and the wall is carved away entirely from the output.  In order to prevent these topological errors, we check for occlusions when carving each line segment $(s,p)$.  If another wall sample $p'$ is located in between the positions of $s$ and $p$, then the line segment is truncated to $(s,p')$.  Thus, no features captured by wall samples are ever fully carved away, which preserves the details of the environment.

% room labeling section
\subsection{Labeling Rooms Within the Floor Plan}
\label{ssec:room_label}

A novel contribution of this technique is the use of room labeling to enhance building models, e.g.\ for thermal simulations of interior environments~\cite{Turner14,EnergyPlus}.  One motivation for existing work has been to capture line-of-sight information for fast rendering of building environments~\cite{WalkthroughRendering}.  This technique requires axis-aligned rectilinear building geometry, which often is not a valid assumption.  Others have partitioned building environments into sub-map segments with the goal of efficient localization and tracking~\cite{SpectralClustering}.  This approach is meant to create easily recognizable subsections of the environment, whereas our proposed room labeling technique uses geometric features to capture semantic room definitions for both architectural and building energy simulation applications.

% figure showing room seed identification
\begin{figure}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{visigrapp/algorithm/room_seeds/room_seeds_full_figure}
  \caption[Example room seed partitioning.]{Example room seed partitioning: (a) interior triangulation; (b) the room seed triangles, and their corresponding circumcircles; (c) room labels propagated to all other triangles.}
  \label{fig:roomlabeling}
\end{figure}

Once the volume has been partitioned into interior and exterior domains, the boundary between these domains can be exported as a valid floor plan of the environment.  Keeping volumetric information can also yield useful information, such as a partitioning of the interior into separate rooms.

% definition of a room
We define a {\it room} to be a connected subset of the interior triangles in the building model.  Ideally, a room is a large open space with small shared boundaries to the rest of the model.  Detected rooms should match with real-world architecture, where separations between labeled rooms are located at doorways in the building.  Since doors are often difficult to detect, or not even present, there is no strict mathematical definition for a room, so this labeling is heuristic in nature.

We model room labeling as a graph-cut problem.  First, a rough estimate for the number of rooms and a seed triangle for each room is computed.  A seed triangle is representative of a room, where every room to be modeled has one seed triangle.  These seeds are used to partition the remainder of interior triangles into rooms.  This process typically over-estimates the number of rooms, so prior knowledge of architectural compliance standards is used to evaluate each estimated room geometry.  Using this analysis, the number of ill-formed rooms is reduced, providing an update on the original seed points.  This process is repeated until the set of room seeds converges.

%\subsubsection{Forming Room Seeds}
%\label{sssec:room_seeds}

We use the Delaunay property of the triangulation to identify likely seed triangle locations for room labels, which states that no vertex is strictly inside the circumcircle of any triangle in this triangulation.  If we assume that the input wall samples represent a dense sampling of the building geometry, this property implies that the circumcircles of none of the interior triangles intersect the boundary walls of the carved floor plan, forcing these circles to represent only interior area.  This make-up allows each triangle's circumradius to provide an estimate of the local feature size at its location on the floor plan boundary polygon.  Given the example interior triangulation shown in Figure~\ref{fig:roomlabeling}a, the highlighted triangles in Figure~\ref{fig:roomlabeling}b show the chosen seed locations.

Triangles with larger circumradii are likely to be more representative of their rooms than those with smaller circumradii.  We form the initial set of room seeds by finding all triangles whose circumcircles are local maxima.  Specifically, given the set of interior triangles $T$, each triangle $t \in T$ has circumcircle $c_t$, which is tested against every other circumcircle in $T$ that is intersected by $c_t$.  If $c_t$ has the largest radius of any intersecting circumcircle, then $t$ is considered a seed for the room labeling.  This process selects the largest triangles that encompass the space of rooms as the seeds for room labeling.  Figure~\ref{fig:roomlabeling}b shows example seed triangles and their corresponding circumcircles.  The result is an estimate of the number of rooms and a rough location for each room.

%\subsubsection{Partitioning Room Labels}

% refining and combining rooms
\begin{figure}
  \centering
  \includegraphics[width=0.75\linewidth]{visigrapp/algorithm/room_merging/room_merging_full_figure}
  \caption[Room labeling refinement example.]{Room labeling refinement example: (a) initial room labels; (b) converged room labels}
  \label{fig:roommerging}
\end{figure}

Let $K$ be the number of room seeds found, with the seed triangles denoted as $t_1,\,t_2,\,...,\,t_K$.  We wish to partition all triangles in $T$ into $K$ rooms.  This step can be performed as a graph-cut on the dual of the triangulation.  Specifically, each triangle $t \in T$ is a node in the graph, and the edge weight between two abutting triangles is the length of their shared side.  Performing a min-cut on this graph partitions rooms to minimize inter-room boundary length.  In other words, rooms are defined to minimize the size of doors.  This process propagates the room labels to every triangle, and the boundaries between rooms are composed of only the smallest edges in the triangulation $T$.  The result of this process is shown in Figure~\ref{fig:roomlabeling}c.

A greedy algorithm can alternatively be employed to flood-fill rooms.  To do so, we initialize the labeling so that each seed triangle $t_i$ has a unique label, while all non-seed triangles are unlabeled.  We then construct a priority queue of all edges in $T$ that separate a labeled triangle from an unlabeled triangle, sorted by decreasing edge length.  Iterating through this queue, the top value represents the longest edge in the queue, adjoining a labeled triangle $u \in T$ and unlabeled triangle $v \in T$.  We then pop this edge, propagate the label from $u$ to $v$, then push all edges of $v$ adjoining unlabeled triangles onto the queue.  

%\subsection{Refining Rooms}
%\label{sec:refiningrooms}

Room labels partition $T$ into a set of rooms $R = \{R_1,\,R_2,\,...,\,R_K\}$, where each room $R_i$ contains a disjoint subset of $T$ and has seed triangle $t_i$.  The initial room seeds over-estimate the number of rooms, since a room may have multiple local maxima.  This case is especially true for long hallways, where the assumption that one triangle dominates the area of the room is invalid.  An example is shown in Figure~\ref{fig:roomlabeling}c, where two lower rooms, shown in green and purple, are properly labeled, but their adjoining hallway is broken into three subsections.  The solution is to selectively remove room seeds and redefine the partition.

A room is considered a candidate for merging if it shares a large perimeter with another room.  Ideally, two rooms sharing a border too large to be a door should be considered the same room.  By Americans with Disabilities Act Compliance Standards, a swinging door cannot exceed 48 inches in width~\cite{ADACompliance}.  Accounting for the possibility of double-doors, we use a threshold of 2.44 meters, or 96 inches, when considering boundaries between rooms.  If two rooms share a border greater than this threshold, then the seed triangle with the smaller circumradius is discarded.  This process reduces the value of $K$, the number of rooms, while keeping the interior triangulation $T$ unchanged.  With a reduced set of room seeds, existing room labels are discarded and the process of room partitioning is repeated.  This iteration repeats until the room labeling converges.

Another way room labels are refined is by comparing the path of the mobile mapping system to the current room labeling for each iteration.  The mobile scanning system does not necessarily traverse every room, and may only take superficial scans of room geometry passing by a room's open doorway.  Since the room is not actually entered, the model is unlikely to capture sufficient geometry, and so only a small handful of wall samples are acquired for such a room.  It is desirable to remove this poorly scanned area from the model rather than keeping it as part of the output.  After each round of room partitioning, if none of the triangles in a room $R_i$ are intersected by the scanner's path, then we infer that room has not been entered.  The elements of $R_i$ are removed from the interior triangulation $T$.  Since the topology of the building model is changed, the set of room seeds is recomputed in this event and room labeling is restarted.  This process will also remove areas that are falsely identified as rooms, such as ghost geometry generated by windows and reflective surfaces, which cause rooms to be replicated outside the actual model.

Figure~\ref{fig:roommerging} shows an example of the room refinement process for the hallways and classrooms in an academic building. Figure~\ref{fig:roommerging}a shows the initial room seeds that were found based on circumcircle analysis.  The hallways of this building are represented by several room labels, but after room label refinement as shown in Figure~\ref{fig:roommerging}b, the hallways are appropriately classified.  Additionally, rooms that are insufficiently scanned and represented with triangulation artifacts are removed from the model in the manner described above.

% simplification of floor plan geometry
\subsection{Simplification of Floor Plan Geometry}
\label{ssec:fp_simplify}

The interior building model is represented as a triangulation of wall samples, which densely represent the building geometry.  In many applications, it is useful to reduce the complexity of this representation, so that each wall is represented by a single line segment.  This step is often desirable in order to attenuate noise in the input wall samples or to classify the walls of a room for application-specific purposes.  The goal is to simplify the wall geometry while preserving the general shape and features of the building model.

We opt to simplify walls using a variant of QEM~\cite{QEM,Turner14}.  Since this mesh is in the plane, only vertices incident to the model boundary are considered for simplification.  The error matrix $Q_v$ of each boundary vertex $v$ is used to compute the sum of squared displacement error from each adjoining line along the boundary polygon.  Since error is measured via distance away from a line in 2D, each $Q_v$ has size $3 \times 3$, and is defined as:

\begin{equation}
Q_v = \sum_{l \in lines(v)} E_l
\end{equation}

where $E_l$ is defined from the line equation $ax + by + c = 0$, with $a^2 + b^2 = 1$:

\begin{equation}
E_l = \left[ \begin{array}{c c c}
a^2 & ab & ac \\
ab & b^2 & bc \\
ac & bc & c^2 \end{array} \right]
\end{equation}

The simplification of the boundary proceeds in a similar manner to QEM, but if a wall vertex $v$ is contained in multiple rooms or if it is connected by an edge to a vertex that is contained in multiple rooms, then it is not simplified.  This constraint is used to preserve the fine details of doorways between rooms, while freely simplifying walls that are fully contained within one room.  Wall edges are iteratively simplified until no simplification produces error of less than the original wall sampling resolution, $r$.  Thus, walls are simplified while preserving any geometry features of the building interior.

Since we are interested in preserving the 2D triangulation $T$ of the building model, in addition to the boundary polygon, every edge simplification is performed by collapsing an interior triangle.  This computation simplifies the boundary polygon of the model while still preserving the room labeling of the model's volume.  These triangle collapses do not preserve the Delaunay property of the triangulation, but do preserve the boundaries between room volumes, which is more desirable in the output.

We can compare this method of simplification to the RANSAC fitting described in the previous method, in Section~\ref{sec:eigencrust}.  By using local simplification, each modification to the floor plan geometry only requires access to a small number of elements, allowing for rapid simplification.  With the RANSAC method, each adjustment is very expensive, since it must iterate several times over the entire set of input wall samples.  The one limitation to the QEM simplification method is that it does not support circular or curved surfaces.  This limitation, however, is reasonable, since curved features can be approximated with a set of piecewise-planar walls.

% floor plan height extrusion
\subsection{Extruding Floor Plan Geometry to 2.5D Models}
\label{ssec:fp_extrusion}

% example simplification and height extrusion
\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{visigrapp/algorithm/height_extrusion/height_extrusion_full}
  \caption[Example of creating a 3D extruded mesh from 2D wall samples.]{Example of creating a 3D extruded mesh from 2D wall samples:  (a) walls of generated floor plan with estimated height ranges; (b) floor and ceiling heights are grouped by room; (c) simplification performed on walls; (d) floor and ceiling triangles added to create a watertight mesh.}
  \label{fig:heightextrusion}
\end{figure}

As mentioned in Section~\ref{sec:wall_sampling}, each input wall sample also references the vertical extent for the observed scans at that location.  This information can be used to convert the labeled 2D interior building model to a 2.5D extruded model, by using the minimum and maximum height values for each scan as an estimate of the floor and ceiling heights, respectively~\cite{Turner14}.

Since these wall samples are collected using 2D planar scanners in an environment containing clutter, the minimum and maximum heights associated with each point are noisy.  Figure~\ref{fig:heightextrusion}a shows an example room with these initial heights.  To produce aesthetically-pleasing models, each room uses a single floor height and a single ceiling height.  This assumption is reasonable since the goal of this processing is to produce a simplified building mesh.  This step demonstrates the utility of room labeling to modeling.  The height range for each room is computed from the median floor and ceiling height values of that room's vertices.  An example is shown in Figure~\ref{fig:heightextrusion}b and the corresponding result from the simplification process from Section~\ref{ssec:fp_simplify} is demonstrated in Figure~\ref{fig:heightextrusion}c.

The 2D triangulation of a room is then used to create the floor and ceiling mesh for that room, with the boundary edges of the triangulation extruded to create rectangular vertical wall segments.  The result is a watertight 3D mesh of the building, capturing the permanent geometry in an efficient number of triangles.  Figure~\ref{fig:heightextrusion}d shows an example of this watertight extruded geometry, including the effects of wall boundary simplification on the resulting extruded mesh.

\section{Results}
\label{sec:fp_results}

% overview of results
In this section, we show several results for each of the methods described in this chapter, as well as comparing these methods to each other and to ground truth models of the scanned environments.  We first detail results of the eigencrust floor plan generation method in Section~\ref{ssec:eigencrust_results}.  Next, we detail results of the room partitioning floor plan generation method in Section~\ref{ssec:visigrapp_results}.  Lastly, we compare these methods in Section~\ref{ssec:fp_results_compare}.

% eigencrust results
\subsection{Eigencrust Results}
\label{ssec:eigencrust_results}

%%%%% CORY THREE RESULTS %%%%%

\begin{figure}[p]

% Floor #2
\centering
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.7\linewidth]{3dimpvt2012/cory_three_results/cory_three_2_wall_samples}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.7\linewidth]{3dimpvt2012/cory_three_results/backpack_cory_three_2_final_triangles}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.7\linewidth]{3dimpvt2012/cory_three_results/cory_three_2_boundary_snapped}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.7\linewidth]{3dimpvt2012/cory_ground_truth/03_4_overlay}}
\end{minipage}
\centerline{(a)}
\linebreak

% Floor #1
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_three_results/cory_three_1_wall_samples}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_three_results/backpack_cory_three_1_final_triangles}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_three_results/cory_three_1_boundary_snapped_labeled}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_ground_truth/03_3_overlay}}
\end{minipage}
\centerline{(b)}
\linebreak

% Floor #0
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_three_results/cory_three_0_wall_samples}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_three_results/cory_three_0_triangles}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_three_results/cory_three_0_boundary_snapped_labeled}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/cory_ground_truth/03_2_overlay}}
\end{minipage}
\centerline{(c)}
\linebreak

\caption[Example floor plan on three-story building.]{Example floor plans on three-story building: (a-c) Processing of each story, with (left to right) wall sample locations, triangulation labeling, watertight curve-fit model, and comparison against ground-truth blueprints.}
\label{fig:cory_three_results}

\end{figure}

%%% BHH HALLWAY %%%

\begin{figure}

% point cloud
\centering
\begin{minipage}[b]{0.335\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_hallway/bhh-hallway_points}}
\end{minipage}
\begin{minipage}[b]{0.3\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_hallway/bhh-hallway_points_zoom}}
\end{minipage}
\linebreak 
\linebreak 

% wall samples
\begin{minipage}[b]{0.3\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_hallway/bhh_hallway_wall_samples_boxed}}
\end{minipage}
\begin{minipage}[b]{0.3\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_hallway/bhh_hallway_wall_samples_zoom}}
\end{minipage}
\linebreak
\linebreak 

% boundary
\begin{minipage}[b]{0.3\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_hallway/bhh_hallway_boundary_boxed}}
  \centerline{(a)}
\end{minipage}
\begin{minipage}[b]{0.3\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_hallway/bhh_hallway_boundary_snapped_zoom}}
  \centerline{(b)}
\end{minipage}
\linebreak 
\linebreak 

\caption[Hallway of hotel, captured with mobile scanner.]{Hallway of hotel, captured with mobile scanner.  (a) The full floor plan; (b) a close-up of a hallway intersection.  The point cloud (top) was converted into wall sampling locations (middle) and boundary edges were fit to these samples (bottom).}
\label{fig:bhh_hallway_results}

\end{figure}

%%% BHH Lobby %%%

\begin{figure}[t]

\centering
\begin{minipage}[b]{0.40\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_lobby/BHH-Lobby_topdown}}
  \centerline{(a)}
\end{minipage}
\begin{minipage}[b]{0.40\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_lobby/BHH-Lobby_points_center}}
  \centerline{(b)}
\end{minipage}

% wall samples
\begin{minipage}[b]{0.35\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_lobby/bhh_lobby_wall_samples}}
  \centerline{(c)}
\end{minipage}
% boundary
\begin{minipage}[b]{0.35\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{3dimpvt2012/bhh_lobby/bhh_lobby_boundary_snapped}}
  \centerline{(d)}
\end{minipage}

\caption[Hotel lobby, captured with a static scanner.]{Hotel lobby, captured with a static scanner. (a) The captured point-cloud; (b) a close-up of the center columns; (c) the wall samples extracted from this point cloud; (d) the set of wall boundary edges.}
\label{fig:bhh_lobby_results}

\end{figure}

In order to demonstrate the accuracy of the floor plan generation method discussed in Section~\ref{sec:eigencrust}, we apply it to a dataset representing the hallways in three stories of an academic office building, as shown in Figure~\ref{fig:cory_three_results}.  The input point-set has 17.4 million elements and the resultant wall samplings for each story are 5,544 samples, 5,357 samples, and 3,265 samples from bottom story to top respectively.  Our test code, written in unoptimized MATLAB and run on a personal laptop, processes these three stories in 371.4 seconds total.  These results show how our algorithm enforces water-tightness by fitting walls to the ends of any unscanned sections, such as the hallways labeled 1 and 2 in the third column of Figure~\ref{fig:cory_three_results}c.  The resultant models are compared against the current building floor plans, as shown in the fourth column of Figure~\ref{fig:cory_three_results}.  In areas where the wall sampling is dense, our generated floor plan is accurate with respect to these ground-truth blueprints.

Figure~\ref{fig:bhh_hallway_results} shows this same floor plan reconstruction method, applied to a scan taken of a single level of a hotel.  The input point-cloud has 5.8 million elements and the wall sampling produced 13,557 samples.  This set demonstrates a model that is composed of both curves and straight line-segments. Figure~\ref{fig:bhh_lobby_results} shows the lobby of this building, which has been statically scanned.  This dataset has an input point-cloud of 2.7 million points from which 3,568 2D wall samples were extracted. This model demonstrates the ability of our algorithm to capture structural features such as columns.  It shows how areas of high curvature can be modeled just as effectively as straight walls.

While enforcing watertight models allows for walls to be created in areas that are not scanned, it may also cause undesired homography changes.  If two parallel walls are arbitrarily close, the space between them may become incorrectly labeled with neither wall existing in the final model.  An example of this issue can be seen in the north-west stairway in Figure~\ref{fig:cory_three_results}(d), labeled 3.  Another limitation is that the walls placed in areas of sparse scanning may be inaccurate.  The corridor in the south-east corner of the floor plan labeled 2 in Figure~\ref{fig:cory_three_results}(c) is not fully scanned, so a wall is represented at the edge of the scan area, even though the hallway continues much further in reality.  While the sparsity of a scan is typically due to the placement of the scanning system, it can also be caused by the partitioning of the original point-cloud into separate stories.  Note that the south-west stairwell in the same model, labeled 1, is sparsely sampled and as a result the north wall is incorrectly modeled at an angle.  In this case, the wall in question was thoroughly scanned, but higher in the stairway on the next story.  The immediate partitioning of levels in a point-cloud causes cross-level scans to become under-represented and introduces sparsity into the wall sampling.

% comparison of results
\subsection{Comparing Floor Plan Results}
\label{ssec:fp_results_compare}

% figure comparing eigencrust and visigrapp
\begin{figure}[h]

	% wall samples
	\centering
	\begin{minipage}[b]{0.3\linewidth}
	\centering
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/compare/wall_samples_cory3story_level0}}
	\centerline{(a)}
	\end{minipage}%
	\hfill
	% eigencrust
	\begin{minipage}[b]{0.3\linewidth}
	\centering
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/compare/eigencrust_cory3story_level0}}
	\centerline{(b)}
	\end{minipage}%
	\hfill
	% visigrapp
	\begin{minipage}[b]{0.3\linewidth}
	\centering
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/compare/visigrapp_cory3story_level0}}
	\centerline{(c)}
	\end{minipage}

	% caption and label
	\caption[Comparison of proposed floor plan methods.]{Comparison of the floor plan generation methods described in Sections~\ref{sec:eigencrust} and~\ref{sec:visigrapp}:  (a) the wall samples of an office hallway environment; (b) the floor plan generated by the method described in Section~\ref{sec:eigencrust}; (c) the floor plan generated by the method described in Section~\ref{sec:visigrapp}.}
	\label{fig:fp_eigencrust_visigrapp_compare}

\end{figure}

% compare pf and pc floorplans in a figure
\begin{figure}

	% wall samples
	\centering
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/dq/oceanview_office}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/dq/oceanview_office_pf_dq}}
	\centerline{(b)}
	\end{minipage}

	% floorplan
	\centering
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/2d/snapshot_oceanview_office_no_triangles00}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/2d/oceanview_office_pf_floorplan}}
	\centerline{(b)}
	\end{minipage}

	% caption and label
	\caption[Comparison between floor plans generated from point clouds and grid-maps.]{Comparison between floor plans generated using wall samples from point clouds and particle filter grid-maps:  (a) Wall samples of a small office complex generated from 3D point cloud; (b) wall samples generated from particle filter grid-map; (c) floor plan generated using wall samples from (a); (d) floor plan generated using wall samples from (b).}
	\label{fig:pf_pc_compare}

\end{figure}

% compare eigencrust and visigrapp
The floor plan methods described in this chapter use different techniques to accomplish the same goal of exporting 2D floor plan geometry.  Although both methods are volumetric in nature, they result in dramatically different levels of quality in their output.  A comparison between the two methods is made in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}.  The input wall samples for an office hallway environment are shown in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}a.  From these wall samples, a floor plan was generated using both the methods described in Sections~\ref{sec:eigencrust} and~\ref{sec:visigrapp}, shown in Figures~\ref{fig:fp_eigencrust_visigrapp_compare}b and~\ref{fig:fp_eigencrust_visigrapp_compare}c, respectively~\cite{Turner12,Turner14}.

The outputs of this methods are noticeably different in several locations.  First, the location marked (1) indicates a corner of a hallway.  Since the floor plan method described in Section~\ref{sec:eigencrust} performs explicit line fitting on the output walls, the corner of this hallway is much simpler than when compared to that same location on the output of the method described in Section~\ref{sec:visigrapp}.  However, this simplification results in a loss of the inlet geometry of doorways in that hallway.  Next, the location marked (2) shows a dramatic topology error in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}b, whereas the topology is correct in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}c.  This error is due to the eigencrust computation performed by the former method, which can lead to topologically inconsistent labeling of narrow features such as a thin wall between two rooms.  Next, location (3) is similar to the location marked by (1), in that it is a hallway corner that was over-simplified in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}b yet preserved in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}c.  In this case, not only has the line-fitting process removed doorway inlet geometry, but this geometry has been replaced by corners that are no longer the correct angle or orientation.  Another important distinction between the two models is shown at location (4), where a portion of a hallway extending to the left was scanned, but not fully traversed.  The output shown in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}b removed all geometry of this hallway, due to the lack of density in wall samples, yet the model shown in Figure~\ref{fig:fp_eigencrust_visigrapp_compare}c preserved some of this hallway, showing that it is less sensitive to wall sample density.  Lastly, position (5) indicates the one of the downsides to aggressive line fitting.  In Figure~\ref{fig:fp_eigencrust_visigrapp_compare}b, line fitting was used, which caused corners to be clipped and replaced by walls at a 45~degree angle.  Figure~\ref{fig:fp_eigencrust_visigrapp_compare}c shows this same geometry, but its simplification method is less aggressive, resulting in more faithful geometry.

% other features of visigrapp
In addition to the comparisons made above, there are several advancements of the latter floor plan generation scheme~\cite{Turner14}.  In addition to generating watertight geometry of the scanned environment, it also partitions the geometry into separate rooms.  This method also preserves height information of the walls of the floor plan, allowing for the 2D geometry to be extruded into a 2.5D mesh.  Since these methods were not present in the previous method~\cite{Turner12}, no comparisons can be made for these features.

% compare pf and pc floorplans
As mentioned in Section~\ref{sec:wall_sampling}, floor plan geometry is generated using an input of wall sample positions.  These wall samples can be generated either from a 3D point cloud or from the 2D grid-map generated by the particle filter of the localization procedure~\cite{NickJournal}.  While either of these methods can produce wall samples, each has their benefits and detriments.

% analysis of pf pc comparison
Figure~\ref{fig:pf_pc_compare} shows a comparison between floor plans generated using these two methods of computing wall samples.  In Figure~\ref{fig:pf_pc_compare}a, we show example wall samples generated from a 3D point cloud generated from scans of the environment, with the corresponding floor plan shown in Figure~\ref{fig:pf_pc_compare}c.  In Figure~\ref{fig:pf_pc_compare}b, we show wall samples generated from the 2D particle filter grid-map of the same environment, with the corresponding floor plan shown in Figure~\ref{fig:pf_pc_compare}d.  Note that the 2D floor plan generated using the particle filter grid map is much cleaner, with sharper corners and fewer topological errors.  The wall samples generated from the point cloud data are likely to miss some wall features if they are obstructed by non-planar objects, such as furniture or shelves occluding the wall area.  However, the particle filter grid-map version of the wall samples show in Figure~\ref{fig:pf_pc_compare}b do not contain any height information, since they are strictly 2D data.  As such, they are unable to accurately produce an extruded 2.5D model of the environment.  In Chapter~\ref{ch:better_floorplans}, we discuss methods to have the best of both worlds:  floor plans that have accurate and clean features while still retaining full 3D data.

% summary
We demonstrate efficient approaches to automatically generate floor plans of building interiors at real-time speeds.  Classifying and labeling the rooms within each generated floor plan allows for simplification schemes that can preserve fine details at doorways.  These room labels allow for accurate 2.5D extrusion from noisy floor and ceiling height estimates of the input points.  The resulting model is suitable for visualization, simulation, and navigation applications.  Current limitations of this algorithm include the verticality assumption made about observed building features.  If the horizontal cross-section of an environment changes dramatically between different heights, the modeling techniques presented in this paper does not accurately portray the actual geometry.

% visigrapp results
\FloatBarrier
\subsection{Room Labeling Results}
\label{ssec:visigrapp_results}

The approach discussed in Section~\ref{sec:visigrapp} works well on a variety of test cases, spanning several model types including offices, hotels, hospitals, and university buildings.  For the largest models, total processing time to compute an extruded 3D model from 2D wall samples is under 10 seconds.  Most of this time is spent on carving interior triangles, which can be performed real-time in a streaming manner during data acquisition, which typically lasts several minutes.

First, we show sample models resulting from our method in five different environments.  For all the models shown in Figures~\ref{fig:visigrapp_results_a} through~\ref{fig:visigrapp_results_e}, the scale is in units of meters, and the resolution is $5$~cm.  Figure~\ref{fig:visigrapp_results_a} corresponds to an office building, including cubicles and individual offices.  The largest room in this model, shown in teal, primarily contains cubicles.  The cubicle walls do not meet our height threshold of $H=2$ meters, so they are not captured by the wall samples.  Since cubicles are not an architectural feature of the environment, this effect is desirable.  The room shown in purple in the lower-left corner of this model also shows an example error in the building reconstruction.  The adjacent room to the right was briefly seen through a window, but its area was considered part of this purple room rather than being removed in the manner described in Section~\ref{ssec:room_label}, resulting in a small extrusion remaining in the model.

Figure~\ref{fig:visigrapp_results_d} shows a small test model of an apartment office complex and Figure~\ref{fig:visigrapp_results_b} denotes a hotel lobby, hallways, and side rooms.  The vast majority of this model is labeled as one room, consisting of the hallways of the building.  Since no part of these hallways are separated by doors, this result is desirable.  Figure~\ref{fig:visigrapp_results_c} represents an academic research lab, including conference rooms and student cubicles.  The upper portion of the center room, shown in blue, is a kitchenette area, with a counter-top.  Since the counter was not sufficiently captured by the wall samples, it is not represented in the 2.5D extrusion of the model. Figure~\ref{fig:visigrapp_results_e} shows the hallways of an academic building.

% lbnl results
\begin{figure}
	\centering
	
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/dq/lbnl}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/2d/snapshot_lbnl00}}
	\centerline{(b)}
	\end{minipage}
	%\hfill
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/3d/snapshot_lbnl00_cropped}}
	\centerline{(c)}
	\end{minipage}

	\caption[Generated floor plan of an office building.]{Office building: (a) Input represented by 12,823 wall samples; (b) generates floor plan with 19 rooms; (c) the extruded 2.5D mesh of the environment. Extruded 3D mesh represented with 6,084 triangles.  Total processing time required is 7.5 seconds.}
	\label{fig:visigrapp_results_a}
\end{figure}
%(a) This building is represented by 12,823 wall samples, which are used to generated a 3D extruded mesh of 6,084 triangles with 19 rooms.  Total run-time for this model was 7.5 seconds.  

% oceanview office results
\begin{figure}[t]
	\centering
	
	\begin{minipage}[b]{0.50\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/dq/oceanview_office_pf_dq}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/2d/oceanview_office_pf_floorplan}}
	\centerline{(b)}
	\end{minipage}
	%\hfill
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/3d/snapshot_oceanview_office00}}
	\centerline{(c)}
	\end{minipage}

	\caption[Floor plan of an apartment complex office.]{Apartment complex office: (a) Input represented by 3,462 wall samples; (b) generates floor plan with 5 rooms; (c) extruded 3D mesh represented with 512 triangles.  Total processing time required is 1.2 seconds.}
	\label{fig:visigrapp_results_d}
\end{figure}
%(d) This input has 2,957 wall samples that generated a 3D model of 844 triangles with five labeled rooms, generated in 1.2 seconds.

% houston results
\begin{figure}
	\centering
	
	\begin{minipage}[b]{0.4950\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/dq/houston}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.4950\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/2d/snapshot_houston00}}
	\centerline{(b)}
	\end{minipage}
	%\hfill
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/3d/snapshot_houston00}}
	\centerline{(c)}
	\end{minipage}

	\caption[Floor plan of a hotel lobby and hallways.]{Hotel lobby and hallways: (a) Input represented by 33,582 wall samples; (b) generates floor plan with 5 rooms; (c) extruded 3D mesh represented with 5,012 triangles.  Total processing time required is 8.5 seconds.}
	\label{fig:visigrapp_results_b}
\end{figure}
%(b) The input for this model has 33,582 wall samples, which produced a 3D mesh with 5,012 triangles composing 5 rooms in 8.5 seconds.  
%This model is also the largest example output, covering over 260 meters of hallways. 

% trust results
\begin{figure}[t]
	\centering
	
	\begin{minipage}[b]{0.495\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/dq/trust}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.495\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/2d/snapshot_trust00}}
	\centerline{(b)}
	\end{minipage}
	%\hfill
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/3d/snapshot_trust00_cropped}}
	\centerline{(c)}
	\end{minipage}

	\caption[Floor plan of a university office area.]{University office area: (a) Input represented by 12,183 wall samples; (b) generates floor plan with 4 rooms; (c) extruded 3D mesh represented with 4,912 triangles.  Total processing time required is 7 seconds.}
	\label{fig:visigrapp_results_c}
\end{figure}
%(c) The input data for this area have 12,183 wall samples, which generated a 3D model with 4,912 triangles and 4 labeled rooms in 7 seconds.  

% cory 307 results
\begin{figure}[t]
	\centering
	
	\begin{minipage}[b]{0.4\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/dq/cory307}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.4\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/2d/snapshot_cory30700}}
	\centerline{(b)}
	\end{minipage}
	%\hfill
	\begin{minipage}[b]{0.75\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{visigrapp/results/models/3d/snapshot_cory30700}}
	\centerline{(c)}
	\end{minipage}

	\caption[Floor plan of a university office area and hallway.]{University office building: (a) Input represented by 12,125 wall samples; (b) generates floor plan with 7 rooms; (c) extruded 3D mesh represented with 3,604 triangles.  Total processing time required is 4.5 seconds.}
	\label{fig:visigrapp_results_e}
\end{figure}
%(e) with 12,125 wall samples.  The computed model contains 3,604 triangles depicting 7 rooms, and was generated in 4.5 seconds.

% show example textured model
Since these models were generated with a system that captures imagery in addition to laser range points, these models can also be texture-mapped with the scenery of the environment~\cite{Cheng13}.  Figure~\ref{fig:visigrapp_texture} depicts the hallways of an academic building with and without texturing.

\begin{figure}[t]
   \centering
   \frame{\includegraphics[width=0.95\linewidth]{visigrapp/results/coryf2_2d_texture_pics/snapshot05}}
   \centerline{(a)}
   \hfill
   \frame{\includegraphics[width=0.95\linewidth]{visigrapp/results/coryf2_2d_texture_pics/snapshot03}}
   \centerline{(b)}
   \caption[Interior view of a texture-mapped extruded floor plan.]{Interior view of 3D extruded reconstructed model: (a) without and (b) with texture-mapping~\cite{Cheng13}.
   \protect
   \label{fig:visigrapp_texture}}
\end{figure}

% section on limitations
\FloatBarrier
\section{Limitations}
\label{sec:fp_limitations}

The method of generating a 3D model by extruding a 2D floor plan requires assumptions about the building geometry, which can lead to limitations in the accuracy of the final model.  This method assumes a 2.5D nature for the building environment, specifically that all surfaces are either perfectly vertical or horizontal.  Additionally, this method assumes a single height per room, which means drop ceilings and other vertical variations are not captured.

This reduction is especially noticeable in the lack of furniture geometry.  As we discuss in Chapter~\ref{ch:better_carving}, the lack of furniture in a 2.5D extruded model can be useful for further processing, but as a final model it presents some challenges.  We can observe this quality in Figure~\ref{fig:cory299_texture}.  This figure shows a classroom environment, represented by a texture-mapped 2.5D extruded floor plan model.  The classroom contains several pieces of furniture as well as a projector screen in the front.  Since the projector screen is a large vertical surface, it was classified as a ``wall'' and included in the model geometry.  This model geometry represents the screen extending fully to the floor, whereas it does not do so in reality.  As such, the texture-mapping of this geometry is incorrect in the lower section, since the texture does not match the reconstructed geometry.  While the texture looks reasonable from the angle viewed in Figure~\ref{fig:cory299_texture}a, the change in angle in Figure~\ref{fig:cory299_texture}b highlights this inaccuracy.  

This model also shows how the lack of furniture in the reconstruction geometry causes the texture of furniture from the imagery to be projected onto the floors and walls, causing distortion.  These models were texture-mapped using the techniques described in~\cite{Cheng14}, which assumes that any geometry observed in the camera imagery is correctly represented in the reconstructed model.  Since this model is missing furniture geometry, the texture gets distorted. 

% cory 299 with texture
\begin{figure}[H]
   \centering
   \begin{minipage}[b]{0.49\linewidth}
   \includegraphics[width=1.0\linewidth]{visigrapp/results/coryf2_2d_texture_pics/snapshot00}
   \centerline{(a)}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\linewidth}
   \includegraphics[width=1.0\linewidth]{visigrapp/results/coryf2_2d_texture_pics/snapshot01}
   \centerline{(b)}
   \end{minipage}

   \caption[Limitations of 2.5D models.]{A texture-mapped extruded floor plan of a classroom environment: (a) the front of the room; (b) the same view from a different angle.  Note that the projection screen is represented as a vertical surface extending from floor to ceiling in the 2.5D extrusion.  Due to this limitation in the geometry representation, the texture on the lower portion of this surface is incorrect.
   \protect
   \label{fig:cory299_texture}}
\end{figure}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Complex 3D Model Generation
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Complex 3D Model Generation}
\label{ch:carving}

% motivation introduction to chapter
The methods described in this chapter compute a fully-detailed 3D meshed surface from the raw scans taken from our ambulatory scanning system discussed in Chapter~\ref{ch:hardware}.  The goal of these approaches is to preserve all detail of the scanned environment.  Such models are important for visualization and analysis purposes, where detail is more important than minimalism of representation.  Such models are also used to aid in the processing of indoor navigation algorithms, which require accurate depth maps~\cite{Liang13}.  Objects within the environment, such as furniture and light fixtures, are preserved in these methods.  As we discuss in Chapter~\ref{ch:better_carving}, these objects can be explicitly detected and modeled separately.

% overview of chapter contents
This chapter describes two separate algorithms we developed to to perform this task.  Both methods generate watertight 3D meshes that utilize adaptive triangle sizes in order to ensure efficiency of representation.  In Section~\ref{sec:fss_stats}, we discuss probabilistic analysis that is performed on the raw data scans, which can be used to aid the volumetric labeling of the model environment.  Next, in Section~\ref{sec:3dv2013}, we detail our first complex carving technique, which represents the environment volumetrically by an advancing voxel boundary.  We refer to this technique as ``voxel carving'', which produces a piece-wise planar, watertight model of the interior environment of the building~\cite{Turner13}.  In Section~\ref{sec:procarve}, we introduce a new method that expands on the voxel carving method by introducing the probabilistic models discussed in Section~\ref{sec:fss_stats}.  This method also performs the volumetric labeling using an octree structure rather than voxels, to allow for adaptive resolutions within the model and to allow efficient parallel processing.  Again, the method results in a watertight mesh of the environment that can be represented via piece-wise planar surfaces.  Lastly, in Section~\ref{sec:carving_results}, we show comparative results of these two methods on real-world data.

% section on probability analysis on input data
\section{Statistical Models of Input Scans}
\label{sec:fss_stats}

% section on fss statistics, including lots of expansion on
%	noisypath
%	sensor noise model
%	time synchronization noise model
%	transformation probability model between system and sensor and point
%	carve mapping

% overview of this section
In this section, we discuss how each input scan point is analyzed in order to generate a volumetric occupancy model of the scanned environment.  Our goal is to convert the input set of laser scan points into a labeling of space where each location $x \in \mathbb{R}^3$ is assigned a likelihood of being {\it interior} or {\it exterior}.  We perform this task in two steps.  The first step is to form a probabilistic model of each scan point's position, coupled with the position of the originating sensor.  We discuss sources of noise and our probability model for each scan point in Section~\ref{ssec:point_noise_sources}.  The second step is to use this probability model to estimate the scan point's vote for the {\it interior} likelihood of each location in space intersected by its scan ray.  We refer to this step as ``carve mapping'', and detail its application in Section~\ref{ssec:carve_map_generation}.  

Once we obtain these estimates for each scan point, it is possible to generate an occupancy model of the entire scanned environment, as discussed in Section~\ref{sec:procarve}.  Note that the method used in Section~\ref{sec:3dv2013} simply takes the maximum likelihood position of each scan point, and does not consider the distribution of the points' positions when performing volumetric carving.

% subsection detailing sources of noise for each scan point
\subsection{Sources of Point Noise}
\label{ssec:point_noise_sources}

% localization noise toy example
\begin{figure}[t]
	\centerline{\includegraphics[width=0.8\linewidth]{procarve/process/carvemap/localization_noise}}
	
	% caption
	\caption[Example of localization noise affecting point accuracy.]{Any error or noise in the localization process greatly affects the accuracy of scan point positions.  A small variance in the orientation of a sensor in space can dramatically change the computed 3D position of the point.}
	\label{fig:localization_noise}
\end{figure}

% creating a probabilistic model
We compute an estimate of the 3D positions for each scan point and the point's originating sensor.  These values are represented as two 3D Gaussian distributions.  For each input scan, the sensor position is represented by Gaussian $N(\vec{\mu}_s,C_s)$ and the scan point position is represented by $N(\vec{\mu}_p,C_p)$.  For tractability, assume scan frame's distribution is independent from the position of other scan frames.

The uncertainty in the position values of each scan point originate from three independent sources of error: the backpack localization estimate, the timestamp synchronization, and intrinsic sensor noise.  Each of these noise sources affects the system's estimate of its position and orientation in different manners, but all must be accounted for in order to generate accurate measurements in world-coordinates for the model.

% origins of uncertainty: localization
\paragraph*{Localization noise}
Localization noise arises from errors in the estimate of the system trajectory, as detailed in~\cite{NickJournal}, and is by far the largest source of error with typical standard deviations on the order of $20$~centimeters.  Our localization procedure is broken into two steps: dead-reckoning via odometry estimates and global optimzation of path~\cite{toro05,fastslam03}.  Dead-reckoning is performed by estimating the incremental transformation between successive poses of the system, integrating measurements of the velocity and acceleration in order to determine a rough position estimate.  Since this method represents open-loop integration, such measurements are susceptible to drift based on any system bias~\cite{Backpack,ProbabilisticRobotics}.  There exist real-time systems that perform some optimzations via secondary measurements of movement, such as laser scanners or visual cameras~\cite{Shelley14}, which reduces the error of the dead-reckoning path.  Such improvements to the dead-reckoning path are important, since the step of global optimization will redistribute any error within a pose uniformly across the traversed path~\cite{toro07}.

Variations in uncertainty between poses are due to the optimized constraint-network of loop closures in the estimated path of the system.  Such optimization systems provide a covariance matrix representing the uncertainties of the values for each pose in the trajectory path.  These uncertainties depend on the timestamp, $t$, since the system moves around the path over time.  These values include both position and orientation of the scanning system at a given time.  The covariance of the system's world position $(x,y,z)$ at a given time $t$ is denoted by the $3 \times 3$ covariance matrix $C_{pose_T}(t)$.  Similarly, the covariance of the system's orientation $(\theta,\phi,\psi)$ at time $t$ is denoted by $3 \times 3$ covariance matrix $C_{pose_R}(t)$.  The uncertainty in the orientation of the sensor can cause major positional uncertainty for range measurements, as shown in Figure~\ref{fig:localization_noise}, and is important to take into account.  While there are some cross-correlations between the position and orientation terms, these are ignored for tractability.  

The next step is to combine these covariances to estimate the uncertainty of the position of a given scan point $p$ taken at time $t$, and the corresponding position of the originating sensor, $s$.  First, the uncertainty in the position of the sensor $s$ due to localization noise is estimated to be:

% how localization noise affects position of sensor
\begin{equation}
C^{(s)}_{pose}(t) = C_{pose_T}(t) + \left[ T_{s\rightarrow c}(t) \right]_x \, C_{pose_R}(t) \, \left[ T_{s\rightarrow c}(t) \right]_x^T
\end{equation}

where $\left[.\right]_x$ denotes the cross-product matrix for a given vector.  This equation uses the small-angle approximation to apply the rotations specified by $C_{pose_R}(t)$.  Since we assume the uncertainty in these orientations is small, this approximation is valid.  The value $T_{s\rightarrow c}(t)$ denotes the translation between system common and the specific sensor $s$, as discussed in Chapter~\ref{sec:localization}.  Since the sensors are rigidly mounted to the system, this distance is independent of time, but the displacement in world coordinates depends on time due to the system changing orientation over the course of the data acquisition.  The final value represents the covariance of the position of sensor $s$, in model coordinates.  Similarly, we can compute the effect of the localization  uncertainty on the position of scan-point $p$:

% how localization noise affects position of scan point
\begin{equation}
C^{(p)}_{pose}(t) = C_{pose_T}(t) + \left[ T_{p\rightarrow c}(t) \right]_x \, C_{pose_R}(t) \, \left[ T_{p\rightarrow c}(t) \right]_x^T
\end{equation}

where $T_{p\rightarrow c}(t)$ denotes the translation between the point $p$ and the system common position at time $t$.  The resulting value specifies the uncertainty in the scan point position, in world coordinates, due to estimated noise from localization.

% timesync errors
\paragraph*{Timestamp synchronization noise}
Timestamp synchronization errors are due to our system combining measurements from several sensors, whose timestamps need to be transformed to a common clock.  This synchronization is done using a linear fitting of the individual sensors' clocks with the common system clock and produces an estimate for the standard deviation of each sensor's synchronization, $\sigma_t$.  Note that $\sigma_t$ is constant for a given sensor across the entire dataset, since it is derived from a linear fit of all sensor frames.  Mis-synchronization of timestamps can result in spatial errors of scan points, especially when the system is moving or rotating rapidly while scanning distant objects.  In these cases, an estimate of the scan point's position changes depending on our estimate of when a scan is taken.  However, since our sensors are synchronized to an accuracy of $\sigma_t \approx 1$ millisecond, synchronization error is usually the lowest source of noise in the scan points, contributing uncertainty to scan point positions of under $1$~centimeter.  We represent the uncertainty in the sensor position estimate and the scan-point position estimate at a given time $t$ caused by the time synchronization process as zero-mean Gaussians with $3 \times 3$ covariance matrices $C^{(s)}_{ts}(t)$ and $C^{(p)}_{ts}(t)$, respectively:

% how time synchronization affects sensor position
\begin{equation}
C^{(s)}_{ts} = \left[ T_{s\rightarrow c}(t) \right]_x \, \sigma_{t} \, \vec{\omega}(t) \, \vec{\omega}^T(t) \, \sigma_{t} \, \left[ T_{s\rightarrow c}(t) \right]_x^T
\end{equation}

% how time synchronization affects scanpoint position
\begin{equation}
C^{(p)}_{ts} = \left[ T_{p\rightarrow c}(t) \right]_x \, \sigma_{t} \, \vec{\omega}(t) \, \vec{\omega}^T(t) \, \sigma_{t} \, \left[ T_{p\rightarrow c}(t) \right]_x^T
\end{equation}

where $\vec{\omega}(t)$ represents the estimated angular velocity of the system at time $t$. Note that this model only considers rotational velocity, and not linear velocity.  This simplification is due to our estimates of linear velocity being more difficult to formulate, since they cannot be directly measured.  Additionally, rotational velocity produces the dominating source of noised caused by timestamp mis-synchronization, since the estimated position of a point scanned far away is more affected by rotation of the sensor than translation of the sensor.

% fss scan statistics
\paragraph*{Sensor hardware noise}
The third source of noise depends on the sensor hardware.  Our system uses Hokuyo UTM-30LX sensors, whose intrinsic noise characterization is given in~\cite{Pomerleau12}.  These sensors are popular among robotics groups due to their low noise characteristics given their range~\cite{GTSAM14}.  Typically this noise contributes on the order of 1 to 2 centimeters to the standard deviation of the positional estimate of scan points.  This uncertainty value increases as the range distance of the point increases, with accuracy dropping dramatically after $10$~meters and accurate measurements stopping entirely at a range of $30$~meters.  

We represent the uncertainty a scan-point position caused by intrinsic sensor noise as a zero-mean Gaussian with covariance $C^{(p)}_{noise}(t)$.  The sensor noise only affects our estimate for the position of the scan point, not the position of the sensor itself.  Additionally, the variance magnitude is dependent on the range distance of each point, so the value changes from point-to-point with respect to time $t$.

For a sensor $s$ on our scanning system scanning a point $p$, we can model the total estimate of the uncertainties of the positions of $s$ and $p$ at a given time $t$ by combining the above terms.  The resulting $3 \times 3$ covariance matrices $C_s(t)$ and $C_p(t)$ are given by:

\begin{equation}
	C_s(t) = C^{(s)}_{pose}(t) + C^{(s)}_{ts}(t)
\end{equation}
\begin{equation}
	C_p(t) = C^{(p)}_{pose}(t) + C^{(p)}_{ts}(t) + C_{noise}
\end{equation}

We now have estimates of the means and covariances of the positions of both the originating sensor and the scan-point itself for each scan-point collected by our system during a dataset collection.  In the next section, we discuss how these values are used to develop a volumetric model for each individual scan-point, allowing it to be used to label volumes in the scanned environment as either {\it interior} or {\it exterior}.

% subsection on converting from scan points to carve maps
\subsection{Carve Map Generation}
\label{ssec:carve_map_generation}

% carve mapping toy example
\begin{figure}[t]
	\centering
	\begin{minipage}[t]{0.50\linewidth} % point distributions
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/carvemap/point_dists}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.50\linewidth} % carve map
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/carvemap/carve_map}}
		\centerline{(b)}
	\end{minipage}

	% caption
	\caption[Example carve mapping.]{Example carve mapping: (a) the spatial distribution of sensor location and scan-point positions; (b) the computed carve mapping, indicating the areas estimated to be interior and exterior based on this scan.}
	\label{fig:carvemap}
\end{figure}

% carve map generation
We now have estimates for the positions of both the sensor and each scan-point taken at time $t$, with covariances $C_s(t)$ and $C_p(t)$ respectively.  We model the positions of the sensor and scan-point to be represented by Gaussians $N(\vec{\mu}_s,C_s(t))$ and $N(\vec{\mu}_p,C_p(t))$, respectively. An example of this model is shown in Figure~\ref{fig:carvemap}a.

Next, we use these distribution models for each scan point to form a ``carve mapping'' $p(x) : \mathbb{R}^3 \mapsto [0,1]$, which describes the likelihood of any location $x \in \mathbb{R}^3$ of being {\it interior} or {\it exterior} based on the position estimates from a scan point.  For example, if $p(x) \leq 0.5$, then $x$ is more likely in an {\it exterior} location and if $p(x) > 0.5$, then $x$ is more likely to be {\it interior}.  

Since each individual scan ray is represented by a probabilistic model, we want to first determine how each ray would label each position $x$ in the environment, and then merge the estimates of that position from all available scan rays in order to determine the maximum likelihood estimate of $p(x)$.  In order to determine how best to label $p(x)$ with a single scan ray, note that we can infer environment geometry based on the orientation of each ray.  A time-of-flight scan indicates that there exists a light-of-sight path between the sensor and the scan-point, which indicates volume near this scan ray should be marked {\it interior}.  Additionally, the fact that a scan-point stopped at a particular distance indicates that a solid object exists in the environment that occluded the ray.  Positions past the scan point should be marked {\it exterior}.  However, positions that are far away from the scan ray should be less influenced by the presence of the ray, since they remain unobserved.  

For a single scan ray, we encompass these qualities in our model by defining $p(x)$ as:

\begin{equation}
	p(x) = F_s(x_{\parallel}) \, f_{\perp}(x_{\perp}) \, (1 - F_p(x_{\parallel})) + 0.5 (1 - f_{\perp}(x_{\perp}))
	\label{eq:carvemap}
\end{equation}

where $F_s(.)$, $F_p(.)$, and $f_{\perp}(.)$ represent one-dimensional marginal distributions derived from the scan ray model, as described below.  We split a location $x = x_{\parallel} + x_{\perp}$, as shown Figure~\ref{fig:carvemap}b, where $x_{\parallel}$ is the distance along the length of the scan ray, and $x_{\perp}$ is the distance orthogonal to the scan ray.  This decomposition allows us to weight the influence of the scan ray on our estimate $p(x)$ so represent a fall-off as $x_{\perp}$ increases.

$F_s(x_{\parallel})$ is the marginal one-dimensional (1D) Cumulative Distribution Function (CDF) of the sensor position's distribution along the length of the scan ray, derived from the Gaussian distribution of the sensor position.  Note that the marginal distribution of a multivariate Gaussian is also Gaussian.  Similarly, $F_p(x_{\parallel})$ is the marginal CDF of the scan-point position's distribution along the length of the ray. 

Lastly, $f_{\perp}(x_{\perp})$ is the 1D Probability Mass Function (PMF) of the lateral position of the scan ray.  This function is derived as the marginal distribution along the direction $x_{\perp}$ of the Gaussian defined by $N(x_{\parallel}, C^*)$, where $C^*$ is defined as the weighted average between the covariances of the sensor and scan-point positions:  $C^* = ( 1-f ) \,  C_s(t) \; + \; f \, C_p(t)$ and $f = \| \vec{x}_{\parallel} \| \; / \; \| \vec{\mu}_p - \vec{\mu}_s \|$, or the fractional distance of $x_{\parallel}$ along the scan ray.

The combination of these values in Equation~\ref{eq:carvemap} yields the mapping shown in Figure~\ref{fig:carvemap}b.  Values in blue are less than $0.5$, indicating a likelihood of a location being {\it exterior}.  As shown, these values occur just past the scan position.  Values in red are greater than $0.5$, indicating a likelihood of a location being {\it interior}.  These values occur around the sensor position and along the scan ray.  As the query location $x$ moves away from the scan ray and $x_{\perp}$ increases, then $p(x)$ approaches $0.5$, indicating unknown state.  In other words, locations close to the scanned ray are likely to be {\it interior}, locations past the scan point are likely to be {\it exterior}, and locations far away from the scanned ray are unknown.

In addition to the value $p(x)$, which serves as an estimate for the probability position $x$ is {\it interior}, we also compute a weighting on this estimate, $w(x) = f_{\perp}(x_{\perp})$.  This weighting approaches zero as $x$ moves farther away from the scan ray, ensuring that a scan ray is only used to estimate the {\it interior} probability for locations close to it.  We use this weighting when consolidating estimates of the value $p(x)$ from many different scans, as we discuss in Section~\ref{sec:procarve}.

The above mapping allows us to use each scan ray to ``carve'' the volume of the model by assigning a probability to all scanned areas indicating the likelihood of areas being interior or exterior.  In Section~\ref{sec:procarve_wedge_and_chunk}, we discuss how the estimates from all input scans are used to generate a model of the entire environment.  

% section on voxel carving
\section{Voxel Carving Surface Reconstruction}
\label{sec:3dv2013}

% section introduction
This section describes our voxel carving method for surface reconstruction of building interiors.  First, a point-cloud is generated of a scanned environment, as shown in Figure~\ref{fig:3dv2013_mydesk}a.  The point-cloud is used to determine the locations in the volume that are {\it interior} and {\it exterior} via a voxel carving scheme.  We introduce a novel data structure that allows the carving to be computed in a memory efficient and scalable manner.  Second, once interior and exterior voxels are labeled, the surface defined between these two labelings is segmented into planar regions, as shown in Figure~\ref{fig:3dv2013_mydesk}b.  Each region is meshed with triangles that are proportional to its size, as shown in Figure~\ref{fig:3dv2013_mydesk}c.  The result accurately and efficiently depicts the geometry of the building.

% example output of this technique
\begin{figure}[t]

	% show pointcloud of building
	\begin{minipage}[b]{0.3\linewidth}
	\centerline{\includegraphics[height=2.7cm]{3dv2013/results/coryInside_no_roof_pointcloud_cropped}}
	\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.3\linewidth}
	\centerline{\includegraphics[height=2.7cm]{3dv2013/results/coryInside_no_roof_surface00_cropped}}
	\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.3\linewidth}
	\centerline{\includegraphics[height=2.7cm]{3dv2013/results/coryInside_no_roof_triangles00_cropped}}
	\centerline{(c)}\medskip
	\end{minipage}

\caption[Example processing of office building interior.]{Example processing of office building interior.  Ceiling has been removed for visualization:  (a) Input point-cloud; (b) Output surface; (c) Triangulation of surface.}
\label{fig:3dv2013_mydesk}
\end{figure}

% labeling voxels as interior/exterior
\subsection{Voxel Labeling}
\label{ssec:voxel_labeling}

This interior/exterior volume classification is performed on a voxel grid~\cite{Turner13}.  Given an input resolution size $r$, each voxel is a cube whose sides are length $r$.  Initially, all voxels are assumed to be {\it exterior}, referring to any space outside of the scanned volume or space that is represented by solid objects.  The process of {\it carving} refers to relabeling a voxel from exterior to interior, which occurs when a voxel is found to intersect the line segment from the scanner to a corresponding scan point.  If a laser passes through a voxel, that voxel is considered interior space.

For each laser scanner, there exists a track in space that represents the scanner's movement during data collection.  This track is represented by a sequence of scanner positions $S = (\vec{s}_{1},\,\vec{s}_{2},\,\vec{s}_{3},\,...,\,\vec{s}_{N})$, where $N$ is the number of locations sampled during data collection for the given sensor.  These track samples are shown as purple circles in Figure~\ref{fig:voxel_carving}a.

At the $i^{\text{th}}$ timestep, each scanner sweeps an arc defined by the set of points $P_{i} = \{\vec{p}_{i,1},\,\vec{p}_{i,2},\,...,\,\vec{p}_{i,j},\,...,\,\vec{p}_{i,M}\}$, where $M$ is the number of samples along the arc.  Each scan-line is shown in solid red in Figure~\ref{fig:voxel_carving}a.  A scan-line for a given point goes from the sensor position $s_i$ to the scan point position $p_{i,j}$.  These scan-lines can be interpolated in two dimensions, indexed by $i$ and $j$.  The first interpolates the laser scans temporally, while the second interpolates the scans spatially along the scan arc.  These interpolations are shown as dashed lines in Figure~\ref{fig:voxel_carving}a.  By performing bilinear interpolation, a continuous surface of scans can be estimated from each scan point $\vec{p}_{i,j}$, shown as the interior of the quadrilateral ($\vec{p}_{i,j}$, $\vec{p}_{i,j+1}$, $\vec{p}_{i+1,j+1}$, $\vec{p}_{i+1,j}$).  To efficiently determine which voxels are intersected by this interpolation, the carving operations are performed using ray-tracing between interpolated scanner position $\vec{s}$ and interpolated scan point position $\vec{f}$.  Each pair $(\vec{s}, \vec{f})$ denotes a line segment to carve.  An example set of these segments is shown in green in Figure~\ref{fig:voxel_carving}b.  By spacing these segments no more than distance $r$ apart, each voxel within the interpolated volume is assured to be carved.  We perform ray-tracing on each segment and relabel every intersected voxel as {\it interior}.  This step produces a set of voxels as shown in Figure~\ref{fig:voxel_carving}c.

Note that the method described above performs the equivalent of a polyhedron/cube intersection by interpolating the polyhedron volume by a series of line segments.  Such a computation is efficient since it allows arbitrary orientations of the original scan positions.  As we discuss further in Section~\ref{sec:procarve}, this technique can be improved further by performing a series of triangle/cube intersections, thus reducing the total number of intersection tests when the successive scan points are spaced far apart, requiring significant interpolation.

% make figure of prism carving and interpolation
\begin{figure}

  \begin{minipage}[b]{0.3\linewidth}
  \centerline{\includegraphics[height=2.7cm]{3dv2013/diagrams/triangle_prism_carve_a}}
  \centerline{(a)}\medskip
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
  \centerline{\includegraphics[height=2.7cm]{3dv2013/diagrams/triangle_prism_carve_b}}
  \centerline{(b)}\medskip
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
  \centerline{\includegraphics[height=2.7cm]{3dv2013/diagrams/triangle_prism_carve_c}}
  \centerline{(c)}\medskip
  \end{minipage}

\caption[Method of interpolating scans to carve voxels.]{(a) The input point-cloud is used in conjunction with the track of each scanner to define interior space to carve; (b) Carving is performed using ray-tracing from scanner location to an interpolation of the input points; (c) The result is a set of voxels labeled as {\it interior}.}
\label{fig:voxel_carving}
\end{figure}

% discuss dgrid data structure
\subsection{Voxel Data Structure}
\label{ssec:voxel_carving_data_structure}

In most common voxel representations, each voxel in 3D space is explicitly stored in an array in memory.  Even though this approach is straight-forward and easy to use, its memory usage is proportional to the volume represented.  For sizable models, this memory footprint rapidly becomes intractable, necessitating splitting models into smaller chunks and processing each separately~\cite{Carving}.  This step adds redundant computation and storage overhead.  Adaptive approaches such as octrees reduce memory consumption by only representing the subset of relevant volume, but they still explicitly represent volume, an approach that rapidly fills memory~\cite{OctreeSculpting,Yang05}.

Rather than storing all relevant voxels in memory, in this paper we propose a data structure that implicitly represents the interior and exterior voxels by only explicitly storing the boundary voxels.  A boundary voxel is defined to be one that is labeled as exterior, but has at least one face incident to a voxel labeled interior.  The number of boundary voxels is proportional to the surface area of a model, so storing the boundary only requires $O(n^2)$ memory, whereas the full volume would require $O(n^3)$ memory to store, where $n$ is the characteristic length of a model.

The data structure used during carving is a map between boundary voxel locations $v \in \mathbb{Z}^3$ and six boolean flags $(f_1,f_2,...,f_6) \in \{\texttt{false},\texttt{true}\}^6$, with the following invariants.  Each of these flags represents one of the six faces of the referenced voxel.  Marking $f_i = \texttt{true}$ indicates that the neighboring voxel of $v$ that shares face $f_i$ must be interior.  If $f_i = \texttt{false}$, then this neighboring voxel is exterior, which may mean it is also a boundary voxel.

Figure~\ref{fig:dgrid_boundary_carving} demonstrates in 2D how a voxel representation of the full model can be built from a starting configuration using ray-tracing as a primitive operation, while still respecting the above invariants.  The starting configuration for the 2D map is shown in Figure~\ref{fig:dgrid_boundary_carving}a, with a single interior voxel represented using four boundary voxels.  This interior voxel is initialized to be at the scanner's start position, which is known to be interior.  Dark green lines indicate faces marked as $\texttt{true}$.  Recall that interior voxels denoted in white are not explicitly stored in the map while the boundary voxels, denoted in light green, are stored explicitly.  During the carving process, if a voxel $v$ is designated to be carved then any of its faces that are flagged as $\texttt{false}$ must be incident to exterior voxels, as shown in Figure~\ref{fig:dgrid_boundary_carving}b.  Each of these neighboring exterior voxels, $v^{\prime}$, is added to the map, as they are now boundary voxels, and the face of $v^{\prime}$ that is incident to $v$ is flagged as $\texttt{true}$.  Lastly, $v$ is removed from the map, which now represents that $v$ is part of the interior volume, as seen in Figure~\ref{fig:dgrid_boundary_carving}c.  Any carving attempt on a voxel that is not in the map can be ignored, since all carving initiates from within the interior volume.  By using only this operation, the map invariants are preserved and will always consistently define an interior volume.

This data structure uses less memory than the corresponding adaptive octree structure discussed later in Section~\ref{sec:procarve}, since it only explicitly stores elements at the boundary between interior and exterior volumes.  There are a number of downsides, however, such as the inability to query if a given point in space is interior or exterior, and the uniformity of the voxels.  As we discuss further in Section~\ref{sec:procarve}, an octree structure may have some memory overhead, but allows for further computation than what is accomplished here.

% show boundary carving
\begin{figure}[t]

  \centerline{\includegraphics[width=0.9\linewidth]{3dv2013/diagrams/boundary_voxel_carving}}
  \centerline{(a)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(b)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(c)\,\,\,\,\,}
\caption[A 2D example of carving a voxel.]{A 2D example of carving a voxel.  Stored boundary voxels are shown in green.  White voxels are not explicitly stored. (a) The initial map configuration; (b) Voxel $v$ is carved by removing $v$ from the map and adding additional boundary voxels $v^{\prime}$ to the map; (c) $v$ is represented as interior volume.}
\label{fig:dgrid_boundary_carving}
\end{figure}

\subsection{Preserving Fine Detail}
\label{ssec:voxel_carving_preserving_detail}

This carving process may not preserve features for point-clouds with low noise, but high detail.  Specifically, objects whose feature length is on the order of one voxel size may be carved away.  This issue can be a serious problem if two rooms are separated by a thin wall.  Scanning both of these rooms may carve away this wall, resulting in a final model that shows only one, double-sized room.  In order to preserve these features, we store a second voxel set that specifies the voxels that are intersected by the input point-cloud.  While the original point-cloud is often much too large to be stored in memory at once, this discretization is much smaller and is on the same order of memory usage as the map of boundary voxels.

During the carving of each line segment, if ray-tracing encounters a voxel that is marked to contain input points, then the carving of that segment is truncated just before this voxel.  No features that are represented in the point-cloud are ever carved away.  Since ray-tracing already occurs on a voxelized grid, this occlusion check does not add any appreciable complexity to the computation.

This process does not use any of the probabilistic models discussed in Section~\ref{sec:fss_stats}, but rather assumes that a voxel intersected by a scan point is {\it exterior}, any voxels intersected by a scan line are {\it interior}, and all other voxels are unaffected.  As such, the presence of noisy points may disrupt the quality of the output surface.  Such concerns are mitigated in the procedure we describe in Section~\ref{sec:procarve}, where we introduce a carving mechanism that incorporates the probabilistic models discussed in Section~\ref{sec:fss_stats}.

% voxel carving surface reconstruction
\subsection{Planar Region Fitting}
\label{ssec:voxel_region_fitting}

% show figure of original, flood-filled, and colored voxel faces
\begin{figure}[t]

	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/stairs_voxels/bhh_stairs_voxels_zoom01}}
	\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/stairs_voxels/bhh_stairs_regions00}}
	\centerline{(b)}\medskip
	\end{minipage}
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/stairs_voxels/bhh_stairs_regions02}}
	\centerline{(c)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/stairs_voxels/bhh_stairs_regions03}}
	\centerline{(d)}\medskip
	\end{minipage}

	\caption[Method of region fitting to voxel carving.]{(a) Example carved voxels at the top of a flight of stairs; (b) Regions colored based on voxel face flood-fill; (c) Region growing by finding best-fit planes to voxel faces; (d) Regions relaxed to merge planes that are nearly parallel}
	\label{fig:stairs_voxels}
\end{figure}

% highlight overall approach, and a few technical challenges
Our procedure for surface reconstruction of voxels can be broken into two parts.  First, estimates of planar regions are found around the boundary faces of these voxels.  These regions are formed from connected sets of voxel faces, all of which are positioned on best-fit planes.  Second, each region is triangulated, forming a mesh.  This triangulation lies along the best-fit plane for each region, with elements whose sizes are proportional to the size of the region.

%\paragraph*{Region growing on voxel faces}

The first task is to determine the connectivity along the carved voxel faces. An example of such a carving for a flight of stairs is shown in Figure~\ref{fig:stairs_voxels}a.  Since these faces are squares that form a watertight surface and lie on an axis-aligned grid, each face has exactly four neighbors.  If a voxel face and its neighbor are both oriented in the same direction, e.g.\ both have normal vectors in the Z+ direction, then one can immediately perform a flood-fill operation in order to group these faces into planar regions.  The faces belonging to each region lie exactly on a plane.  The results of this flood-fill operation is shown in Figure~\ref{fig:stairs_voxels}b.

Since the voxels are a discretized representation of the volume, any flat surface of the environment that is not axis-aligned is represented as a zig-zag pattern of voxels.  By fitting planes that only approximate the voxel faces, the output model can contain surfaces that are not axis-aligned.  The approximating planes are found by performing Principle Component Analysis (PCA) on connected subsets of voxel faces~\cite{PCA}.  For any connected set of voxel faces $V$, PCA is performed on the four corners of all the faces to estimate a best-fit plane.  If $V$ is well-modeled by this plane, then the elements of $V$ are grouped together as one planar region.  $V$ is considered well-modeled if the maximum distance of $V$ from the plane is at most $r$.  This threshold guarantees that any voxels intersected by the modeling plane are incident to the faces in $V$.

Starting with the regions found in the flood-fill operation above, adjacent regions of voxel faces are progressively merged by attempting to model their union with a single best-fit plane.  If this plane meets the threshold described above, then the two adjacent regions are replaced by one region representing their union.  This step is referred to as {\it region growing}.  Even though this stage reduces the total number of regions, it typically results in an over-fitting of too many regions.  An example of this stage is shown in Figure~\ref{fig:stairs_voxels}c.

In order to yield a more aesthetically pleasing output, we further relax these region definitions.  If two adjacent regions are fit by planes whose normal vectors are within $15^{\circ}$, then they are replaced by a single region defined by their union.  The result of this processing yields plane definitions that closely resemble an intuitive labeling of the floors, walls, and ceilings.  This final region labeling is shown in Figure~\ref{fig:stairs_voxels}d. 

% meshing of planar regions
\subsection{Triangulation of Regions}
\label{ssec:voxel_triangulation}

Once the set of voxel faces has been partitioned into planar regions, it is necessary to triangulate these regions.  Since the output mesh represents the planar regions found in the previous section, an optimum approach would adapt the size of triangles based on the size of these planar regions.

Taking advantage of the existing voxel grid helps ensure that each region is represented with good quality triangles.  This grid allows for regions to be triangulated with a 2D variant of Isosurface Stuffing techniques, which provide strict bounds on resulting triangle angles~\cite{Isostuffing}.  An example region of voxel faces is shown in Figure~\ref{fig:triangulation}a.  Since this region is best-fit by a plane that is not axis aligned, the region is composed of voxel faces in a zig-zag pattern.  The voxel faces that are most aligned with the normal vector of the region's plane, shown in red, are considered the dominant faces of the region.  These dominant faces are projected along their corresponding axis to generate an axis-aligned 2D projection of the region.  This projection is shown with black dashed lines in Figure~\ref{fig:triangulation}a.  The triangulation is found by populating a quadtree that is aligned to the projected grid with the faces of this region.  An example of this quadtree is shown in Figure~\ref{fig:triangulation}b.  The tree is triangulated by placing vertices at the center and corners of the leaf nodes, as shown in Figure~\ref{fig:triangulation}c.  This step results in larger triangles for larger leaf nodes, while still controlling the quality of the output triangles.  This triangulation is projected back onto the plane defined by the region, to result in triangulated representation of this region in 3D space.

% show cartoon of triangulation of region, including quadtree
\begin{figure}[t]

	\begin{minipage}[b]{0.48\linewidth}
		\centerline{\includegraphics[height=5.0cm]{3dv2013/project_plane/dominant_axis_project_a}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.48\linewidth}
		\centerline{\includegraphics[height=5.0cm]{3dv2013/project_plane/2dtri2}}
		\centerline{(b)}\medskip
	\end{minipage}
	
	\begin{minipage}[b]{0.48\linewidth}
		\centerline{\includegraphics[height=5.0cm]{3dv2013/project_plane/2dtri3}}
		\centerline{(c)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.48\linewidth}
		\centerline{\includegraphics[height=5.0cm]{3dv2013/snapshot_triangles_corner00}}
		\centerline{(d)}\medskip
	\end{minipage}

	\caption[Triangulation of planar regions in voxel carving.]{(a) The dominant faces of a planar region (shown in red) are projected to the dominant axis-aligned plane; (b) Projected faces represented in a quadtree structure to reduce number of elements; (c) This quadtree can be triangulated efficiently while ensuring high-quality triangles; (d) An example output of the triangulation of three regions in the corner of a room.}
	\label{fig:triangulation}

\end{figure}

Since the connectivity between voxel faces is well-defined, the connectivity of the output triangulation is also well-defined.  To ensure that the borders between planar regions are represented sharply, the vertices that are shared by multiple regions are snapped onto the intersection of those regions.  This fits the intersection between two regions to a line, and the intersection of three or more regions to a point in space.  This step yields a watertight mesh across regions, as can be seen in the intersection of three regions at the corner of a room in Figure~\ref{fig:triangulation}d.  To limit self-intersections in the final surface, the vertices that are shared by multiple regions are allowed to be displaced up to a distance threshold from their original position in the voxel grid.  This threshold is relaxed as the angle between the regions in question approaches $90^{\circ}$.  The corners between walls and ceilings remain sharp, while the transition between regions that are close to parallel is smooth.  If such a threshold did not exist for the boundaries between near-parallel regions, their shared vertices could produce undesirable artifacts.

% section on octree procarve
\section{Probabilistic Octree Surface Reconstruction}
\label{sec:procarve}

% overview of section
In Section~\ref{sec:fss_stats}, we discuss how a probability model can be computed for each scan point, allowing that point to perform a volumetric labeling of nearby space.  In Section~\ref{sec:3dv2013}, we discuss how a voxel carving method can be used to partition the scan environment into {\it interior} and {\it exterior} domains by performing a set of cube/line-segment intersection tests.  In this section, we combine these two ideas in order to improve upon the voxel carving method described in the previous section.

Rather than simply labeling any voxel that is intersected by a scan ray as {\it interior}, we consolidate all carve mapping estimate of nearby scan rays, ensuring that our estimate of a particular location's {\it interior} probability is as consistent with the entire scan dataset as possible.  In Section~\ref{ssec:procarve_wedge_and_chunk}, we discuss how we consolidate and interpolate each individual scan's probability estimates to develop a unified estimate for a given location across all scans.  In Section~\ref{ssec:procarve_octree}, we describe the octree data structure that is used to house all consolidated estimates for the entire scan environment.  Then, in Section~\ref{ssec:procarve_boundary}, we detail how the volumetric octree structure is converted to a mesh representation of the boundary of the {\it interior} volume.  We describe two different meshing techniques that can be applied to the octree structure and compare the qualities of each.

% discussing wedge generation and chunking
\subsection{Wedge Generation}
\label{ssec:procarve_wedge_and_chunk}

% wedge generation
In Section~\ref{ssec:voxel_labeling}, we discussed how successive scan rays are interpolated in a bilinear fashion:  both between adjacent scan points within a single frame and between the same point index in adjacent frames.  These interpolations represent a ``wedge'' volume between four adjacent scan rays, as shown in Figure~\ref{fig:voxel_carving}.  The goal of this interpolation is to determine which voxel elements intersect a particular scan.  One major change in this section is that we do not simply label any intersected cube as {\it interior}, but rather consolidate estimates of the probability of an element being {\it interior} or {\it exterior} from all nearby scans.  Thus each ``wedge'' is represented to extend to a volume beyond the original four scan rays.  We extend each ray of the wedge by an additional three standard deviations in length, taken from the Gaussian model of the scan point position distribution.  In this way, we ensure that all volume relevant to the scan rays of this wedge is covered.  We precompute all wedges of the dataset so that they can each contribute to the volumetric labeling of the entire scan environment.

% chunking
The scan wedges are originally sorted temporally as the operator moves around the environment.  In order to process the scanned volume in a parallel fashion, we first organize the input scans into spatial chunks.  The environment volume is tessellated into large cubes, and each cube contains a list of all the scan wedges that intersect it.  When we populate the octree structure, each chunk can be processed independently and concurrently via a thread pool.  The scan wedges are inserted into each chunk they intersect, refining the nodes under the chunk node to a fine resolution.  We typically use chunks of size $2$~meters on a side, aligned to our octree structure as non-leaf nodes of the environment.  This size allows each individual chunk to easily fit into memory while still being large enough to efficiently subdivide the entire environment.

% subsection discussing details of octree data-structure
\subsection{Octree Data Structure}
\label{ssec:procarve_octree}

% overview and motivation
As discussed above, the original scans of a dataset generate probability estimates for each spatial location in the scan environment, determining whether a unit of volume is labeled as {\it interior} or {\it exterior}.  In order to perform this labeling, we require a data-structure that allows for efficient representation of spatial information, including fast look-ups for geometric intersection tests.  For these purposes, we use an octree data structure to store {\it interior} probability estimates for each location in space.  The octree is represented to a finite node depth.  As we discuss below, the finest resolution of the octree is typically on the order of $1$~centimeter.  A diagram of how scan wedges are sorted spatially into chunks and inserted into the octree structure is shown in Figure~\ref{fig:octree_population}.

% figure showing octree and chunks
\begin{figure}[t]

	\centerline{\includegraphics[width=0.7\linewidth]{procarve/octree/octree_fig}}

	\caption[Diagram of octree population.]{As each system sensor traverses the environment, scan rays from that sensor are collected and sorted spatially into chunks.  All scans from each chunk are inserted into the octree structure in a parallel fashion.  The leaf nodes of the octree represent the probability of that volume element being {\it interior} or {\it exterior}.}
	\label{fig:octree_population}
\end{figure}

% some background
Octrees have been utilized successfully in the past to represent volumetric scan data~\cite{Hernandez07,Octomap}.  They provide an improvement over the voxel grid structure mentioned in the previous section, since they allow for adaptive resolutions to be used in different locations in the model and provide fast intersection tests against scan geometries.  We use the octree structure to store a probabilistic estimate at every leaf node that indicates the likelihood of that volume element being {\it interior} or {\it exterior}.  The following process inserts all the scans into an octree in a chunk-wise fashion, so that it can be performed in a parallel- and memory-efficient manner.

% data storage in octree
We use the octree structure to determine the locations in the environment volume intersected by at least one scan wedge.  For each leaf nodes $L$ that contains at least one wedge, we merge all the intersecting scan wedges to obtain a fused probabilistic estimate $p_L$.  If any wedge intersects $L$, then all four scan rays from that wedge are used to contribute to the value $p_L$.  Let $S$ be the list of intersecting scan rays.  The carve mapping estimates from each individual scan $i \in S$ are combined to generate the fused, maximum-likelihood probability estimate for this leaf node:

\begin{equation}
p_L = \dfrac{\sum\limits_{i \in S} p_i(x_L) \, w_i(x_L) }{ \sum\limits_{i \in S} w_i(x_L) }
\end{equation}

where $x_L$ represents the center position of cube $L$, and $p_i(.)$ and $w_i(.)$ represent the carve-mapping function and weighting function of each individual scan ray, respectively, as discussed in Section~\ref{sec:fss_stats}.  Once all scans are inserted into the octree, each leaf node contains its respective probability estimate $p_L$, the total weighting of this estimate, the variance of this estimate, and the number of scans that intersected $L$.  All these statistics are used later in the pipeline for analyzing the properties of $L$.  As an example, if $p_L$ is $0.5$ or less, then the node is considered {\it exterior}.  Nodes that are never intersected by any scans are assumed to be {\it exterior} and are assigned a value of $p_L=0.5$.  If the value of $p_L$ is strictly greater than $0.5$, then the node is considered {\it interior}.  The faces between {\it interior} nodes and {\it exterior} nodes are considered boundary faces of the octree, and are useful for determining the position of generated meshes.  As we discuss in Section~\ref{ssec:procarve_boundary}, the position of the mesh between two such nodes is determined using the means and variances of our estimates of $p_L$ for each node.

% simplification per chunk
Once each chunk is completely carved, we simplify its tree structure based on the dynamic range of the contained values.  A node is a candidate for simplification only if all of its sub-nodes are leaf nodes and report the same label, i.e. all are interior or all or exterior.  Additionally, all sub-nodes must have either been observed at least once or none of the nodes observed.  This condition prevents oversimplification in parts of the volume that may have been only partially scanned.  If these conditions are met, then the sub-nodes are deleted and their data are averaged and stored into their parent, which is now the new leaf node at that location.  This process is performed recursively in order to produce a minimally populated tree for the building area being modeled.  The size of each chunk can be adjusted to improve processing.  Large chunks mean that more memory is used at a time, but fewer chunks need to be processed overall.  We typically use chunks with side length $2$ meters, with an initial carving resolution of ~$0.05$ meters.

% wrap up
The final tree is only generated to full depth in the locations that require it, which are boundaries between interior and exterior spaces.  As we discuss later in Chapter~\ref{ch:better_carving}, areas of the environment that are segmented as objects in each room will be re-carved to an even finer resolution, since these locations are likely to contain high detail.

% subsection describing how boundary is found with procarve
\subsection{Computing Boundary Surfaces}
\label{ssec:procarve_boundary}

% overview
Once we have a volumetric representation of the scanned environment in an octree, we want to extract the surface of the environment from this structure.  In this section, we discuss two methods for representing the surface boundary.  The first method performs dense meshing in order to preserve all detail stored in the octree structure.  This technique is useful for representing fine detail observed in the scan environment.  The second method represents the boundary surface of the volume via a set of piece-wise planar regions.  This method is useful for reducing noise on large surfaces observed in buildings, such as floors, walls, and ceilings.

% discuss isosurface computations
\paragraph*{Dense octree meshing}
As discussed earlier, a boundary face is the surface between two abutting leaf nodes of the octree, where one leaf node has $p_{L1} \leq 0.5$ and the other has $p_{L2} > 0.5$.  When determining the offset position of the surface between these nodes, we preserve sub-node accuracy by positioning the boundary at the isosurface $P = 0.5$ generated by linearly interpolating the probability estimates of the two nodes, as shown in Figure~\ref{fig:octree_isosurface}.

% show isosurface subvoxel accuracy
\begin{figure}[h!]
	\centerline{\includegraphics[width=0.5\linewidth]{procarve/octree/isosurface}}
	\caption[Determining sub-node accuracy for boundary surface.]{We place the boundary surface of the octree between two leaf nodes with opposite labeling.  Rather than placing the surface half-way between, we use the value of the probability estimate to determine optimal surface offset, resulting in sub-node surface accuracy.}
	\label{fig:octree_isosurface}
\end{figure}

% dual contouring
Once we have identified the position offset of the surface at all boundary leaf nodes, we can generate a dense mesh of this boundary.  We use a variant of Dual Contouring~\cite{DualContouring}.  It works well with adaptively-sized nodes in an octree and well-represents both curved and sharp features in the output geometry.  Since our data labels are divided into node centers of the tree, rather than node corners, we perform dual contouring by mapping each boundary face of the octree to a vertex in the mesh and each corner of the octree into a face in the meshed output.  Examples of this meshing technique is shown in Figure~\ref{fig:gradlounge_procarve_compare}.

% planar meshing
\paragraph*{Planar region meshing}
To mesh building geometry in a planar fashion, we first partition the boundary faces of the octree into planar regions, in the same fashion as the method described in Section~\ref{ssec:voxel_region_fitting}.  

% intersections between planar regions
Each planar region represents a set of node faces along with fitting plane geometry.  The fitting plane of each region is formed by running Principal Component Analysis (PCA) on the centers of the boundary faces.  To maximize the accuracy of this surface position, we perform PCA on the isosurface positions found above, rather than the grid-aligned node faces.  Once the set of node faces is partitioned into planar regions, we generate a watertight mesh by finding the intersection points between each pair of neighboring planes and insert vertices for our output mesh.  

Naively intersecting the fitting planes of each region to position the output vertices may result in artifacts or self intersections at locations where two nearly-parallel regions are neighbors.  Figure~\ref{fig:plane_xtion} shows an example of this phenomenon in 2D.  Since the two regions shown are close to parallel, their intersection point shown as the black star is far away from the adjoining endpoints of the regions.  Rather, we use a pseudo-intersection point that is closer to the original corner position, shown as the green star in Figure~\ref{fig:plane_xtion}.  We therefore use the following technique to prevent over-constraining the intersection points of degenerate region neighbors.

These intersection points are represented by corners in the octree that intersect faces from more than one planar region.  For a given node corner with position $\vec{x}_0$ that intersects a set of planar regions $R$, we wish to find a final position of the mesh vertex that corresponds with this node corner.  If we merely took the intersection of all planes, the vertex position may be under-constrained if only two regions are touching the corner, or if some of the regions are close to being planar with each other.  
We compute this point with the following process.

Let matrix $M$ be defined so that each row $\vec{n}_i^{\,T}$ is the normal vector of the $i^{\textit{th}}$ intersecting planar region, and vector $\vec{p} = \texttt{diag}(P\,M^T)$, where matrix $P$ is defined so that its $i^{\textit{th}}$ row is any point on the $i^{\textit{th}}$ plane.  We compute the desired position of the intersection point $\vec{x}$ with the following:

\begin{equation}
	\vec{x} = \sum_{i = 0}^{3} \left( \delta_i \dfrac{\vec{p} \cdot \vec{u}_i}{s_i} + (1-\delta_i) ( \vec{x}_0 \cdot \vec{v}_i ) \right) \vec{v}_i
	\label{eq:plane_xtion}
\end{equation}

where $\vec{u}_i$ and $\vec{v}_i$ are the $i^{\textit{th}}$ columns of matrices $U$ and $V$ in the SVD decomposition of $M = U \, S \, V^T$, respectively, and $s_i$ is the $i^{\textit{th}}$ singular value of this decomposition.  The term $\delta_i$ is defined as $1$ if $s_i \geq \alpha$ and as $0$ otherwise, for some threshold $\alpha$.  Note this approach finds the intersection of all given planes associated with this corner, but does not incorporate the constraints from two nearly-parallel planes.  Equation~\ref{eq:plane_xtion} sums over all excited dimensions of the set of intersecting regions, and either projects the intersection point along a dimension if it is highly constrained or merely takes the point's original coordinate in that dimension is not well constrained.  In the 2D example in Figure~\ref{fig:plane_xtion}, the normal vectors $\vec{n}_1$ and $\vec{n}_2$ are near enough to parallel to be considered degenerate.  The corresponding nullspace is shown in grey.  As a result, the pseudo-intersection is the original corner position projected onto this nullspace rather than the true intersection shown in black. We use the threshold parameter of $\alpha=0.2 * s_{min}$ for our examples in this paper, where $s_{min}$ is the smallest singular value.

Once the locations of vertices shared by two or more planar regions are computed, then the interior area of each region is triangulated using a 2D variant of isosurface-stuffing~\cite{Isostuffing}, as described in~\cite{Turner13}.  This produces an efficient number of triangles for large, planar areas and preserves the sharp corners at the intersection of these regions. 

% plane intersection diagram
\begin{figure}
	\centerline{\includegraphics[width=0.8\linewidth]{procarve/process/plane_intersection/plane_xtion_diagram}}
	\caption[Computing intersection point for planar regions.]{An example of plane intersection in 2D.  Two adjoining regions are shown, with normals $n_1$ and $n_2$.  The naive intersection of the two regions is shown as a black star.  Rather, we join the two regions with a vertex shown at the green star, which prevents sharp discontinuities in the output.}
	\label{fig:plane_xtion}
\end{figure}

% results section for 3D carving
\section{Results}
\label{sec:carving_results}

% overview paragraph of this section
In this section, we present several comparisons and results of the methods described in this chapter.  In Section~\ref{ssec:compare_3dv2013_procarve}, we compare and contrast our proposed methods with an existing carving approach and with each other.  In Section~\ref{ssec:3dv2013_results}, we show several results of applying our methods on a variety of datasets, as well as discuss the scalability of our methods.  In addition, more results from the method described in Section~\ref{sec:procarve} are also shown in Chapter~\ref{ch:better_carving}.

% comparison to 3dv2013
\subsection{Comparison of Methods}
\label{ssec:compare_3dv2013_procarve}

% figure: comparing with carving method
\begin{figure}
	
	\centering
	\begin{minipage}[b]{0.27\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/visual_compare/coryf3_pic}}
	\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.35\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/visual_compare/snapshot_coryf3_marching_cubes_no_planes00}}
	\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.35\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/visual_compare/snapshot_coryf3_elturner_model00}}
	\centerline{(c)}\medskip
	\end{minipage}

	\caption[Comparison between voxel carving methods.]{A visual comparison of meshing method for a hallway model:  (a) photograph of scanned area; (b) an existing voxel carving method~\cite{Carving}; (c) the proposed method at 5 cm resolution.}
	\label{fig:mc_compare}

\end{figure}

The results of our surface reconstruction procedure are analyzed quantitatively and qualitatively.  For quantitative analysis, the resulting mesh is compared to an existing voxel carving scheme, which uses Marching Cubes to generate a final output~\cite{Carving}.  Figure~\ref{fig:mc_compare} shows a qualitative comparison of the two schemes.  Note that the corners where the floor meets the walls or where the walls meet the ceiling are preserved in our proposed method, whereas in traditional marching cubes, they are rounded off.  Further, since our method explicitly fits planar regions to the mesh, the output mesh contains less surface noise.  Additionally, by using adaptive triangulation of these regions, the mesh can be represented with far fewer elements.

% compare figures of 3d carving from both 3dv2013 and siggraph
Next, we compare the method of voxel carving described in Section~\ref{sec:3dv2013} with the improvements to that method proposed in Section~\ref{sec:procarve}.  Among the modifications are that probabilistic models are used to assess volumetric labeling of the input scans, an adaptive octree data structure is used to store the {\it interior}/{\it exterior} labelings, and a more sophisticated method is used to position the intersecting vertices between planar regions.  The effects of these modifications can be seen in Figure~\ref{fig:gradlounge_procarve_compare}, which shows a small lounge environment that includes a couch, a coffee table, and a pool table.  Figure~\ref{fig:gradlounge_procarve_compare}a shows a reference photograph of this environment and Figure~\ref{fig:gradlounge_procarve_compare}b shows the raw input scan points used to generate the resulting models.  

The original method of voxel carving is shown in Figure~\ref{fig:gradlounge_procarve_compare}c~\cite{Turner13}.  Note how all surfaces in the model are represented as piece-wise planar.  Additionally, the borders of these planar regions are often represented as rough or jagged.  Figure~\ref{fig:gradlounge_procarve_compare}d shows the modified method, using probabilistic octree carving.  Again, all surfaces are represented as piece-wise planar regions, as discussed in Section~\ref{ssec:procarve_boundary}.  In the modified case, however, note that the borders between regions are preserved as sharp and clean.  This characteristic is especially noticeable on the couch, where the separation between the seat and back of the couch is much noisier in Figure~\ref{fig:gradlounge_procarve_compare}c than in Figure~\ref{fig:gradlounge_procarve_compare}d.  Additionally, the top of the coffee table is skewed in Figure~\ref{fig:gradlounge_procarve_compare}c, whereas in Figure~\ref{fig:gradlounge_procarve_compare}d it is level.

We can also see the effect of using the probabilistic carving between these two models.  Since the method described in Section~\ref{sec:3dv2013}, and shown in Figure~\ref{fig:gradlounge_procarve_compare}c, assumes that any voxels intersecting scan points are exterior, the result is a ``bloated'' version of all objects, where their surfaces are offset from the scan points by half a voxel size on average.  Comparing to Figure~\ref{fig:gradlounge_procarve_compare}d, which represents the process described in Section~\ref{sec:procarve}, we see a very different result.  Since the full probability model for each scan point is used to determine the optimum position of the exported surface no bloat occurs, and the modeled output objects are the correct size.

Since, in both Figures~\ref{fig:gradlounge_procarve_compare}c and~\ref{fig:gradlounge_procarve_compare}d, all surfaces are represented with planar regions, a lot of detail gets lost in region fitting.  We can observe the original detail of the probabilistic octree model in Figure~\ref{fig:gradlounge_procarve_compare}e.  This figure shows a dense surface reconstruction of the same environment, which does not use any planar fitting.  An alternate view of this same output geometry is shown in Figure~\ref{fig:gradlounge_procarve_compare}f, which shows the same model with Gouraud shading.  Note that this version of the model preserves more detail, including the individual cushions in the couch and the legs of the pool table.  By not performing planar region fitting, however, we also preserve noise on flat surfaces, such as on the walls and floors.  In Chapter~\ref{ch:better_carving}, we discuss methods to keep the best of both worlds:  high-fidelity dense surface reconstruction on small objects such as furniture and light fixtures and aggressive plane-fitting methods on the large surfaces of the building such as floors, walls, and ceilings.

% figure: comparing 3dv2013 and procarve
\begin{figure}
	
	\centering

	% input
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/compare_to_3dv2013/photo}}
	\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/compare_to_3dv2013/pointcloud}}
	\centerline{(b)}\medskip
	\end{minipage}

	% planar meshing
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/compare_to_3dv2013/3dv2013_planar_tris}}
	\centerline{(c)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/compare_to_3dv2013/procarve_planar_tris}}
	\centerline{(d)}\medskip
	\end{minipage}

	% dense meshing
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/compare_to_3dv2013/procarve_dense_tris}}
	\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/compare_to_3dv2013/procarve_dense_smooth}}
	\centerline{(f)}\medskip
	\end{minipage}

	\caption[Comparison between region fitting methods.]{Comparisons between the various reconstruction methods discussed in this chapter.  Example shown on small lounge area:  (a) photograph of scanned environment; (b) raw point cloud scans of environment; (c) voxel carving output as described in Section~\ref{sec:3dv2013}; (d) octree carving output with improved planar region fitting, as described in Section~\ref{sec:procarve}; (e) octree carving output with dense meshing; (f) alternate shading on (e).}
	\label{fig:gradlounge_procarve_compare}
\end{figure}

% results from just 3dv2013
\subsection{Voxel Carving Results}
\label{ssec:3dv2013_results}

% show coolest results

% new park mall
\begin{figure}
	\begin{minipage}[t]{0.4\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/snapshot_newparkmall00_cropped}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.55\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/newparkmall}}
	\centerline{(b)}
	\end{minipage}
	\caption[Voxel carving result for a large shopping mall.]{Voxel carving result for a large shopping mall:  (a) Surface reconstruction of a shopping mall; (b) input point-cloud.  Resolution is 10 cm.}
	\label{fig:3dv2013_newparkmall}
\end{figure}

% walmart
\begin{figure}[p]
	\begin{minipage}[b]{0.98\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/walmart_full_color00}}
	\centerline{(a)}
	\end{minipage}
	\begin{minipage}[b]{0.48\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/walmart_mesh_snapshot00}}
	\centerline{(b)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.48\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/walmart_pointcloud}}
	\centerline{(c)}
	\end{minipage}
	\caption[Voxel carving of warehouse-sized retail center.]{Surface reconstruction of a warehouse-sized retail shopping center using voxel carving technique: (a) output mesh, shown from top-down, with each region given random color; (b) closer view of shopping aisles in output mesh; (c) corresponding view of raw point-cloud scans.  Resolution is 10 cm.}
	\label{fig:3dv2013_walmart}
\end{figure}

% cory 540
\begin{figure}
	\begin{minipage}[b]{0.48\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/snapshot_cory540_r05_p_nice00}}
	\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.48\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/cory540}}
	\centerline{(b)}\medskip
	\end{minipage}
	\caption[Voxel carving result of conference room.]{Voxel carving result of a $10.5 \texttt{m} \times 9.5 \texttt{m}$ conference room with table: (a) The output mesh; (b) corresponding input point-cloud.  Resolution is 5 cm.}
	\label{fig:3dv2013_cory540}
\end{figure}

% pier 15
\begin{figure}

	\centering
	\begin{minipage}[b]{0.98\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/pier15_triangles00}}
	\centerline{(a)}
	\end{minipage}
	\begin{minipage}[b]{0.98\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/pier15_pointcloud_depth}}
	\centerline{(b)}\medskip
	\end{minipage}

	\caption[Voxel carving results of a construction area.]{Voxel carving results of a construction site:  (a) output mesh includes several people and a static scanning station; (b) point-cloud of same view, colored by depth.  Resolution is 5~cm.}
	\label{fig:3dv2013_pier15}
\end{figure}

% bhh hallway
\begin{figure}[t]

	% top-down view
	\centering
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/snapshot_bhh_r05_full_nice01.png}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/bhhhallwaytop.png}}
	\centerline{(b)}
	\end{minipage}
	\\
	% interior view
	\begin{minipage}[b]{0.75\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/results/snapshot_bhh_r05_mirror_nice00}}
	\centerline{(c)}
	\end{minipage}

	% caption
	\caption[Voxel carving results of hotel hallways.]{Voxel carving results of the hallways in a hotel: (a) output mesh of hotel hallway, showing full top-down view; (b) corresponding full top-down view of input point-cloud; (c) output mesh of interior of hallway, showing a mirror and shelf.  Model generated at 5~cm resolution.}
	\label{fig:3dv2013_bhh_hallway}
\end{figure}

The proposed algorithm from Section~\ref{sec:3dv2013} was run on several datasets, which range in size from a single conference room to full floors of buildings such as hotels and shopping malls.  The results are shown for sections of these models, along with the corresponding views of the original point-clouds.  For these models, large flat areas are represented by fewer, larger triangles.

Figure~\ref{fig:3dv2013_newparkmall} shows the reconstruction of a shopping mall's food court.  The input point-cloud contains significant noise due to the amount of glass surfaces in the model, since most storefronts in the mall are glass.  In the food court, the restaurants are well-modeled, as well as the ceiling and skylights.  Figure~\ref{fig:3dv2013_walmart} shows our largest model, with 220 million input scan points.  This model represents the aisles of a retail store, covering an area of $112.2\texttt{m} \times 77.5\texttt{m}$ using 2.7 million triangles.  A smaller dataset is shown in Figure~\ref{fig:3dv2013_cory540}, representing a $10.5\texttt{m} \times 9.5\texttt{m}$ conference room with a hexagonal table in the center.  This table, along with the podium to the left, is well-represented in the output.  The ceiling of the conference room is inset with hanging lights, which can also be seen in the model.  

Figure~\ref{fig:3dv2013_pier15} shows a modeling of a construction site.  The surfaces of the room are represented by large regions, while the detail of objects is preserved.  Run-time analysis was performed on the dataset shown in Figure~\ref{fig:3dv2013_pier15}.  The input to this dataset contains 25 million points.  The code was run on a personal laptop with an Intel i7-2620M processor.  The voxel carving, at 5 centimeter resolution, took 55 minutes of processing time.  The surface reconstruction of these voxels took 1 minute and 2 seconds.  Previous voxel carving schemes processed similar models of 15 million points in 16 hours at the same resolution~\cite{Carving}.  Computation time was recorded for this same dataset with a resolution of 2 cm.  Voxel carving took 12 hours and 10 minutes for this resolution, while surface reconstruction took 9.5 minutes.

Figure~\ref{fig:3dv2013_bhh_hallway} shows an example of the full extent of a $96.7\texttt{m} \times 75.7\texttt{m}$ H-shaped hotel hallway.  The input point-cloud has 84 million points while the output model contains 933,000 triangles grouped into 3,096 planar regions.  Data storage is an important factor in our algorithm.  While the scheme described above only requires a small subset of the point-cloud to be in memory at any given time, it is also important to make sure that the intermediary and output data structures are reasonably sized.  Figure~\ref{fig:3dv2013_bhh_hallway} shows a moderate-size model depicting the corridors in a hotel, represented with a 6.3 GB point-cloud.  The voxel carving, at a resolution of 5 cm, requires 12.7 MB of memory.  If a conventional voxel grid structure were used to represent the entire bounding box, then 94.4 MB of memory would be required at this resolution.

% texture-mapping
Lastly, we can apply texture-mapping to these generated models to aid in assessing the quality of the geometry.  A major motivation for producing models that are composed of large, planar regions is to aid in texture-mapping~\cite{Cheng13}.  This method of texture-mapping produces a single image for each region in the model.  Since all major building features -- such as floors, walls and ceilings -- are represented by large planar regions, there is a reduction in the number of seams in the applied texture.  Objects that are significantly non-planar, such as the ladder that is along the side of the hallway, have some projection artifacts from the texture-mapping process, since these objects are represented with piece-wise planar geometry.

% coryf3 texture mapped
\begin{figure}[t]

	\centering
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/texture/snapshot_coryf3_triangles00}}
	\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.95\linewidth}
	\centerline{\includegraphics[width=1.0\linewidth]{3dv2013/texture/snapshot_coryf3_texture00}}
	\centerline{(b)}
	\end{minipage}

	% caption
	\caption[Texture-mapping applied to voxel carving result.]{A voxel carved model of the hallway of an academic building: (a) the geometry of the model; (b) with texture-mapping applied~\cite{Cheng13}.}
	\label{fig:3dv2013_bhh_hallway}
\end{figure}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Comparing 2D and 3D models
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Comparisons of 2.5D and 3D Models}
\label{ch:fp_carving_compare}

% topic sentence for this chapter
In this chapter, we show comparisons between the surface reconstruction techniques described in Chapters~\ref{ch:floorplan} and~\ref{ch:carving}.  In doing so, we analyze the size of the produced models and associated run times, compare our results to state-of-the-art methods, and discuss limitations.  We also show examples of these models with texture-mapping applied~\ref{Cheng14,Turner14Journal}.  While this dissertation does not focus on the texture-mapping procedure, such processing is useful for determining the qualitative accuracy of the model geometry, and how it aligns with the captured camera imagery.

% comparison of 3D carving method to 2.5D extruded floorplan
Our 2.5D approach produces simplified models when compared to surface reconstruction techniques that preserve fine detail with more complex output.  Specifically, the 3D carving method includes interior objects such as furniture.  Figure~\ref{fig:visigrapp_compare_to_carving} compares the models resulting from our 2.5D method described in Chapter~\ref{sec:visigrapp} with that of the 3D building modeling technique for hotel hallways~\cite{Turner14}.  The two methods result in 2,944 triangles and 4.1 millions triangles, respectively.

% basic comparison of two methods
\begin{figure}
   \centering
   \begin{minipage}[b]{0.49\linewidth}
   \includegraphics[width=1.0\linewidth]{visigrapp/results/compare/snapshot10}
   \centerline{(a)}
   \end{minipage}
   \hfill
   \begin{minipage}[b]{0.49\linewidth}
   \includegraphics[width=1.0\linewidth]{visigrapp/results/compare/snapshot09}
   \centerline{(b)}
   \end{minipage}
   \caption[Comparison of 2.5D simple model with a 3D carved model.]{Comparison of models from (a) our 2.5D simple modeling approach with (b) the approach discussed in this chapter.}
   \label{fig:visigrapp_compare_to_carving}
\end{figure}

% figure with third-party comparison
\begin{figure}[t]

	\begin{minipage}[b]{0.58\linewidth}
		\centerline{\includegraphics[height=4.3cm]{jstsp2014/compare/pointcloud_bright}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.4\linewidth}
		\centerline{\includegraphics[height=4.3cm]{jstsp2014/compare/mura13_triangles.png}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.58\linewidth}
		\centerline{\includegraphics[height=4.3cm]{jstsp2014/compare/carving_triangles_bright}}
		\centerline{(c)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.4\linewidth}
		\centerline{\includegraphics[height=4.3cm]{jstsp2014/compare/floorplan_triangles_bright}}
		\centerline{(d)}\medskip
	\end{minipage}

	\caption[Comparison of models to existing approach.]{Comparison to state-of-the-art method: (a) Static point-cloud scans from VmmlLab set of~\cite{Mattausch14}; (b) reconstruction of dataset using method described in~\cite{Mura13}; (c) reconstruction of dataset using method from this Chapter; (d) reconstruction of dataset using method from Chapter~\ref{sec:visigrapp}.}
	\label{fig:stateofartcompare}

\end{figure}

% compare to third-party reconstruction techniques
We compare our reconstruction schemes from Chapter~\ref{ch:carving} to static scanning systems, even though they were originally designed to work with ambulatory acquisition systems~\cite{Backpack}.  As shown in Figure~\ref{fig:stateofartcompare}, point-clouds generated from traditional static-scanning technologies can be used as inputs for our reconstruction techniques. The represented scans in Figure~\ref{fig:stateofartcompare}a are taken from the VmmlLab dataset of~\cite{Mattausch14}.  The original paper for this dataset details segmentation of point-clouds, and not surface reconstruction, but the same group has also developed surface reconstruction approaches~\cite{Mura13,Mura14}. This dataset contains 133 million points from three scan locations, representing a 20 foot $\times$ 30 foot room.  We can convert these data to be used by our techniques by treating each scan location as a pose in the path of an assumed mobile system.  A comparison to a state-of-the-art method is shown in Figure~\ref{fig:stateofartcompare}b, which generates a model of floors, walls, and ceilings also using floor plan generation techniques~\cite{Mura13}.  This model is represented with 12 triangles.  The models shown in Figures~\ref{fig:stateofartcompare}c and~\ref{fig:stateofartcompare}d are constructed from the methods described in Chapters~\ref{ch:carving} and~\ref{ch:floorplan}, respectively.  The detailed model in Figure~\ref{fig:stateofartcompare}c is represented by 6.6 million triangles, while the simple model shown in Figure~\ref{fig:stateofartcompare}d is represented by 124 triangles.  The remainder of the examples shown in this section are generated from our ambulatory system.  Note that the main difference between these two sources of scans is the level of mis-registration noise. Unlike ambulatory systems, which can result in mis-registration up to 27~cm~\cite{NickJournal}, static scanning systems can be accurate to 0.25~cm~\cite{Mura13}.

% pier 15
\begin{figure}[t]

	% top row
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.3cm]{jstsp2014/joint/pier15/pier15_camera}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.3cm]{jstsp2014/joint/pier15/pier15_carving01}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.3cm]{jstsp2014/joint/pier15/pier15_carving00_compressed}}
		\centerline{(c)}\medskip
	\end{minipage}
	
	% bottom row
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.3cm]{jstsp2014/joint/pier15/pier15_pointcloud00_compressed}}
		\centerline{(d)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.3cm]{jstsp2014/joint/pier15/pier15_floorplan01}}
		\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.3cm]{jstsp2014/joint/pier15/pier15_floorplan00_compressed}}
		\centerline{(f)}\medskip
	\end{minipage}
	
	\caption[Comparison of models of construction environment.]{Close-up of models generated with the techniques described in this paper:  (a) photograph of scanned area; (b) surface carving model from Chapter~\ref{ch:carving}; (c) surface carving with textures; (d) point-cloud of scanned area; (e) extruded floor plan model from Chapter~\ref{ch:floorplan}; (f) extruded floor plan with texturing.}
	\label{fig:compare_pier15}

\end{figure}

% discuss runtime and memory usage of each process

Run-time analysis was performed on the dataset shown in Figure~\ref{fig:compare_pier15}.  The input to this dataset contains 25 million points.  The code was run on a laptop with an Intel i7-2620M processor with 8 GB of RAM.  All approaches presented in this paper were implemented in C++ as single-threaded programs.  The voxel carving method described in Chapter~\ref{ch:carving}, at 5~cm resolution, took 55 minutes of processing.  The surface reconstruction of these voxels took 1 minute and 2 seconds.  Previous voxel carving schemes processed similar models of 15 million points in 16 hours at the same resolution~\cite{Carving}.  Computation time was recorded for this same dataset with a resolution of 2~cm.  Voxel carving took 12 hours and 10 minutes at this resolution and surface meshing took 9.5 minutes.

The same input point-cloud was processed using the 2.5D modeling approach described in Chapter~\ref{ch:floorplan} on the same hardware.  The processing step of extracting wall samples from the input point-cloud took 84.3 seconds.  Once these wall samples were generated, the process of generating the mesh took a total of 3.5 seconds.  This step includes data i/o, the floor plan generation, and the 3D extrusion of the floor plan.  While our surface carving routine is efficient when compared to other similar techniques, generating a model using 2D information is orders of magnitude quicker.  Since the floor plan generation technique can be applied in a streaming fashion to input grid-map data, it could be able to run in real-time for compatible SLAM systems.

The texture-mapping process requires more computation time than the surface reconstruction schemes.  Using a single-threaded implementation on the same hardware, texturing the output surface of the voxel carving method as shown in Figure~\ref{fig:compare_pier15} took 10 hours and 44 minutes.  The texture-mapping of the surface generated from the floor plan extrusion approach took 113 minutes. The shorter time to texture-map this surface is due to the far fewer number of elements in the output mesh.

% Mission bay figure, showing limitation of texture projection
\begin{figure}[t]
	\centerline{\includegraphics[width=0.82\linewidth]{jstsp2014/joint/missionbay/OR_comparison}}
	\caption[Comparison of surface reconstruction methods for hospital operating rooms.]{Surface reconstruction of a hospital's operating room area:  (a) Floor plan of environment generated using method from Chapter~\ref{ch:floorplan}; (b) surface carving model from Chapter~\ref{ch:carving}; (c) surface carving with textures; (d) extruded floor plan model from Chapter~\ref{ch:floorplan}; (e) extruded floor plan with textures.  Note that the objects in the environment are not represented in the mesh of the extruded floor plan, so their texture is projected to the back wall of the room.}
	\label{fig:comparison_missionbay}
\end{figure}

While using an extruded floor plan mesh to generate a texture-mapped model is far more computationally efficient, this method has its own set of limitations.  By effectively removing the geometry for furniture and other objects in the environment, this method creates a disparity between what is seen in the camera imagery and the reconstructed mesh.  As such, the texture for these objects is incorrectly projected onto the wall surfaces behind the objects, as shown in Figure~\ref{fig:comparison_missionbay}.  The advantage of a fully-3D meshing method is the preservation of fine detail in the geometry, while the advantage of the floor plan extrusion method is speed, simplicity, and a higher proportion of large, planar surfaces that allow for efficient texturing.

% houston figure
\begin{figure}

	% top row
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/houston_3D_interior_2}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/results_houston_2_3d_compressed}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/houston_2D_interior_2}}
		\centerline{(c)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/results_houston_2_2d_compressed}}
		\centerline{(d)}\medskip
	\end{minipage}
	
	% bottom row
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/houston_3D_interior_3}}
		\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/results_houston_3_3d_compressed}}
		\centerline{(f)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/houston_2D_interior_3}}
		\centerline{(g)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[height=4.4cm]{jstsp2014/joint/houston/houston3_2_compressed}}
		\centerline{(h)}\medskip
	\end{minipage}

	\caption[Comparison of modeling results of a hotel lobby.]{Modeling results of hotel lobby: (a) Desk area modeled with method from Chapter~\ref{ch:carving}; (b) texture-mapping applied; (c) area modeled with method from Chapter~\ref{ch:floorplan}; (d) texture-mapping applied; (e) main lobby area modeled with method from Chapter~\ref{ch:carving}; (f) texture-mapping applied; (g) area modeled with method from Chapter~\ref{ch:floorplan}; (h) texture-mapping applied.}
	\label{fig:comparison_houston}

\end{figure}

% discuss data product size (compare output of 2D vs 3D meshing)
Figure~\ref{fig:comparison_houston} shows the results of modeling the scans collected in a hotel lobby.  The input point clouds for this model consisted of 70.4 million points, which comprised 5.16 GB on disk.  The generated models cover 2,317 square meters, or about 25,000 square feet.  The surface carving model, as shown in Figures~\ref{fig:comparison_houston}a and~\ref{fig:comparison_houston}e, is represented by 2.65 million triangles.  The extruded floor plan, as shown in Figures~\ref{fig:comparison_houston}c and~\ref{fig:comparison_houston}g, is modeled with 2,944 triangles.  This reduction by a factor of a thousand means that finer details in the model such as furniture or drop ceilings are not present, but can aid in many applications, including texture-mapping.

% talk about size of textured models
The size of the resulting texture-mapped models is much larger due to the generated high-resolution textures from camera imagery.  The texture-mapping of the surface carving model, as shown in Figures~\ref{fig:comparison_houston}b and~\ref{fig:comparison_houston}d, is 1.45 GB on disk comprised of textures for 1,277 distinct surfaces.  The texture-mapping of the extruded floor plan depicted in Figures~\ref{fig:comparison_houston}f and~\ref{fig:comparison_houston}h takes up 488 MB on disk with 566 surfaces.  The largest surface for texturing is the floor of the main lobby area, whose texture is represented by a $14210 \times 10555$ image.  A video fly-through of these models can be found online~\cite{video}.

% lead into next two chapters, discuss how methods can improve each other
While the 2D floor plans, 2.5D extruded models, and the full 3D complex models all have their respective uses and applications, we see that they represent the building in fundamentally different ways.  As such, these modeling methods can be used to enhance each other, using one method to improve the quality of another.  In Chapter~\ref{ch:better_floorplans}, we discuss how analysis of the 3D complex model can be used to improve the results of the 2D floor plans and their corresponding extruded models.  In Chapter~\ref{ch:better_carving}, we discuss how the opposite can also be applied, where the 2D floor plans are used to improve the quality of the 3D models, including added analysis about separating the 3D geometry of furniture from the rest of the building model.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Improving floorplans with carving
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Improvements of 2D Floor Plan Models using Complex 3D Models}
\label{ch:better_floorplans}

In Chapter~\ref{ch:floorplan}, we discussed method to generate 2D floor plans and 2.5D extruded models from floor plans.  These models were generated by analyzing the raw point-cloud data.  In Chapter~\ref{sec:procarve}, we discussed how we can generate a probabilistic octree representation of the scanned environment.  This octree can be used to generate complex 3D models of the scanned area.

In this chapter, we show how the processing steps used to generate the octree data structure, which represents a complex 3D model, can be used to improve the quality of the 2D floor plan models.  We discuss how using the octree data as input instead of raw point cloud values can improve the accuracy and aesthetics of the output floor plans.

In Section~\ref{sec:octree_level_split}, we describe how the octree structure can be utilized to improve how we split a multi-story model into individual levels.  In Section~\ref{sec:oct2fp}, we discuss how the existing floor plan technique can be improved by using octree input.  In Section~\ref{sec:hia}, we show how these octrees can also be used for unique 2D visualizations of the scanned environment.

\section{Level-Splitting using Octree Structure}
\label{sec:octree_level_split}

% talk about level-splitting with octree
First, we split a given octree model into separate building levels.  Such a split can be performed on the original point cloud scans, as described in Chapter~\ref{sec:pointcloud_level_split}.  This method of level splitting finds horizontal surfaces in the point cloud data by performing a histogram of the point elevations and searching for peaks.  An alternate method to perform level splitting on a building model is to find horizontal regions in the boundary of the octree data structure generated in Chapter~\ref{ssec:procarve_octree}.  

% example level splitting
\begin{figure}
	\centerline{\includegraphics[width=0.8\linewidth]{procarve/level_splitting/octree_level_split}}

	\caption[Using octree to find levels of two-story dataset.]{An example of using the octree data structure to perform level-splitting in a two-story office building.  This graph shows a histogram of surface area of horizontal surfaces with respect to elevation.  By finding peaks in floor and ceiling surfaces, we can estimate where one level ends and another begins.}
	\label{fig:octree_level_split}
\end{figure}

We first produce an octree data structure of the scanned environment.  Next, we perform planar region fitting on the boundary faces of this octree, as described in Chapter~\ref{ssec:procarve_boundary}.  Next, we select all horizontally-aligned regions within the model, discarding any regions whose normals are more than $5$~degrees away from vertical.  We then discard any regions that do not meet a certain surface area threshold.  This steps allows for us to only consider the dominant horizontal surfaces when finding the elevations of floors and ceilings, which provides a much cleaner histogram representation than if every point in the point cloud were used.  We can also more accurately separate floors (whose normal is upward facing) from ceilings (whose normal is downward facing) than if performing histogramming with a point cloud, whose components have no normal information.  Additionally, a histogram of points assumes a relative uniform sampling of each surface.  If the operator traversed one level significantly slower than another level, then the first level will have more points, skewing the histogram.  By performing the histogram on surface area, we can assure that the geometry will be analyzed independently of how it was traversed.

Figure~\ref{fig:octree_level_split} shows an example of this analysis on a two-story dataset.  We plot the total surface area of horizontal planar regions found at each elevation.  Elevations that have a peak amount of floor surfaces are likely to denote the beginning of a level and elevations that have a peak amount of ceiling surfaces are likely to denote the end of a level.  We then deduce the total number of levels by finding the pair of floor and ceiling heights for each level.  The model can be split by level at the elevation halfway between the ceiling of the lower level and the floor of the upper level.  The visualization of the two levels of this building model are shown later in this chapter, in Section~\ref{sec:hia}.

% section on improving wall samples using octrees
\section{Generating Floor Plans from Octree Structure}
\label{sec:oct2fp}

Several methods of surface reconstruction of building environments ignore low-level detail and aim to only model the primary floors, walls, and ceilings within the environment.  In this section, we describe how we use the populated octree from Chapter~\ref{sec:procarve} to generate this simplified model.  We use the same technique discussed in Chapter~\ref{sec:visigrapp} to produce 2D floorplans of the environment and extrude 2.5D models using the height information from each room~\cite{Turner14}.  Rather than applying this technique directly on wall samples generated from raw point-cloud scans of the environment, we instead produce the input data for this method from our populated octree.  We show that this approach yields a more accurate model that is both well-aligned with our octree and more aesthetically consistent with real-world geometry.  The remainder of this section describes how we produce 2D wall samples from the octree, use these samples to generate a 2D floorplan of the environment, and create a 2.5D simplified model from the floorplan.

\subsection{Wall Sampling}
\label{ssec:oct2dq}

In order to generate a floor plan that can be extruded into a simplified 2.5D model, we first need to generate a set of wall samples in the environment.  Wall samples are a set of points in 2D space that are locations with high likelihood of being wall positions.  This set of points is used by the 2D floor plan generation procedure as input data~\cite{Turner14}.  In Chapter~\ref{sec:wall_sampling}, these wall samples are generated either from the output grid-map of a particle filter or by analyzing a 3D point cloud of the environment.

However, this approach is not robust to misclassification of some surfaces as walls.  Any long vertical structure contributes to the wall samples, causing environment features such as bookshelves, doors, or other objects to be detected as walls.  By finding wall position estimates from the octree, rather than directly from the point-cloud, we use topology information to ensure that detected surfaces are large and planar.  We show that this approach not only produces a floor plan better aligned with the complex geometry of the octree, but also one that is less affected by clutter, such as furniture, than when using the point cloud directly to generate the floor plan.

% figure about finding regions and identifying which are walls
\begin{figure}
	% region growing
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sampling/original_regions}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sampling/merged_regions}}
		\centerline{(b)}
	\end{minipage}
	

	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sampling/wall_regions}}
		\centerline{(c)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sampling/wall_points_darker}}
		\centerline{(d)}
	\end{minipage}

	% caption
	\caption[Generating wall samples from an octree.]{Generating wall samples from an octree: (a) initialize regions on the octnodes' boundary faces; (b) perform region growing to form large planar regions; (c) filter out wall regions; (d) generate points along planar regions to make wall samples.}
	\label{fig:oct2dq}
\end{figure}

The first step of generating wall samples from the octree is to identify large planar surfaces.  We cluster the boundary faces of the octree into planar regions as discussed in Chapter~\ref{ssec:procarve_boundary}.  Figure~\ref{fig:oct2dq}a shows the initial boundary regions of a model, with each initial region depicted as a separate color.  These regions are iteratively merged in the process described in Chapter~\ref{ssec:voxel_region_fitting}.  The result is a single planar region for each dominant surface of the model, as shown in Figure~\ref{fig:oct2dq}b.  

%more discussion about which regions are selected
We then filter the regions based on the normal direction of the fitting plane, keeping only the surfaces that are vertically-aligned, as shown in Figure~\ref{fig:oct2dq}c.  Any surface whose normal vector is more than $5$~degrees off of horizontal is rejected.  This approach makes the reasonable assumption that walls are vertical, which fits almost all building types.  We also filter wall surfaces based on their surface area and vertical height.  Regions that are less than $0.05$~square meters or have a vertical extent less than $1$~meter are not considered.

The output regions have gaps corresponding to the portion of the walls hidden behind any furniture in the model.  In order to counteract these occlusions, we expand the represented geometry of each wall to include any {\it exterior} points that are within the 2D convex hull of each wall planar region.  The check for whether points are {\it exterior} is performed using the populated octree.  Figure~\ref{fig:oct2dq}d shows a set of these points, sampled uniformly, across the wall plane.  

Once we obtain these 3D wall positions, we can convert them to the 2D wall sample representation described in Chapter~\ref{sec:wall_sampling}.  Each column of sampled points along the 3D surface becomes a 2D wall sample in the XY-plane.  The vertical extent of this column is used to estimate the height of the wall at that position.  These wall samples are a cleaner representation than those extracted from point-clouds directly, since any surface variations have been flattened beforehand.  Additionally, since the entire surface has been interpolated behind objects in the environment, the wall representations are more robust to clutter in the environment, which leads to fewer false positives in the presence of large obstacles such as bookshelves or doors.  These attributes can be seen by comparing wall samples develop with this new method as opposed to wall samples taken directly from point clouds, as shown in Figure~\ref{fig:octree_fp_compare}.

% subsection on making floor plans from these new wall samples
\subsection{Floor Plan Generation}
\label{ssec:octree_floorplan}

% show figure comparing wall samples and floorplans
\begin{figure}

	% using pointcloud
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/floorplan/original_dq_zoom}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/floorplan/original_fp_zoom}}
		\centerline{(b)}
	\end{minipage}
	
	% using octree
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/floorplan/octree_dq_zoom}}
		\centerline{(c)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/floorplan/octree_fp_zoom}}
		\centerline{(d)}
	\end{minipage}

	% caption
	\caption[Comparison of wall samples and floor plans.]{Comparison of wall samples and floorplans: (a) wall sampling generated from original point cloud; (b) corresponding floorplan; (c) wall sampling generated from octree; (d) corresponding floorplan.  All units are in meters.}
	\label{fig:octree_fp_compare}
\end{figure}

Once we use the octree to generate the wall samples for a model, we can feed those samples into the 2D floor plan generation method discussed in Chapter~\ref{sec:visigrapp}.  This method produces a watertight 2D model that defines the scanned area.  Figure~\ref{fig:octree_fp_compare} shows a comparison of floor plans generated with the octree versus floor plans generated with the raw scan points.  Figures~\ref{fig:octree_fp_compare}a and~\ref{fig:octree_fp_compare}b show the wall samples and the floor plan generated from the raw scan points, respectively.  Obstacles in the environment such as furniture are not properly removed, causing the output walls to be noisy.  Figures~\ref{fig:octree_fp_compare}c and~\ref{fig:octree_fp_compare}d show the wall samples and floor plan generated from the octree.  Since the region merging allows more sophisticated separation of large planar surfaces, the furniture in the environment is properly removed and the output model is cleaner.  As an example, the upper-right corner of this model contains a large bookcase, which appears in the wall sampling in Figure~\ref{fig:octree_fp_compare}a and is propagated to the floor plan in Figure~\ref{fig:octree_fp_compare}b.  This bookcase is correctly removed from the wall samples generated from the octree in Figure~\ref{fig:octree_fp_compare}c, which means the wall is more appropriately represented in the floor plan shown in Figure~\ref{fig:octree_fp_compare}d.

% fp comparison figure
\begin{figure}[t]

	\centering
	% floorplan from pointcloud
	\begin{minipage}[t]{1.0\linewidth}
		%\centerline{\includegraphics[width=1.0\linewidth]{oceanview-office/oceanview_fp_pc}}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/mulford/mulford_fp_pc}}
		\centerline{(a)}
	\end{minipage}

	% floorplan from octree
	\begin{minipage}[t]{1.0\linewidth}
		%\centerline{\includegraphics[width=1.0\linewidth]{oceanview-office/oceanview_fp_oct}}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/mulford/mulford_fp_oct}}
		\centerline{(b)}
	\end{minipage}

	% caption
	\caption[Comparison of floor plans of large office environment.]{Comparison of floor plans of large office environment: (a) floor plan generated from raw point cloud scans; (b) floor plan generated from octree proposed here.  The octree floor plan has much fewer artifacts due to clutter or furniture, as shown in areas ``1'' through ``4''.}
	\label{fig:compare_fp_mulford}

\end{figure}

% better floorplans
The approach discussed in this section uses the generated octree to produce improved wall samples, which in turn can be used to .  When compared to floor plans generated from raw point cloud scans, our proposed approach results in not only better alignment to the complex geometry, but also a cleaner floor plan~\cite{Turner14}.  We demonstrate this contrast with Figure~\ref{fig:compare_fp_mulford}, which represents a scan of a 14,079 square foot office area with over $50$ rooms.  This data acquisition took 25 minutes and processing the octree took 84~processor~hours.  An example floor plan from the previous method is shown in Figure~\ref{fig:compare_fp_mulford}a.  This floor plan has several artifacts caused by clutter and furniture in the environment.  Locations ``1'' and ``3'' show rooms filled with large amounts of objects, causing holes in the floor plan.  Locations ``2'' and ``4'' show rooms with a single large object that occluded part of a wall, causing a incorrect notch in the floor plan.  The floor plan of the same environment generated by our proposed technique is shown in Figure~\ref{fig:compare_fp_mulford}b.  This floor plan correctly separates the rooms of the environment and does not have the same artifacts as in the previous method.  Additionally, long hallways in the building are correctly represented as one room, rather than being split into several small segments.

In addition to the examples shown here, several more examples of improved floor plans are shown along-side 3D models in Chapter~\ref{ch:better_carving}.

% improving height estimates with octree
\subsection{2.5D Extrusion}
\label{ssec:octree_fp_extrusion}

% show figure comparing wall sample heights
\begin{figure}

	\centering
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sample_heights/photo}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sample_heights/original_wall_heights}}
		\centerline{(b)}
	\end{minipage}
	
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sample_heights/new_wall_heights}}
		\centerline{(c)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/wall_sample_heights/overlay_wall_heights}}
		\centerline{(d)}
	\end{minipage}

	% caption
	\caption[Comparison of wall extrusion methods.]{Example showing improvement in estimating wall heights for extruded floor plans: (a) reference photo of environment; (b) the original wall surfaces, whose heights are determined by the surfaces' vertical extents; (c) the modified wall surfaces, whose heights are determined by the elevation of floor and ceiling planar regions in the room; (d) overlay of the original and new wall surfaces.}
	\label{fig:octree_wall_sample_heights}
\end{figure}

Height information is stored in this floor plan, so a 2.5D model is extruded, resulting in a simplified representation of the floors, walls, and ceilings in the environment.  As originally discussed in Chapter~\ref{ssec:fp_extrusion}, the floor plans are extruded into 2.5D models by assigning a single pair of floor and ceiling heights to each room in the environment.  The values for these heights are taken by finding the median floor and ceiling heights across all wall samples in each room.  As shown in Chapter~\ref{ssec:fp_extrusion}, if the wall samples are generated directly from the point cloud, their estimates of heights are very noisy.  This effect is due to the heights being based on the local surface directly intersected by the wall sample.  If part of a wall is occluded by furniture, then those wall samples will have inaccurate heights.

By generating wall samples from the boundary surfaces of the octree, we are able to perform more robust analysis for the extruded heights of the floor plans.  Rather than looking directly at the vertical extent of a wall surface, we instead observe the elevations of large horizontal surfaces in the environment.  These surfaces are likely to be floors and ceilings for each room.  We use the topology of the octree to find the closest large, horizontal surface to each candidate vertical wall surface.  When extracting wall samples, we save the elevations of the nearby floors and ceilings, instead of relying on the vertical extent of the wall itself.

This process allows for higher accuracy in the heights of each room, since it is robust against clutter in the environment.  An example is shown in Figure~\ref{fig:octree_wall_sample_heights}.  This figure shows a kitchen environment, as seen in the reference photograph in Figure~\ref{fig:octree_wall_sample_heights}a, with example wall surfaces detected using both methods.  Figure~\ref{fig:octree_wall_sample_heights}b shows the original wall surfaces, whose vertical extents are estimated by the vertical extent of the original planar region found in the octree boundary.  Figure~\ref{fig:octree_wall_sample_heights}c shows the new method of computing wall surfaces, whose vertical extends are estimated by first classifying large horizontal surfaces as floors and ceilings, then adjusting the wall heights by the elevations of these surfaces.  Lastly, Figure~\ref{fig:octree_wall_sample_heights}d shows both sets of surfaces overlayed.  Note that the original surfaces are not consistent with the actual height of the room, whereas by aligning the heights of walls with the elevations of floors and ceilings, we can produce a more consistent floor plan extrusion that is more accurate to the original model.

% hia stuff
\FloatBarrier
\section{Visualizing Top-Down Histograms of Octree Carvings}
\label{sec:hia}

The octree data structure described in Chapter~\ref{ssec:procarve_octree} represents the full 3D volumetric information about a scanned environment.  These values can span across several scanned floors of the environment and are used to generate the fully-detailed 3D models shown in Chapter~\ref{ch:carving}.  We can also compile these data to present a 2D visualization of the modeling environment, as discussed in this section.

We generate a top-down, 2D histogram of the values stored in the octree.  In each node, a value for the probability of interior is stored.  By generating a sum for each $xy$-position of these values, which intersects all nodes along a vertical column, we can determine the expected ``open height'', in meters.  This value indicates how much of the model is labeled {\it interior} at each $xy$-position, yielding a map of the 2D representation of each level.

% show figure of example hia
\begin{figure}

	\centering
	\begin{minipage}[t]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hia/coryf23/coryf23_level_0_hia}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hia/coryf23/coryf23_level_1_hia}}
		\centerline{(b)}
	\end{minipage}

	% caption
	\caption[Top-down histogram of two-story octree model.]{An example top-down histogram of a two-story model, generated from a volumetric octree structure: (a) the first level; (b) the second level.  These plots are colored by the amount of open height at each location, so that red indicates a large open space and blue indicates a very shallow volume.}
	\label{fig:hia_coryf23}
\end{figure}

Figure~\ref{fig:hia_coryf23} shows an example of this top-down histogram.  This figure uses the same model that was represented in Figure~\ref{fig:octree_level_split}, which represents a two-story academic office building.  Since these models show a representation of all volume that was scanned within the octree, the output includes partial scans of the exterior area of the building, which were observed through windows and doors to the outside.  As we discuss later in Chapter~\ref{ch:better_carving}, these artifacts can be removed by performing an intersection test with the generated floor plan of the environment.

% show figure of example hia in gradlounge (high-res)
\begin{figure}

	\centering
	\begin{minipage}[t]{0.55\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hia/gradlounge/photo}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.35\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hia/gradlounge/gradlounge_highres}}
		\centerline{(b)}
	\end{minipage}

	% caption
	\caption[Top-down histogram of single room.]{A high-resolution top-down histogram of the octree model of a single room:  (a) reference photograph of room; (b) histogram visualization of model, with features labeled.}
	\label{fig:hia_gradlounge}
\end{figure}

We can also view a close-up representation of one room in this same area, as shown in Figure~\ref{fig:hia_gradlounge}.  This room is also present in the upper-left corner of Figure~\ref{fig:hia_coryf23}b.  This visualization represents all the features in the room environment, including detail of the furniture within the room.  This level of detail includes the curvature of the cushions and armrests on the couch, and the shape of the legs of the pool table.  Note that the histogram picks up the detail of the light fixtures on the ceiling as well, which show up as three bars across the length of the room.  The bookshelf on the right-side of the room shows up with less clarity than other objects in the environment.  This is due to most of the bookshelf being {\it interior} area: open shelves facing forward.  This visualization shows how analysis of the octree structure gives more accurate wall positions than wall samples taken from the point-cloud scans directly.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Improving carving with floorplans
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Improvements of 3D Complex Models using 2D Floor Plan Models}
\label{ch:better_carving}

In this chapter, we expand on the techniques described in Chapter~\ref{ch:carving}.  These techniques are used to generated fully-detailed, complex 3D models of the observed environment.  In this chapter, we add information gained from the floor plan geometry in order to improve the quality and capabilities of these dense models.  By combining both types of modeling techniques, we can improve the accuracy of the 3D complex models and allow for further analysis of the scanned area.

In Section~\ref{sec:explosion_removal}, we detail how floor plan intersection tests can be used to remove unsightly artifacts in the 3D complex models.  Such artifacts are primarily due to scans that go beyond the building environment, such as scans of the outdoors through windows.  In Section~\ref{sec:procarve_main_idea}, we discuss how combining floor plan and octree volumetric models allow for segmentation of objects and furniture in the environment.  Such segmentation allows for more detailed meshing and resolution improvement on the areas of the model that require it, fully watertight models of the interior furniture, and more accurate building models.  Lastly, in Section~\ref{sec:procarve_results}, we show several real-world scans with this technique applied, show-casing how such analysis can improve model output quality.

% removing explosions from carving
\section{Removing Exterior Scanning Artifacts}
\label{sec:explosion_removal}

% figure showing trimming for floorplans, pointclouds, carvings
\begin{figure}[t]

	\centering

	% before
	\begin{minipage}[t]{0.3\linewidth} % floor plan
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/joint/office/floorplan_notrim}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth} % point cloud
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/joint/office/pointcloud_notrim}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth} % carving
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/joint/office/carving_notrim}}
		\centerline{(c)}\medskip
	\end{minipage}

	% after
	\begin{minipage}[t]{0.3\linewidth} % floor plan
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/joint/office/floorplan_withtrim}}
		\centerline{(d)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth} % point cloud
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/joint/office/pointcloud_withtrim}}
		\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth} % carving
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/joint/office/carving_withtrim}}
		\centerline{(f)}\medskip
	\end{minipage}
	
	\caption[Using room labels to trim and improve models.]{Using room labels to trim and improve models.  Top-down view of: (a) Floor plan before room trimming; (b) point cloud before trimming, colored by height with ceiling points removed for viewing; (c) top-down view of surface carving before trimming; (d) floor plan after trimming; (e) point cloud after trimming; (f) surface carving after trimming.}
	\label{fig:explosion_trimming}

\end{figure}

% trimming and improving floorplan by rooms
Room labeling within a model provides an effective mechanism to prevent misrepresentation of poorly scanned areas.  The mobile scanning system does not necessarily traverse every room and may only take superficial scans of room geometry while passing by a room's open doorway.  When a room is not entered, the model is unlikely to capture sufficient geometry and therefore it is necessary to remove this poorly scanned area from the model.  If none of the triangles for a room within the floor plan are intersected by the scanner's path, we can infer the room is never entered.  The room's triangles are then relabeled from interior to exterior, removing it from the floor plan.  Figures~\ref{fig:explosion_trimming}a and~\ref{fig:explosion_trimming}d show an example floor plan before and after such trimming occurs, respectively.  Note the removal of a sharp extrusion in the bottom-right corner, which was a partial scan through a window.

% removing explosions in 3d carving using floorplans
Similar refinement can be used to improve the 3D carving method described in Chapter~\ref{ch:carving}.  Due to the nature of voxel carving, laser scans that pass through a building window or open doorway can capture geometry outside of the desired scanned area.  Since the outside volume is only observed from a few angles, the resulting carving produces undesirable artifacts.  Using the 2.5D extruded floor plan, we can automatically remove scans from the input point cloud that fall outside our desired area.  Figures~\ref{fig:explosion_trimming}b and~\ref{fig:explosion_trimming}e show the corresponding point clouds before and after the result of this trimming, colored by height with the ceiling points removed.  Any scans that pass through windows or doorways are removed.  As a result, the surface carving of these point clouds discussed in Chapter~\ref{sec:3dv2013}, as shown in Figures~\ref{fig:explosion_trimming}c and~\ref{fig:explosion_trimming}f respectively, can be improved by reducing these undesirable artifacts.  Similarly, the improved surface reconstruction scheme using an octree structure, as discussed in Chapter~\ref{sec:procarve} can also be improved by preventing any nodes labeled {\it interior} by the carving but are well outside the floor plan geometry from being considered when exporting the mesh.

% how to bloat floor plan to accomplish this
We do not simply remove any points that are outside of the floor plan area, since there may be geometry for interior walls or other features that are not labeled as {\it interior} in the floor plan that we still want to keep.  Instead, we make the assumption that any points that are at least some distance $d$ meters from any {\it interior} area of the floor plan are likely to be outside the scan environment.  Figure~\ref{fig:bloated_fp} shows an example of this process.  In Figure~\ref{fig:bloated_fp}a, a top-down view of the 3D complex mesh is shown for a model of a residential apartment.  The corresponding floor plan of the same area is shown in Figure~\ref{fig:bloated_fp}b, in purple.  We also show the extend of the ``expanded'' floor plan area, in red, which will be intersected with the octree volume in order to determine which nodes should be ignored.  Any octree nodes that are outside of the red area of the expanded floor plan are not considered for meshing.  The result of this modification is shown in Figure~\ref{fig:bloated_fp}c, where the artifacts in the model caused by scanning exterior area through doors and windows are removed, leaving only the building geometry.

% figure showing bloated fp
\begin{figure}[t]

	\centering

	% before
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/trimming/erics_apt_before}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/trimming/erics_apt_bloated_fp}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/trimming/erics_apt_after}}
		\centerline{(c)}\medskip
	\end{minipage}
	
	\caption[Removing artifacts from octree carving with floor plans.]{Example of using floor plan geometry to remove scan artifacts from 3D octree carving:  (a) a top-down view of the original octree carving; (b) the corresponding floor plan geometry (in purple) with a $1$~meter buffer of nearby area (in red); (c) the resulting octree carving after only considering interior area within the nearby area of the floor plan geometry.}
	\label{fig:bloated_fp}

\end{figure}

% segmenting furniture and buildings
\FloatBarrier
\section{Segmenting Furniture and Room Geometry}
\label{sec:procarve_main_idea}

% display cartoon of set difference method
\begin{figure}[t]
	\centerline{\includegraphics[width=1.0\linewidth]{procarve/mainidea/cartoon_flowchart}}
	\caption[Diagram showing how models are used to segment objects in scene.]{The scanned volume is meshed using two approaches that are combined to separate room geometry and object geometry.  The complex geometry from the octree (upper left, in red) and the simple geometry from the 2.5D model (lower left, in blue) are both volumetric techniques.  They are combined to extract the object volume (upper center, in green) and the building volume (lower center, in grey).  These volumes are meshed separately and exported (right, in black).}
	\label{fig:procarve_mainidea}
\end{figure}

Our proposed method takes the octree-based 3D carving method described in Chapter~\ref{sec:procarve} and the 2.5D extruded floor plan method described in Chapter~\ref{sec:visigrapp} in order to perform object segmentation within the scanned environment.  In both modeling formats, each point in space has some probability of being {\it interior} or {\it exterior}.  We define {\it interior} space to be empty or open area that range scans can pass through.  We define {\it exterior} space to be solid material in the environment, including furniture and building structure.

% motivation of separate meshing techniques
Once we have a fully populated octree containing interior/exterior values, there is enough information to generate a surface reconstruction of the environment.  Such volumetric representations of building environments can be fed into existing techniques to generate a mesh of the scanned area~\cite{Turner13,Kintinuous,Carving}.  However, these meshes are limited in that they use the same method for meshing all parts of the environment.  In this section, we discuss how we segment this representation to separate the volume of objects, such as furniture, from the rest of the building geometry and use different meshing techniques on each of these parts separately.

% high level overview of floorplan generation
The primary goal of our approach is to use this volumetric information to form two watertight meshes of the environment.  The first mesh only represents the building geometry, including floors, walls, ceilings, windows, and doors.  The second mesh represents the objects in the environment such as furniture, light fixtures, or other items.  The block diagram of the method used for this segmentation is shown in Figure~\ref{fig:procarve_mainidea}.  We first generate two representations of the same environment.  The populated octree represents a complex model of the volume, as shown by the red graphic in the upper-left of Figure~\ref{fig:procarve_mainidea}.  We then generate a simplified 2.5D model of the same volume, as shown by the blue graphic in the lower-left of Figure~\ref{fig:procarve_mainidea}.  This simplified model is obtained by first generating a 2D floorplan of the scanned area, then extruding the floor plan to form a 2.5D model that explicitly represents the floors, walls, and ceilings of the model.  This mesh does not represent any interior objects, but is aligned with the major surfaces of the building.

% merging models and getting meshes
We then perform a set difference between these two volumetric models, keeping the volume that is labeled {\it exterior} by the octree and {\it interior} by the extruded floor plan.  This subset of the volume represents the objects in the environment, and is shown as the green graphic in the upper-center of Figure~\ref{fig:procarve_mainidea}.  Similarly, we can denote the union of the {\it interior} space of both models to be the building geometry, as shown in the lower-center of Figure~\ref{fig:procarve_mainidea}.  Once the model is segmented into object and building geometry, we can generate meshes for each type.  The object geometry is refined to enhance detail and is meshed uniformly to preserve its fine structure.  The building geometry is split into planar surfaces and each surface is triangulated efficiently, preserving the sharp corners between floors, walls, and ceilings.  We perform this planar meshing on the carved representation of the building geometry, rather than simply using the 2.5D extruded mesh, because it preserves building features such as windows and door-frames.  These two meshes are combined to form the whole environment, shown by the graphic at the right-side of Figure~\ref{fig:procarve_mainidea}.  This approach has the added benefit of modeling hidden surfaces, such as the backs of furniture or areas of walls occluded by objects.

% section on aligning floor plans to octree geometry
\subsection{Aligning Floor Plan Geometry to Octree Structure}
\label{ssec:procarve_fp_alignment}

% display the flow chart for this approach
\begin{figure*}[t]
	\centerline{\includegraphics[height=2.0cm]{procarve/process/flowchart/system_flowchart}}
	\caption[System flowchart of our approach.]{System flowchart of our approach.  Scan Preprocessing is described in Chapter~\ref{sec:fss_stats}, Carving is detailed in Chapter~\ref{sec:procarve}, Wall Sampling and Floor Plan Generation are discussed in Chapter~\ref{sec:oct2fp}, Merge Models is delineated in Chapter~\ref{ssec:procarve_fp_alignment}, and Planar and Detailed Meshing are documented in Chapter~\ref{ssec:procarve_boundary}.}
	\label{fig:procarve_flowchart}
\end{figure*}

% basic overview of section, bring us up to speed on what is being used
In this section, we combine many ideas that are introduced throughout this dissertation.  The overview of data flow through this process is outlined in Figure~\ref{fig:procarve_flowchart}.  In Chapter~\ref{sec:fss_stats}, we discuss how we probabilistically model the input scans in order to produce volumetric estimates of {\it interior}/{\it exterior} occupancy from each scan point.  In Chapter~\ref{sec:procarve}, we describe how these scans are efficiently combined to generate a unified occupancy estimate for the entire scan volume.  These occupancy estimates are stored in an octree, representing a complex model of the environment.  In Chapter~\ref{sec:oct2fp}, we detail how the octree is used to produce a simplified building model by first generating a 2D floor plan and then extruding the floor plan into a 2.5D mesh.  

In this section, we discuss how the complex model in the octree and the simplified model in the 2.5D extruded floor plan are merged in order to segment the volume delineating objects such as furniture in the environment.  This processes produces two separate watertight models:  one for the building elements itself and one for the objects and furniture inside the building.  These two types of models are meshed using the two separate techniques discussed in Chapter~\ref{ssec:procarve_boundary}:  dense uniform meshing on the octree structure and planar meshing by fitting large planar regions to the octree boundary.  Each of these surface reconstruction techniques are geared to efficiently characterized their respective parts of the building.

% figure about floorplan alignment
\begin{figure}[t]

	% full nodes
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/fp_align/original_octree}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/fp_align/furniture_unaligned}}
		\centerline{(b)}
	\end{minipage}
	
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/fp_align/furniture_aligned}}
		\centerline{(c)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/fp_align/furniture_aligned_highres}}
		\centerline{(d)}
	\end{minipage}
	
	% caption
	\caption[Aligning floor plans to octree geometry for object segmentation.]{Example of aligning floor plan to segment objects: (a) original octree nodes, at max resolution of $6.25$ cm; (b) segmented objects using unaligned floor plan; (c) segmented objects using aligned floor plan; (d) the segmented objects are recarved to a resolution of $0.8$ cm.}
	\label{fig:procarve_fp_align}
\end{figure}

% explain the alignment process (high-level)
Both the extruded floor plan and the original octree are volumetric models of the environment, so we can classify the overlapping volumes into three categories.  First, locations that are {\it exterior} in the octree yet {\it interior} in the extruded floor plan are objects or furniture in the environment.  Locations labeled {\it exterior} by both models are considered part of the building structure.  Lastly, all locations labeled {\it interior} by the octree are considered open space interior to the building, regardless of the extruded floor plan's labeling.  Volume intersected by the boundary of the 2.5D floor plan is considered {\it exterior}, since these represent the primary building surfaces and not objects within the building.  Using this segmentation, we can now consider the objects in the building separately from the 2.5D building structure.  For instance, in Figure~\ref{fig:procarve_fp_align}a, we see the original octree leaf nodes of a scanned environment.  By performing a set difference of the octree volume from the volume of the 2.5D model of the environment, we can extract the furniture and other objects.  Figure~\ref{fig:procarve_fp_align}b shows the segmentation using an unaligned floor plan and Figure~\ref{fig:procarve_fp_align}c shows the result with a fully aligned floor plan.  The unaligned floor plan was generated directly from the raw point cloud of the scans whereas the aligned floor plan was generated with our method described in Chapter~\ref{sec:oct2fp}.  With a properly segmented representation of the room's objects, we can recarve the nodes of the octree containing object geometry, since these locations tend to have finer detail than the rest of the model.  Figure~\ref{fig:procarve_fp_align}d shows an example of this recarving, which has been refined from the original resolution of $6.25$ centimeters to a new resolution of less than a centimeter.

% TODO additional info/math on fp alignment

% why alignment is important
The effect of misalignment can be seen by comparing Figures~\ref{fig:procarve_fp_align}b and~\ref{fig:procarve_fp_align}c.  In Figure~\ref{fig:procarve_fp_align}b, the octree was segmented using a floor plan generated directly from the point cloud and not the octree.  As such, parts of the back wall and window are mislabeled as objects and kept in the output.  However, in Figure~\ref{fig:procarve_fp_align}c, the octree was segmented using a floor plan generated via the technique described in this section.  The back wall is no longer mislabeled and only the actual furniture in the environment are segmented as objects.

% TODO additional info on refining object resolution

% wrap up
Once we have fully merged these models, each node of the octree is labeled as either object geometry or room geometry.  In addition to refining the resolution of object nodes, we can use this labeling to adjust how we generate a mesh for each portion of the environment.  

% subsection on using separate meshing techniques to represent parts of model
\subsection{Meshing Building and Object Volumes}
\label{ssec:procarve_meshing}

% figure shows different meshing techniques, and hidden surfaces modeled
\begin{figure}[t]

	% picture of kitchen
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/meshing/camera_image}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/meshing/kitchen_all}}
		\centerline{(b)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/meshing/kitchen_room_tris}}
		\centerline{(c)}
	\end{minipage}
	
	% show the object geometry from various angles
	\centering
	\hfill
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/meshing/kitchen_object_tris}}
		\centerline{(d)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.3\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/process/meshing/kitchen_object_frombehind}}
		\centerline{(e)}
	\end{minipage}
	\hfill	

	% caption
	\caption[Example meshing output of residential area.]{Example meshing output of residential area: (a) photo of area; (b) all reconstruction geometry; (c) geometry of room surfaces only, colored by planar region; (d) geometry of objects only; (e) geometry of objects from behind, showing watertightness.}
	\label{fig:procarve_kitchen}
\end{figure}

% overview
After segmenting the octree geometry into objects and rooms, we can mesh each separately.  Objects such as furniture, light fixtures, doors, etc. tend to have higher detail than the room-level geometry.  The room-level geometry tends to be composed of large, planar surfaces.  We use a dense meshing technique to represent the object geometry, which preserves detail and curves in the geometry.  For the room-level geometry, we identify planar regions and mesh each plane with large triangles.  Figure~\ref{fig:procarve_kitchen} shows an example of applying both of these methods to a given model.  Figure~\ref{fig:procarve_kitchen}a shows a photograph of the scanned area (a kitchen table) and Figure~\ref{fig:procarve_kitchen}b shows the final output of all meshing approaches combined.

% planar meshing
To mesh building geometry, we first partition the boundary faces of the octree into planar regions, as discussed in Chapter~\ref{sec:procarve_boundary}.   We perform planar meshing on the octree elements in order to represent building features rather than simply using the 2.5D extruded mesh generated from the floor plan because the latter does not capture sufficient detail of the environment.  Features that do not follow the 2.5D assumption, such as windows or door-frames, are unable to be captured by the extruded floor plan mesh.  As shown in Figures~\ref{fig:banner}d and~\ref{fig:procarve_kitchen}c, the planar mesh of the building surface still provide geometry for features such as window recesses.

% dense meshing
We choose to use dense meshing on the objects and furniture found in the environment.  This dense meshing technique is fully described in Chapter~\ref{ssec:procarve_boundary}, and based off of dual contouring~\cite{DualContouring}.  The advantage to this method is that it preserves all detail from the octree when generating the output surface, and takes full advantage of the probabilistic nature of the octree, allowing for sub-voxel accuracy in the mesh.  Figure~\ref{fig:kitchen}d shows the object mesh in isolation for an example model.  

The disadvantage of this method is the output mesh size.  Even though there is some adaptive meshing, due to the adaptive nature of the octree, by far most of the output is of uniform element size, resulting in a large number of elements.  We make the assumption that objects and furniture in the building represent only a fraction of the total surface area observed in the model, which enables us to generate dense meshes for these objects of fine detail, while still leaving the remainder of the model to be meshed more economically.

% watertightness
An important aspect of meshing these two segments separately is to ensure watertightness of building and object models.  The surfaces of walls hidden behind any occluding objects are still meshed, even though they are never directly scanned.  This effect can be seen in Figure~\ref{fig:kitchen}c.  Similarly, the hidden surfaces of objects are also fully meshed.  Figure~\ref{fig:kitchen}e shows the rear surface of the table, which was filled in based on the wall location.

% results section for better carving
\FloatBarrier
\section{Results}
\label{sec:procarve_results}

% grad lounge
\begin{figure}[t]
	% show the area
	\begin{minipage}[c]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/gradlounge_pic}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/gradlounge_nodestris}}
		\centerline{(b)}
	\end{minipage}
	
	% output
	\begin{minipage}[c]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/gradlounge_mesh_all}}
		\centerline{(c)}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/gradlounge/gradlounge_mesh_room}}
		\centerline{(d)}
	\end{minipage}

	% make a caption
	\caption[Surface Reconstruction of small room.]{An area modeled by our technique: (a) a photo of the room; (b) the volumetric boundary of room; (c) final mesh with room and objects modeled; (d) final mesh of room only, colored by planar region.}
	\label{fig:procarve_gradlounge}
\end{figure}

% text that references fig:procarve_gradlounge
We aim to improve on existing methods by combining two fundamentally different surface reconstruction techniques for building environments.  We first generate both a fully detailed model of the environment and a highly simplified representation of the same scanned area, using the approaches of~\cite{Turner13,Turner14}.  We then combine these models to segment volumetric representations of the interior objects in the environment, such as furniture or light fixtures, from the permanent surfaces of the building such as floors, walls, and ceilings. These steps allow us to generate accurate, watertight models of objects in the building distinct from the building model itself, as demonstrated in Figure~\ref{fig:procarve_gradlounge}.  We first produce a volumetric representation of the entire space, stored in an octree, as shown in Figure~\ref{fig:procarve_gradlounge}b.  We use this representation to produce a rich model of the environment, as shown in Figure~\ref{fig:procarve_gradlounge}c.  The objects of the environment, shown in white, can be separated from the building structure, as shown in Figure~\ref{fig:procarve_gradlounge}d.  We use different meshing techniques for the objects and the building itself in order to ensure the best representation of each type of surface.  The result is a rich model of the environment that represents a whole building based on level, room, or individual objects.

% Hybrid Operating Room table
\begin{figure}[t]

	% camera photo	
	\centering
	\begin{minipage}[t]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/HybridORs/camera_photo}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.49\linewidth}% full model
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/HybridORs/full}}
		\centerline{(b)}
	\end{minipage}

	% foreground object
	\begin{minipage}[t]{0.6\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/HybridORs/object_foreground}}
		\centerline{(c)}
	\end{minipage}

	% caption
	\caption[Example scan of equipment in hospital operating room.]{Example scan of equipment in hospital's hybrid operating room: (a) picture of scanned area; (b) model of area; (c) object model triangulation of operating table and equipment.}
	\label{fig:procarve_hybrid}

\end{figure}

% overview
The goal of our technique is to generate models of large scanned environments and still preserve fine detail of objects in those environments.  In this section, we discuss the advantages of our method both with qualitative examples and quantitative results.  All models shown were generated on an Intel Xeon 3.10 GHz processor.

% discuss hybrid OR
In Figure~\ref{fig:procarve_hybrid}, we show results for a scan we generated of several rooms in a hospital operating area.  This model contains four rooms, covering a total of 1,937 square feet, and was scanned using our hardware system in 2 minutes 47 seconds.  Processing this model took a total of 6 hours and 8 minutes.  The room shown in Figure~\ref{fig:procarve_hybrid}a is a hybrid operating room, which contains several medical scanners affixed to the ceiling.  As shown in Figure~\ref{fig:procarve_hybrid}b, our approach segments the geometry of the scanners from the rest of the building and generates a mesh for the entire environment.  Figure~\ref{fig:procarve_hybrid}c shows the geometry of the operating table and medical equipment only.  Note that this object would be difficult to model with techniques that semantically classify geometry to form a mesh, since it is unlikely that shape libraries would have many examples of such an usual device.  Since our technique does not need to classify the shape, we can still generate an accurate representation of its geometry.

% mega-figure of 6thfloor mission bay UCSF
\begin{figure}[p]

	% Close-up of reception desk
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/reception_desk_north/camera_image}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/reception_desk_north/mesh_all_color}}
		\centerline{(b)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/reception_desk_north/mesh_room_tris}}
		\centerline{(c)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/reception_desk_north/mesh_objects_tris}}
		\centerline{(d)}
	\end{minipage}

	% floorplan
	\begin{minipage}[t]{1.0\linewidth}
		\centerline{\includegraphics[width=0.7\linewidth]{procarve/6thfloor/floorplan/6thfloor_all}}
		\centerline{(e)}
	\end{minipage}

	% close up of room
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/room_next_to_skipped/camera_image}}
		\centerline{(f)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/room_next_to_skipped/mesh_all_color}}
		\centerline{(g)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/room_next_to_skipped/mesh_room_tri}}
		\centerline{(h)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/6thfloor/room_next_to_skipped/mesh_object_tri}}
		\centerline{(i)}
	\end{minipage}	

	% caption
	\caption[Example model of large office environment.]{Example model of large office environment:  (a) Photo of area ``1'' in model; (b) full mesh of area ``1''; (c) triangulation of room geometry in area ``1''; (d) triangulation of object geometry in area ``1''; (e) generated floor plan of scanned environment, colored by room; (f) photo of area ``2'' in model; (g) full mesh of area ``2''; (h) triangulation of room geometry in area ``2''; (i) triangulation of object geometry in area ``2''.}
	\label{fig:procarve_6thfloor}

\end{figure}

% 6th floor
In order to show the scalability of our approach, we scanned a large office environment, shown in Figure~\ref{fig:procarve_6thfloor}.  This model contains a complete scan of 41~rooms as well as the surrounding hallways, totaling 13,048 square feet.  The data acquisition was completed in 17~minutes and processed in 48~processor-hours.  The building geometry is represented by 4~million triangles and the objects are meshed with a total of 13~million triangles.  Even though the building surfaces compose most of the environment, they represent the minority of triangles since those surfaces can be meshed efficiently as planar regions.  By comparison, if the entire model had been meshed using dual contouring, then it would have been composed of 21.5~million triangles.

We show examples of the output model from multiple locations.  Figures~\ref{fig:procarve_6thfloor}a-\ref{fig:procarve_6thfloor}d show the reception desk located at point ``1'' in the floor plan shown in Figure~\ref{fig:procarve_6thfloor}e.  The desk, computers, and sink in this area are correctly segmented as objects and the primary building surfaces are meshed in a planar fashion.  The building surfaces can be seen in Figure~\ref{fig:procarve_6thfloor}c and the objects in Figure~\ref{fig:procarve_6thfloor}d.  We also show an example resulting mesh from inside one of the rooms, in Figures~\ref{fig:procarve_6thfloor}f-\ref{fig:procarve_6thfloor}i.  These figures represent location ``2'', as marked on Figure~\ref{fig:procarve_6thfloor}e.  Note that the door frame, desks, and other objects in this room are correctly segmented from the model.  Most of the wall fixtures have shapes that are non-planar, so they are more accurately modeled using a dense meshing scheme as discussed in Chapter~\ref{ssec:procarve_boundary} rather than the corresponding planar meshing scheme.  The primary building surfaces are still meshed in a planar fashion, resulting in a model that preserves the sharp edges and flat surfaces of the floors, walls, and ceilings.

% limitations
Our approach does have some limitations.  Since the object segmentation procedure described in Section~\ref{sec:procarve_main_idea} uses volumetric intersections with an extruded 2.5D model based on a floor plan, it relies on assumptions this 2.5D model makes about the scanned environment.  In this approach, each room has fixed floor and ceiling heights.  If a room's ceiling is not horizontal, then it is approximated with a horizontal surface.  Figure~\ref{fig:procarve_bookcase} shows a few additional limitations.  This figure shows a set of boxes on top of a raised platform next to a bookcase.  The raised platform is identified as a separate object, even though it is part of the building structure, since it is at a different elevation than the rest of the floor in this room.  Additionally, this figure shows a floor-to-ceiling bookcase.  Since the position of fitted walls is found by looking for vertical surfaces, it is difficult to accurately gauge the depth of the shelves, especially when they are filled.  As a result, the wall is not positioned correctly, and only part of the bookcase is segmented as an object.  It is important to note, however, that all these limitations are highly localized.  Other parts of the model, such as the set of boxes or the coat-rack, are still meshed correctly in the presence of these issues.

% figure of Eric's apt bookcase and ledge, showing limitations
\begin{figure}[t]

	\begin{minipage}[t]{0.45\linewidth} % camera photo	
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/erics_apt/bookshelf/camera_image}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\linewidth} % mesh
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/erics_apt/bookshelf/mesh}}
		\centerline{(b)}
	\end{minipage}
	
	% caption
	\caption[Example mesh of bookcase and boxes.]{Example mesh of bookcase and boxes: (a) photograph of scanned area; (b) generated mesh, showing both building and object geometry.}
	\label{fig:procarve_bookcase}
\end{figure}

% TODO add examples of mulford

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Applications
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Applications of Building Models}
\label{ch:applications}

% motivation for this chapter
Now that we have discussed various techniques for producing models of scanned building environments, we review a variety of applications for these models.  In this dissertation we have proposed three main categories of models:  2D floor plans, 2.5D extruded models that produce highly simplified geometry, and fully complex 3D models.  In this chapter, we discuss applications for each of these techniques that go beyond visualization of the building geometry.  Each of the discussed methods uses the generated mesh of the building environment to perform some analysis or computation.  As such, the complexity of the mesh plays an important role in how effective these computations can be, and it is often important to have all categories of models for each analyzed environment.

% overview of section
In Section~\ref{sec:application_energyplus}, we discuss how these models can be used to aid building energy simulations, which help for energy audits for indoor environments.  Understanding the geometry of the building environment at many resolutions facilitates detection of important features in the building environment.  In Section~\ref{sec:application_indoornav}, we discuss how building models can be used to facilitate indoor navigation.  By producing a fully scan of a building environment, we can generate a database of fingerprints, uniquely identifying each location in the environment.  Then, at a later time, a user can identify their location in the environment by performing a look-up in this database.

% section on energy simulation
\section{Building Energy Simulation}
\label{sec:application_energyplus}

% interest in BIMS in general
Building Information Models (BIMs) have seen considerable use in the Construction, Engineering, and Architecture communities~\cite{AutodeskBIM}.  It is often important to not just have the architectural blue-prints for a building, but to also be able to compare those plans to the as-built layout of the building.  Such comparisons allow for architects to ensure the structure is being built to specification, or to account for any deviations that may take place.  Additionally, renovations to a building environment can make the original architectural plans out-of-date for such concerns.

% recent interest in energy
Similarly, recent trends have been to perform as-built surveys of building environments when performing energy audits.  The large-scale of design for a building can often play important roles in its energy efficiency, but such analysis also includes many building features that would not be on the original architectural plans, such as the types of bulbs used for lighting or the window material used.  As-built analysis is required in order to produce the most accurate energy audit for a building environment~\cite{EBConsulting}.

% energyplus and the need for simple geometry
Recently, tools have been developed to perform an automated simulation of the thermal flow of a building environment using imported building geometry~\cite{EnergyPlus}.  While these tools were intended to require human users to manually generate the geometry, our modeling system can be employed to produce models of the quality required to perform thermal simulations.  Since these simulations are performed across the entire building environment, highly simplified geometry is required.  Models of buildings as discussed in Chapter~\ref{ch:carving} contain millions of triangles, and are intractably large for such simulation engines.  This need for simplified 3D geometry of the building environment is part of the motivation for our work on extruded floor plans discussed in Chapter~\ref{sec:visigrapp}.

% model with windows
\begin{figure}[t]

	\centerline{\includegraphics[width=0.8\linewidth]{applications/energyplus_window_detection}}
	
	% caption
	\caption[Example building energy model with automatically-detected windows.]{Exmaple building model used for energy audit simulation.  This model features automatically-detected windows based on the input scans from our backpack system.}
	\label{fig:windows}
\end{figure}

% discuss windows, plugloads, lights
Part of the analysis required to produce energy simulations with out automatically generated models is to detect building features relevant to energy usage.  Such features include windows, light fixtures, equipment plug-loads, and occupancy.  Even though the simplified 2.5D extruded floor plan model is used to form the geometry for these simulations, all scan components are necessary to perform the processing and analysis to detect these features.  Figure~\ref{fig:windows} shows an example model used for energy simulation, generated by our backpack scanning system.  The windows shown in this model are automatically generated using the floor plan model from Chapter~\ref{sec:visigrapp} and the colored point clouds from Chapter~\ref{sec:pointcloud}~\cite{Zhang14}.  Plugload detection is performed by classifying objects via depth-maps produced using the 3D complex models discussed Chapter~\ref{sec:3dv2013}.  Detected computers are used to estimate the occupancy counts, which are split into zones based on the partitioned rooms in the environment as shown in floor plan generation in Chapter~\ref{sec:visigrapp}.

% wrap up
The result is a full building simulation based on automatically generated models of the building environment, using scans from our backpack-mounted system.  Such simulations can be expanded even further using the additional sensors on the scanning system.  Thermal cameras can be used to detect faulty HVAC equipment or leaks in the insulation~\cite{Oreifej14}, barometers can be used to map pressure variance from room-to-room within the building, and dataset timestamps can be used to determine local weather conditions during the model acquisition.

% section on indoor navigation
\section{Indoor Navigation}
\label{sec:application_indoornav}

% motivation
Another application for building models is for the field of indoor navigation.  Since indoor environments are often GPS-denied, it is difficult for a user to identify their position and orientation while indoors.  A popular technique is to perform a building scan using a modeling system, such as our backpack system.  Once the building model is processed, then a user can traverse the building environment at a later date, using much simpler equipment such as a cell-phone.  This technology gives navigation ability to users in populated indoor environments without the need for modifications to the building environment or requiring carrying bulky scanning equipment. 

% section on indoor navigation
Several technologies have been utilized in order to produce a unique fingerprint for each location in the building environment.  A popular technique is to produce fingerprints from camera imagery~\cite{Liang13}.  This method records images features taken at each location during the data acquisition.  Later, a user can return the that location in the environment and take a new picture.  By comparing image features, a match can be found and a location and orientation returned to the user.  In order to process these images, a detailed 3D model of the environment, as discussed in Chapter~\ref{sec:3dv2013}, must be produced in order to generate normal-maps and depth-maps for each camera image taken in the original dataset.  These geometry attributes are used to compute the difference between the pose of the original database image and the new query image, allowing for accurate positioning in the environment.

% indoor navigation with wifi
A more advanced technique combines imagery localization with readings from Wi-Fi antennas~\cite{Levchev14}.  This method tracks a users continuous movement by first performing a rough localization using fingerprinting of observed Wi-Fi signals, then tracks their local movements using imagery.  In order to perform such tracking, this method requires an accurate floor plan of the environment, as generated by Chapter~\ref{sec:visigrapp}, in order to ensure that estimates of the user position does not vary outside of the valid interior area of the building.

% figure of stoneridge wifi
\begin{figure}[t]

	\begin{minipage}[t]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{applications/stoneridge/stoneridge_map}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.49\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{applications/stoneridge/stoneridge_wifi}}
		\centerline{(b)}
	\end{minipage}
	
	% caption
	\caption[Analysis of Wi-Fi signals in scans.]{Example of how Wi-Fi signals can be analyzed by our scanning system: (a) the listed directory of a shopping mall~\cite{Stoneridge}; (b) observed Wi-Fi signal strength.  Observed Wi-Fi hotspots match those listed in the mall directory.}
	\label{fig:stoneridge_wifi}
\end{figure}

% discuss wifi mapping
Individual rooms in a building are likely to have a unique set of signal strengths for all observed Wi-Fi SSIDs, which enables fingerprinting of the environment.  However, these Wi-Fi signals also allow for rich analysis of building structures.  By observing and mapping Wi-Fi signal strength, we can determine the likely location of wireless access points.  Figure~\ref{fig:stoneridge_wifi} shows an example of this process.  Figure~\ref{fig:stoneridge_wifi}a shows the directory listing of the Stoneridge Mall~\cite{Stoneridge}, which includes the rough locations of their Wi-Fi hotspots.  By performing a scan of the public areas of the environment, we can visualize a floor plan of this area, as shown in Figure~\ref{fig:stoneridge_wifi}b.  For each position traversed during scanning, we can collect the signal strength of the mall's public wireless network.  As observed in this figure, the maximum strength comes from areas in the mall that match the hotspot locations listed, showing that we can track wireless access points.  These signals are broadcast at frequencies that do not easily transmit through thick building material, such as brick or concrete, allowing some analysis of structure materials based on areas with low signal strength.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Conclusion
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Conclusions}
\label{ch:conclusion}

% summarize scanning system
In this dissertation, We present a variety of robust methods of surface reconstruction designed for indoor building environments.  Our methods can take input scans with high noise typically observed in mobile ambulatory acquisition systems.  We showed how our backpack-mounted hardware system provides a rapid and efficient mechanism for generating models of large building environments with minimal acquisition time.

% summarize different methods of modeling
Since indoor environments are often GPS-denied, our system computes localization of the path the human operator traversed.  This localization approach allows for scalable acquisitions of tens of thousands of square feet with only 5-10 inches of positional error globally.  The modeling techniques we describe in this dissertation are formulated to be robust to this level of noise in the input localization, and can provide accurate geometry of the building environment using our backpack system.

We discuss three modes of indoor building modeling:  2D floor plans, simplified 3D models, and fully-detailed complex 3D models.  Each of these modeling approaches produces a watertight representation of the scanned interior environment and is useful for may varied applications in construction, architecture, engineering analysis, virtual/augmented reality, and indoor navigation.  We show how we have advanced our techniques for each of these model types to allow scalable acquisition of large building environments.  We also show the application of each of these techniques on a wide array of real-world datasets, encompassing building types such as academic buildings, office environments, hospitals, residential areas, shopping malls, construction sites, and hotels.

% summarize how these methods can be combined
Additionally, we show that these techniques are not only useful independently, but can be combined to further improve accuracy and functionality.  Full 3D analysis can be utilized to improve the accuracy of the geometry generated for 2D floor plans or to allow for new methods of 2D visualization.  Floor plans can be employed in order to improve the aesthetics and accuracy of fully 3D models, as well as allow for segmentation of the watertight geometry for furniture and other objects in the building environment.  Such segmentation can utilized to improve the level of detail for how these objects are represented in the exported model.  This segmentation could also be used in the future for more accurate classification of these objects.  

Once objects are separated from the building geometry, the building itself can be modeled more accurately, producing estimates for positions of surfaces that would otherwise be occluded by clutter in the environment.  Our approach allows partitioning of building geometry into levels, rooms, and individual objects.  This degree of segmentation is an important step to automatically generating richly defined Building Information Models that represent all aspects of the environment. 

% summarize applications
We show how these techniques are applicable to a variety of existing research topics, including automated energy auditing and indoor navigation.  By producing richer models of building environments that go beyond simplified representations of observed geometry, we can provide avenues for simulation and analysis that will improve how we use these spaces and how they are constructed in the future.

% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
\bibliography{elturner}
\vfill

% that's all folks
\end{document}
