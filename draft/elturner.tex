%\documentclass[journal]{IEEEtran}
%\documentclass[11pt,draftcls,onecolumn]{IEEEtran}
%\documentclass[10pt,twocolumn,twoside]{report}
\documentclass[12pt,onecolumn,oneside]{book}
\usepackage[margin=1.0in]{geometry}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% *** HEADER/PAGE NUMBERING RELATED PACKAGES ***
\usepackage{blindtext}% adds the blindtext
%\usepackage{showframe}% shows a framed header, type area and footer
\setlength{\headheight}{1.1\baselineskip}
%\setlength{\headsep}{10pt}


% *** GRAPHICS RELATED PACKAGES ***
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{../figures/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpg,.jpeg,.png}

% *** MATH PACKAGES ***
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\interdisplaylinepenalty=2500

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=blue
}

% Force headers and page numbers to always be on the upper right
\usepackage[nouppercase,automark]{scrpage2}
\clearscrheadfoot
\pagestyle{scrheadings}
%See the KOMA-script documentation for more information
\ihead{\rightmark}
%\ihead[]{\headmark}%helpful for two-sided printing
\ofoot[\pagemark]{\pagemark}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% begin document with roman page numbering
\pagenumbering{roman}
\begin{document}

% TODO deal with title and author pages, which have special requirements

\title{Eric's Ph.D. Thesis}

% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Eric~Turner}
%\thanks{This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

% make the title area
\maketitle

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% ABSTRACT
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the abstract uses arabic page numbering and has its own counter
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

% TODO do abstract, which also has special formatting requirements

%\begin{abstract}
This is where the abstract will go.
%\end{abstract}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% FRONT MATTER:  dedication, table of contents, list of figures, acknowledge
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{roman}
\setcounter{page}{1}

% dedication page
\clearpage
\vspace*{\fill}
\begin{center}
To Mom.
\end{center}
\vfill % equivalent to \vspace{\fill}
\clearpage

% table of contents
\addcontentsline{toc}{chapter}{Table of Contents}
\tableofcontents{}

% list of figures
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures{}

% list of tables
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listtablename}
\listoftables{}

% acknowledgement section
\newpage
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Acknowledgements}
\chapter*{Acknowledgements}

% berkeley is an awesome place
As soon as I saw University of California, Berkeley, I knew I would love it here.  A campus cut with forrested paths and narrow streams seemed straight out of a Calvin \& Hobbes strip.  As soon as I met the people here, I immediately found that the intellectual adventure matched the aesthetics.

% Avideh and thesis committee
I first want to thank my advisor, Professor Avideh Zakhor.  The potential she sees in her students becomes manifest with each challenge she places before them.  My understanding of the field and of academics in general has become so much richer thanks to her guidance.  I also would like to thank Jonathan Shewchuk, Kyle Steinfeld, and Carlo Sequin, who served on my qualifying exam and dissertation committees.  These are some of the best professors I've ever met, and have provided me with excellent guidance both in the classroom and in my committee.

% funding
This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

% lab folks
I also want to extend my deep gratitude to all my colleagues in the Video and Image Processing Lab.  John Kua, Michael Krishnan, Rick Garcia, Shangliang Jiang, Nicholas Corso, Shicong Yang, Richard Zhang, and Joe Menke have all lent me guidance and helped me through.  I especially want to mention Nicholas, who ensured that I took a daily constitutional around campus rather than hiding away in our lab.

% TODO rest of acknowledgements
5) Bay-area friends
7) Dad and family
8) Sam

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Introduction
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{Introduction}
\label{ch:introduction}

% motivation text
Laser scanning technology is becoming a vital component of building construction and maintenance.  During building construction, laser scanning can be used to record the as-built locations of HVAC and plumbing systems before drywall is installed.  In existing buildings, blueprints are often outdated or missing, especially after several remodelings.  Such scans can be used to generate building models describing the current architecture.  Meshed triangulations allow for the efficient representation of the scanned geometry.  In addition to being useful in the fields of architecture, civil engineering, and construction, these models can be directly applied to virtual walk-throughs of environments, gaming entertainment, augmented reality, indoor navigation, and energy simulation analysis.  These applications rely on the accuracy of a model as well as its compact representation.

% figure with pointcloud and models
\begin{figure}

	% top row
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_photo}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_triangles}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_texture_compressed}}
		\centerline{(c)}\medskip
	\end{minipage}
	
	% bottom row
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_pointcloud_compressed}}
		\centerline{(d)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_triangles}}
		\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_texture_compressed}}
		\centerline{(f)}\medskip
	\end{minipage}
	
	\caption[Models generated with our techniques.]{Models generated with the techniques described in this dissertation:  (a) photograph of scanned area, academic building; (b) surface carving model of this area; (c) surface carving model with texturing; (d) point cloud of captured scans; (e) extruded floor plan model of area; (f) extruded floor plan with texturing.}
	\label{fig:coryf2}

\end{figure}

% applications
Generating an accurate model of indoor environments is an emerging challenge in the fields of architecture and construction for the purposes of verifying as-built compliance to engineering designs~\cite{Bosche10,Xiong13}.  This task is made more challenging by the GPS-deprived nature of these environments~\cite{Liang13}.  Another application that requires an exported mesh to retain as much detail as possible is historical preservation via a virtual reality model~\cite{VillageHeritage,Carving}.  Building energy efficiency simulations can use watertight meshes of the environment to estimate airflow and heat distribution~\cite{EnergyPlus}.  These simulations require simplified meshes as input, since finite element models are difficult to scale.  It is also important to be able to generate an immersive visualization and walk-through of the environment for these applications, so experts can remotely inspect the scanned environment via telepresence, a task that currently requires expensive travel and on-site visits.  Different applications require models of different complexities, both with and without furniture geometry.  The modeling approaches detailed in this report are useful for both types of applications, as shown in the examples in Figure~\ref{fig:coryf2}.  Figure~\ref{fig:coryf2}a is a photograph of the scanned area: the hallways of an academic building, encompassing about 1,000 square meters of scanned area.  Figure~\ref{fig:coryf2}d represents the captured 3D point-cloud of this area.  Figures~\ref{fig:coryf2}b and~\ref{fig:coryf2}c show a high-detail 3D mesh of 2.7 million triangles generated using the algorithm in Chapter~\ref{ch:carving}, with and without texturing, respectively.  Figures~\ref{fig:coryf2}e and~\ref{fig:coryf2}f show a low detail model of 2,644 triangles generated using the approach in Chapter~\ref{ch:floorplan}, with and without texturing~\cite{Turner14Journal}.

% brief overview of different modeling techniques presented
One of the primary challenges of indoor modeling is the sheer size of the input point-clouds.  Scans of single floors of buildings result in point-clouds that contain hundreds of millions of points, often larger than the physical memory in a personal computer.  Man-made geometry is typically composed of planar regions and sharp corners, but many conventional surface reconstruction schemes assume a certain degree of smoothness and result in rounded or blobby output if applied to these models~\cite{Powercrust,OctreeSculpting,Carving,ProgressiveMesh,Poisson,EigencrustShewchuk}.  In addition to large flat regions, building interiors also contain many small details, such as furniture.  A surface reconstruction scheme must be able to represent the large surfaces in a building with an efficient number of elements and preserve their sharp features.  The fine details of furniture are useful for some applications whereas others require furniture to be removed.  In this report, we discuss existing modeling techniques that remove fine details and those that preserve these details.  The output of both of these modeling techniques can be texture-mapped with captured camera imagery~\cite{Cheng14}.  An example of texture-mapping these two modeling processes is shown in Figs.~\ref{fig:coryf2}c and~\ref{fig:coryf2}f.

\section{Surface Reconstruction}
\label{sec:surf_recon_background}

% background on surface reconstruction techniques
This section describes common techniques for surface reconstruction from input point-cloud data.  These techniques are useful for generating meshes of objects scanned with table-top scanning systems.

% powercrust and the need for noise-reduction
Powercrust is a volumetric surface reconstruction approach that yields watertight models from point-cloud data~\cite{Powercrust}.  It computes the interior median axis of a shape using the voronoi diagram of the input scan points.  By taking the union of the inner polar balls, the elements of the Delaunay Triangulation that form the interior of the shape can be found, and are used to approximate the modeled volume.  While this approach is popular, one weakness is that it is sensitive to noise in the input scans.  Fortunately, other techniques have improved on this methodology to account for noisy input

% Accounting for noise in volumetric scans
An approach to account for the noise in the Powercrust algorithm is to remove sufficiently small polar balls from consideration, which are likely to be due to input noise~\cite{NoisyPowercrust}.  Another approach is to perform spectral clustering on the tetrahedralization of the points~\cite{EigencrustShewchuk}.  This approach, dubbed Eigencrust, allows for substantially improved robustness to noise.  An important consideration, though, is that the noise models tested are randomly positioned outlier points and randomly perturbed input points.  Both of these approaches assume that the randomness is independent and identically distributed from point-to-point.  Such characteristics are not the dominant case when dealing with noise from mis-registration of scans, which is the most common source of noise from mobile scanning systems.

% Isosurface and the reign of SDFs
Another technique used to produce watertight models is to define implicit functions on the scanned volume, and use an isosurface of this volume as the exported mesh.  Once an implicit function is defined, techniques such as Marching Cubes or Dual Contours can be used to mesh the surface~\cite{MarchingCubes,DualContouring}. A popular method for defining this implicit function is to use Signed-Distance Fields~\cite{SignedDistanceFields}.  Another popular approach is Poisson Surface Reconstruction~\cite{Poisson}.  Both of these methods are more robust at mis-registration errors than the previous algorithms.  One downside of their use in the area of modeling man-made objects is that they yield smooth, continuous surfaces.  Such results tend to look overly organic when modeling objects with flat regions or sharp corners.  This issue can be alleviated somewhat by using dual contour isosurface meshing rather than marching cubes, since it preserves sharp edges in the scalar field, but with these methods, the scalar field itself can be overly-smoothed.

% typical hardware set-up for traditional surface reconstruction
An important note is that traditional surface reconstruction techniques, as described above, are written for and tested with common point-cloud test-sets, such as the Stanford Bunny or Dragon.  These models are scanned with precision 3D scanners that yield relatively low noise and minimal mis-registration when compared to scans of larger areas such as buildings.  These approaches are also designed to model isolated objects.  The goal of my thesis is to model buildings, which are scanned at a much larger scale and produce much higher rates of noise and mis-registration.  While watertight volumetric processing is still useful for our applications, different techniques must be applied compared to isolated objects.

\section{Outdoor Building Scanning}
\label{sec:outdoor_scanning}

% motivation for discussing outdoor stuff
When discussing 3D reconstruction of building environments, it is important to note the subfields of both indoor and outdoor building modeling.  Most of this dissertation is specific to indoor building modeling, however many related techniques are used for outdoor modeling as well.

% how to scan outdoors
The current method of outdoor building scanning requires an acquisition vehicle to drive down a street while taking Light Detection And Ranging (LiDAR) scans and panoramic photographs of the surround area~\cite{Zakhor07,Chen07}.  In many applications, it is desirable to generate a triangulated geometry for a collected point-cloud.  This geometry needs to be accurate to the original architecture.  

% pointcloud of building facade
\begin{figure}
	\begin{minipage}[b]{1.0\linewidth}
	  \centering
	  \centerline{\includegraphics[width=0.7\linewidth]{icip2012/rush_pointcloud00}}
	\end{minipage}

	\caption[An example point-cloud of a fa\c{c}ade.]{An example point-cloud of a fa\c{c}ade. Note gaps near windows, ledges, and columns.}
	\label{fig:rush_points}
\end{figure}

For buildings of an appreciable height, the street-level LiDAR scans are taken at a very acute angle with respect to the surface of the building.  Any protrusions from the building cause shadows in the LiDAR and therefore the building is not collected in its entirety, as shown in Figure~\ref{fig:truck_lidar}.  For many applications, a typical solution would be to gather scans from multiple angles \cite{Tang10, Curless96}.  For the urban reconstruction problem, this tactic cannot be used since the scanner is restricted to street-level.

\begin{figure}[t]

% figure:  how to scan outdoors
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[height=4.5cm]{icip2012/truck_lidar}}
\end{minipage}

\caption[Scanning building fa\c{c}ades from a vehicle.]{Locations on the building A, B, C are occluded from the LiDAR collector on the vehicle.}
\label{fig:truck_lidar}

\end{figure}

% background on outdoor modeling
Previous attempts to model buildings have assumed that architecture takes a simple polyhedron geometry. This approach ignores minor details of a building fa\c{c}ade, such as windows and ledges when applied on a large scale \cite{Chauve10, Chen07}.  The goal of this paper is to preserve as much detail as possible in the reconstructed model geometry.  Any interpolation must preserve the sharp edges of corners that occur within these holes.  This requirement diverges from the typical assumptions of most surface completion methods, which usually result in smoothed surfaces \cite{Kazhdan06, Kawai11}.  While developing 3D models with sharp features is a well-explored topic, most current approaches require a sufficient density of points near regions of high curvature, which would not be applicable here \cite{Bernardini04, Mhatre06}.  

% our method
Our goal is to generate sharp, axis-aligned, planar features in locations where the point-cloud is sparse or not sampled at all.  We propose an approach for rectilinear surface reconstruction of building fa\c{c}ades with incomplete 3D point-clouds~\cite{Turner12outdoor}.  Specifically, our proposed algorithm generates geometry for any holes in the point-cloud while preserving flat planes and sharp corners, which are common in modern architecture.  In order to reconstruct a mesh of a building face, first we determine the dominant plane of this fa\c{c}ade, then by treating the original points as a height map on this plane, we uniformly resample these heights over the entire surface using Moving Least-Squares (MLS) smoothing in order to mitigate noise in areas of high sampling, interpolate the areas corresponding to gaps in the point-cloud, and guarantee uniformity of resulting geometry~\cite{Nealen04}.

% Some results
\begin{figure}

\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[width=0.8\linewidth]{icip2012/set2-curve-points00}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
  \centering
  \centerline{\includegraphics[width=0.8\linewidth]{icip2012/set2-curve-screenshot}}
  \centerline{(b)}\medskip
\end{minipage}
%
\caption[Surface reconstruction of building fa\c{c}ade.]{The curvature of the building is preserved when extrapolated; (a) building point-cloud; (b) MLS triangulation.}
\label{fig:extrapolate}
%
\end{figure}

This method results in flat strips of extrapolated fa\c{c}ade.  If the building curves outward or has other non-planar characteristics, these strips conform to the shape of the building, as shown in Figure~\ref{fig:extrapolate}.  Once a geometry has been processed, a texture is applied using panoramic photographs collected from the same location as the LiDAR.  Since these images are projected onto the geometry, any irregularities in the geometry result in dramatic disturbances in texturing.  Examples of this texturing process are shown in Figure~\ref{fig:icip2012_texture}.

\begin{figure}[t]

\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face4_mls}}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face4_texture}}
  \centerline{(b)}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face7_mls}}
  \centerline{(c)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=6.0cm]{icip2012/set1-face7_texture}}
  \centerline{(d)}\medskip
\end{minipage}
\caption[Meshes of building fa\c{c}ades with texture.]{Results of texturing geometry after sharp hole-filling; (a) sampling of flat fa\c{c}ade; (b) with texture; (c) sampling of uneven fa\c{c}ade; (d) with texture.}
\label{fig:icip2012_texture}
\end{figure}

While the techniques used for surface reconstruction of exterior building models are different than those used for interior models, both methods require the efficient processing of large amounts of scan data in the form of point clouds.  Such point clouds may represent noisy or inconsistent geometry, due to being acquired via a mobile scanning system.

\section{Indoor Building Scanning Systems}
\label{sec:indoor_scanning}

% building information models
In the architecture community, there is a continuing push for use of a consolidated Building Information Model file format~\cite{AutodeskBIM}.  Such models need to be semantically-rich for all phases of building development, including architectural, structural, mechanical, electrical, plumbing, etc.  Traditionally, each separate aspect needed to be remodeled from scratch, by hand, which introduced errors into the modeling process.

It is important to distinguish between models that are as-designed compared to as-built.  Each phase of building development will produce a design, but it is still important to compare this design to the as-is nature of the building.  3D scanning of building environments is an important technology to be able to compare the result of construction to what was desired.  With current technology, there is still a fair amount of manual intervention that is  required to convert a 3D scan of a building into a suitable BIM file that can be used for such a comparison.

% competing systems
Traditional industry-standard building scanning uses static scanners.  Such scanners are mounted on tripods, and moved from area to area in the building~\cite{RoomSegmentation,HistWallRecon,BasicPlaneFit}. This scanning process is labor intensive and slow, but results in highly accurate point clouds after stitching.  In order to automate indoor scanning, many mobile systems have been introduced.  Wheeled platforms that carry scanning equipment and are manually pushed through the environment are popular~\cite{Carving, ProbabilisticRobotics}.  Mobility of such systems is limited, since they are unable to traverse rough terrain or stairs easily.  Others have investigated mounting laser range finders on unmanned aerial vehicles~\cite{Quadrotor,QuadrotorMIT}.  Such platforms are agile in that they can scan difficult-to-reach areas.  Such unmanned platforms are limited by short battery life and cannot scan for long durations.

% include system hardware description
% our system and WHAT WE DO DIFFERENT AND WHY IT IS BETTER!!!!!!!!!
In this dissertation, we focus on ambulatory scanning platforms, where the sensor suite is carried by a human operator as the operator moves through the building environment~\cite{Sweep,MITBackpack,VillageHeritage}.  These systems allow for rapid data acquisition and can be actively scanning for several hours at a time.  They use 2D LiDAR scanners due to the cost and weight of full 3D laser range finders.  The captured scans are used both to reconstruct the geometry of the environment and to localize the system in the environment over time.  The datasets shown in this paper were generated by a backpack-mounted system that uses 2D LiDAR scanners to estimate the 3D path of the system over time as well as multiple scanners to generate geometry for the environment~\cite{liu2010indoor,Backpack,Localization,NickJournal}.  This system also has multiple cameras collecting imagery during the data acquisition process, which allows for scanned points to be colored or for generated meshes to be textured with realistic imagery~\cite{Cheng14}. 

% This section describes building meshing techniques
\section{3D Modeling of Building Environments}
\label{sec:building_meshing}

% give a brief overview of the different types of meshing techniques
% Discuss how the below techniques typically use noise-less input from
% static scanners, and do not explicitly detail with misregistration or
% noise.
The primary thesis of this dissertation regards surface reconstruction of scan data of indoor building environments, regardless of what hardware systems are used to collect building scan data.  There exists an emerging field of techniques used to model different aspects of building geometry from captured scans.  These techniques can be classified into three main categories:  Floor-Plan Generation, Simplified 3D Modeling, and Detailed 3D Modeling.  Floor-plan generation focuses on estimating 2D positions of walls in the building.  Simplified 3D modeling similarly focuses on 3D modeling only the permanent features of a building: floors, walls, and ceilings.  Detailed 3D modeling focuses on modeling all aspects of the scanned geometry, including fine details such as furniture or objects observed in the building.  Note that most of the approaches discussed here were developed to be applied to static scans of buildings, which have very low noise and capture high detail. The focus of this dissertation is to develop modeling techniques for mobile scanning systems, which are much more likely to suffer from mis-registration noise or missing geometry.

In this section, we discuss existing techniques to generate each of these types of building models.  This dissertation also describes novel work we contributed, in Chapters~\ref{ch:floorplan},~\ref{ch:carving},~\ref{ch:better_floorplan}, and~\ref{ch:better_carving}.

% This subsubsection describes floor plan generation techniques
\subsection{Floor-Plan Modeling}
\label{ssec:background_floorplan}

Floor-plan modeling techniques are based on the idea of sampling positional information of walls within the environment captured by the scans, then using these wall samples to generate a plan composed of line segments or polygons.  The work of Weiss et al use a cart-based system with a horizontal laser scanner~\cite{Weiss05}.  The output scan points are exactly the sample positions of the walls.  The find lines in this scan map with a Hough Transform, which represent walls.  Okorn et al employ a similar method, but their input scans represent a full 3D point-cloud~\cite{Okorn09}.  The wall sample positions are found by computing a top-down histogram of the input points, and areas of high density are classified as vertical surfaces.  The approaches we discuss in Chapter~\ref{ch:floorplan} employ a similar top-down histogram.  Lastly, Mura et al's paper takes a different approach to estimating wall positions~\cite{Mura13}.  They perform region growing in the 3D point-cloud to fine planar regions, which are projected into 2D and treated as potential wall candidates.  A cell complex is then built to volumetrically identify separate rooms in the model.  This approach is the second publication that performs automatic room partitioning, where the first is our approach as discussed in Chapter~\ref{sec:room_label}~\cite{Turner14}.  There are also methods that take a floor plan as input, and use this information to generate a 3D model of the environment by extruding the defined wall information~\cite{Or05,Lewis98}.  This type of modeling yields aesthetically pleasing results with well-defined floors, walls, and ceilings.

% This subsubsection describes plane fitting for 3D modeling
\subsection{Simplified 3D Modeling}
\label{ssec:background_planefit}

Since building features are almost entirely planar, a popular approach is to explicitly fit planar elements to the input point-clouds.  Sanchez and Zakhor use PCA plane-fitting to find floors, walls, and ceilings explicitly in the point-cloud~\cite{Victors}. Since this method was applied to ambulatory data, it resulted in missing components, holes, and double-surfacing due to mis-registration.  Xiong et al also perform plane-fitting in a similar fashion, but also analyze the computed planes for the locations of windows and doors, which are represented as holes in the surface~\cite{Xiong13}.  Adan et al find planes by first generating a floor-plan, then extrude the floor-plan into a 3D model~\cite{WallFinder}.  One limitation with these methods is that they are not necessarily watertight, though floor plan extrusion can also be done in a watertight fashion~\cite{Mura14,Turner14,Cabral14}.  These approaches allow for accurate wall geometry and reduce the complexity of the output model. Extruded floor plans also allow for models to explicitly define floors, walls, and ceiling surfaces.  

Other approaches have focused to perform volumetric processing to ensure watertightness when computing simplified 3D models.  Xiao et al find horizontal cross-sections of the building, forming a sequence of 2D CSG models that are then stacked together and simplified~\cite{Museums}.  While this approach does lead to aesthetically-pleasing models, it assumes manhattan-world models, which leads to topological errors if an insufficient number of cross-sections are recovered.  Oseau et al use a voxelization approach, with a follow-up graph-cut step to remove small details in the environment, leaving only floors, walls, and ceilings~\cite{Oesau13}.  This optimization step, however, can also cause significant deformations in the final geometry depending on the input parameters. 

% This subsubsection describes dense 3D modeling
\subsection{Detailed 3D Modeling}
\label{ssec:background_3dmodeling}

One of the methods used to preserve the detail of furniture in a building scan is to explicitly search and classify for furniture models in the scan.  These techniques attempt to find locations in the input scans that match best with a stored database of known furniture.  The pre-existing model of the recognized piece of furniture is then oriented in the output model.  Nan et al employ this technique, with explicit classification of chairs and tables~\cite{SearchClassifyPointcloud}.  Kim et al also use this method, using a larger library of objects and operating on noisier scans~\cite{Kim12}.  They also discuss how search-classification can allow for change detection across scans of the same area taken at different times.  A major downside of this method is that the classification is only as good as the database.  Objects that are in unexpected orientations or are not in the database are misclassified.  For instance, a sideways chair is misclassified as a table.  One major benefit of this approach is the ability to model the objects independently of the room itself, as discussed later in Chapter~\ref{ch:better_carving}.  

% object/room detection in buildings
Recently, object detection methods from indoor scans have been proposed without the use of a training dataset~\cite{Mattausch14}.  They segment point clouds using a bottom-up approach to fit rectangular patches on to the scans, then find clusters of patches that are repeated often.  Even though this approach can be applied to large datasets, it assumes very basic building geometry in order to segment objects.  Additionally, it only detects objects of certain complexity, is unable to detect very small objects, and requires objects to be repeated often in the environment.  We expand on this work by segmenting objects volumetrically, rather than in the point cloud domain.

% general carving
There are also methods that capture fine detail of buildings by attempting to be as accurate to the input point-cloud as possible.  Holenstein et al generate a space-carving model that voxelizes the scanned environment, labeling any voxels intersected by a scan-line line to be interior~\cite{Carving}.  The boundary of the interior voxels is meshed with Marching Cubes.  The advantage of this approach is an increased robustness to mis-registration errors, but the downside is that over-carving can result in the loss of fine detail.  Lastly, a popular approach to detailed modeling of environments is Kinect Fusion~\cite{KinectFusion,Kintinuous}.  This method allows for both meshing and tracking, but is limited in that it cannot handle large areas, and the actual meshing approach is a minor extension of Signed-Distance Fields~\cite{SignedDistanceFields}. 

% this section outlines the remainder of the paper
\section{Contributions and Organization of Dissertation}
\label{sec:organization}

% organize paper
In this dissertation, we present several techniques used to automatically generate virtual models of indoor building environments.  The interior environment of a building is scanned using our custom hardware system, which provides a set of data products used to develop these models.  Our modeling technqiues can be separated into two categories:  2D floor plan models and dense 3D models.  All approaches are produced automatically from the output data of our scanning system.  These approaches are intended to be agnostic to the specific hardware used for scanning and in many cases have been applied to scans taken with other hardware systems.  Unless stated otherwise, however, any examples shown in this dissertation were generated from scans acquired using our hardware system.

First, we discuss the specifics of our hardware system in Chapter~\ref{ch:hardware}.  This system is a collection of laser scanners and other sensors, worn as a backpack by a human operator.  The operator traverses the environment at normal walking speed, allowing the system to collect data about the interior area.  We discuss specifics on the mechanics of our backpack system, how the data are logged, and special considerations needed to ensure accuracy of the resulting scans.

Next, in Chapter~\ref{ch:preprocessing}, we discuss preprocessing steps required once a data acquisition is performed.  Specifically, these steps detail how the system is localized while traversing through a GPS-denied environment and how we ensure a consistent global coordinate system.  We also discuss the first modeling technique, which is to generate a point cloud of the environment.  This point cloud can be colored using the camera imagery.  Additionally, these scans are used to determine the number of building levels covered during the acquisition, and the partitioning of those levels.

In Chapter~\ref{ch:floorplan}, we detail the techniques developed to generate 2D floor plans of the interior building environment.  These techniques use the 3D scan information to generate a consolidated 2D estimate of wall positions.  These wall position estimates are used to develop a volumetric floor plan of each level in the building environment.  We show multiple floor plan generation techniques, including methods to automatically detect and classify separate rooms in the environment.  Once a 2D floor plan is produced, its geometry can be extruded into a simplified 2.5D model, which includes height information.

In Chapter~\ref{ch:carving}, we denote multiple methods used to generate complex, fully-3D models of building environments.  We compare the complexity of these models to those generated by extruding 2D floor plans, as well as how the detail of the environment's geometry can be accurately preserved.

In Chapters~\ref{ch:better_floorplans} and~\ref{ch:better_carving}, we discuss how these two types of methods -- 2D floor plan generation and 3D dense modeling -- can be combined to provide additional analysis on building modeling.  This combination of techniques can be used to not only augment each individual model output to improve accuracy, but also provide new information about the environment.  Chapter~\ref{ch:better_floorplans} discuss how the 2D floor plan generation methods can be improved by using the 3D dense modeling results, and Chapter~\ref{ch:better_carving} iterates how the 3D dense modeling is improved by incorporating the 2D floor plan output.

In Chapter~\ref{ch:applications}, we show the applications of these techniques, and how building models at different granularity are required based on how the model is to be used.  We show several examples of use-cases for these modeling techniques.

Lastly, in Chapter~\ref{ch:conclusion}, we offer concluding remarks and directions for future work.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Hardware Description
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Hardware Description}
\label{ch:hardware}

The surface reconstruction algorithms described in this dissertation can be applied on any indoor scanning data.  For practicality purposes, we have developed our own scanning system, and all examples shown in this document are generated with our custom hardware (unless otherwise stated).

We developed a human-mounted ambulatory scanning system, worn as a backpack.  As the human operator walks through the building environment, at normal speed, this system collects data from a variety of sensors on-board.  These data products are logged and then processed offline.  Our developed software can accurately estimate the trajectory of the system over time and localize the system~\cite{NickJournal}.  This procedure allows us to rapidly move through a large environment, spending only a few seconds in each room yet capturing full geometry information.

\section{Mechanical Specification}
\label{sec:mechanical}

% this figure shows side-by-side backpacks
\begin{figure}

	\centering
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_1}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_2}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_3}}
		\centerline{(c)}\medskip
	\end{minipage}	

	\caption[The three backpack scanning systems developed by our lab.]{The three backpack scanning systems developed by our lab:  (a) First generation, weighing 80 pounds; (b) second generation, weighing 32.5 pounds, developed in 2013; (c) third generation, weighing 35 pounds, developed in 2014.}
	\label{fig:all_backpacks}

\end{figure}

% different backpack models
As shown in Figure~\ref{fig:all_backpacks}, our lab has developed three generations of the backpack hardware.  The first generation system, shown in Figure~\ref{fig:all_backpacks}a, was built as a prototype system~\cite{Backpack}.  It contains five Hokuyo laser scanners, two cameras, and an Internal Measurement Unit (IMU).  Since 2012, it has been augmented with WiFi antennas and infrared cameras.  The WiFi antennas are used to record signal-strength of nearby access points, which is useful for indoor localization techniques~\cite{Levchev14}.  The infrared cameras are useful for energy-efficiency analysis of building environments.  These data products are discussed further in Chapter~\ref{ch:applications}.

The second generation backpack, as shown in Figure~\ref{fig:all_backpacks}b, was constructed in 2013.  It contains all sensors available on the first backpack, except for the infrared cameras.  It also has a barometer and a set of 3D magnetometers.  The main modification of the second generation was to reduce the size.  The first generation is about 80 pounds, whereas the second generation is only 32.5 pounds.  The third generation, shown in Figure~\ref{fig:all_backpacks}c, backpack is almost identical to the second generation, but with the addition of infrared cameras.

\section{Sensor Characteristics}
\label{sec:sensor_specs}

% figure of backpack hardware
\begin{figure}[t]

	\begin{minipage}[c]{0.54\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{procarve/hardware/backpack_annotated}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.46\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/backpack_coord_system}}
		\centerline{(b)}\medskip
	\end{minipage}

	\caption[Annotated scanning hardware system.]{Annotated scanning hardware system, worn as a backpack.  The operator walks through the indoor building area as the system scans the surroundings.  Our method combines the LiDAR and inertial measurements in order to form a virtual 3D model for the observed space:  (a) the system worn by operator; (b) the common coordinate frame of backpack system.}
	\label{fig:backpack}
\end{figure}

% briefly describe the type of scans we use as input
Our geometry reconstruction algorithms use the retrieved laser range data as input.  These scans are taken with time-of-flight laser scanners that capture 3D geometry of the environment.  We use multiple 2D laser scanners mounted at different orientations to capture all observed geometry in the environment, as indicated in Figure~\ref{fig:backpack}a.  Since our system is mobile and ambulatory, the resulting point clouds have higher noise than traditional static scanners due to natural variability in human gait.  As such, our approach needs to be robust to motion induced scan-level noise and its associated artifacts.  We use these scans to form watertight models of the indoor building environment and the contained objects.

% sparse but fast
The primary advantage of a backpack system is speed of acquisition.  We use 2D Time-Of-Flight (TOF) laser scanners, which means we have sparser scan information per frame and traverse each area of the environment much faster than when scanning the equivalent area with a Kinect or Google Tango.  Since our system produces much sparser point clouds than Kinect-based scanning, it is not feasible to employ the same averaging techniques that allow for high-quality Kinect scans.  The advantage of these 2D TOF sensors is that they are less noisy, have a longer range, and a wider field of view.  The result is that much larger environments can be covered in significantly less time when using these sensors in a sweeping motion~\cite{Sweep}.  For instance, a backpack-mounted system allows an unskilled operator to rapidly walk through an environment to acquire data.

% the noise characteristics
Our system uses Hokuyo UTM-30LX sensors, whose intrinsic noise characterization is given in~\cite{Pomerleau12}.  Typically this noise contributes on the order of 1 to 2 centimeters to the standard deviation of the positional estimate of scan points.  This uncertainty value increases as the range of the point increases, with accurate measurements stopping at a range of $30$~meters.  We also must consider input noise from our cameras.  We use three 12-megapixel cameras with fisheye lenses, in order to collect as much of the surrounding scenery as possible.  Due to this set-up, a major factor in sensor calibration is how long of an exposure to use for each image.  Since we may be scanning in dark or narrow areas of building environments, ensuring that enough light hits the sensor is challenging, especially with fisheye lenses.  However, if we set the exposure too long, then the imagery is susceptible to motion blur, since the operator often walks or turns quickly.

% data rate from cameras, scalability of systems.
Since we collect 12~MP imagery from three cameras at 2 Hz each, the camera images represent the major of data flow during the data acquisition process, at 72~megabytes per second.  To ensure no data loss, we necessitate a fast internal drive on the backpack system.  We have also configured the system with a USB-3 external drive, with twin SSD drives in a RAID-0 configuration.  This set-up allows us to write data directly to the external drive, which can be hot-swapped for fast processing.  On the second generation system, this storage drive is 500~GB, which allows for just under two hours of consecutive scanning.  The third generation system contains a 1~TB drive, which allows for up to four hours of scanning.  At that rate, we can perform data acquisition of over 100,000 square feet of building space on a single drive.

% also discuss extrinsic calibration of sensors
Since our hardware system contains may separate sensors, it is necessary to create a common system coordinate frame, as shown in Figure~\ref{fig:backpack}b.  Each sensor has an independent coordinate frame, but they are extrinsically calibrated against one another, in order to establish the rotation and translation between any given sensor and the common coordinate frame.  Once this extrinsic calibration process is performed, we can determine the orientation of any given sensor with respect to the common system.  As discussed in Chapter~\ref{sec:localization}, after a data collection we can determine the orientation of the system within a model coordinate frame.  The combintation of the transform generated by extrinsic sensor calibration and the localization process allows for sensor readings to be placed in model coordinates.

% discuss time synchronization, both during collection and processing
In addition to geometric calibration of the individual sensors, we must also consider temporal calibration of the sensor readings to a common clock.  Since each sensor operates independely on a separate clock, it is important to compute the transformation from a given sensor's time frame to that of the common system.  For this computation, we make the assumption that there is an affine transform between any two clocks.  Such an assumption is reasonable, since two clocks can naturally have a phase offset based on when they were initialized, as well as a scalar offset due to factors such as running temperature and age.  Timestamp synchronization is acheived by recording two timestamps for every scan frame of each sensor, one from the sensor clock and one from the system computer.  This process must be done in software since not all sensors have the capacity for hardware triggering.  The linear fit between clocks is computed and typically represents a fitting error of around $2$ to $7$~milliseconds standard deviations.

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Preprocessing and Conventions
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Preprocessing and Conventions}
\label{ch:preprocessing}

This chapter details processing steps that are required before any model generation can occur.  All approaches discussed in this dissertation require a reconstructed path of how the system moved through the environment.  Such a path trajectory is required in order to estimate the position of each scanner at any given time, so that the geometry readings can be placed in global coordinates.  The details of how the localization data are used are described in Sections~\ref{sec:localization} and~\ref{sec:align_path}.

Some methods discussed in this dissertation also require a consolidated 3D point cloud of the scanned environment.  Unlike most geometry products discussed, a point cloud contains no topology information.  Instead, a point cloud is a concatenation of the raw laser range data, transformed into the global coordinate frame.  Specifics on how these values are generated and what conventions are used are discussed in Section~\ref{sec:pointcloud}.

\section{Localization}
\label{sec:localization}

% Nick's localization procedure
Before models of the building geometry can be generated, we must first determine how the operator traversed the environment.  Indoor building environments are typically GPS-denied areas, so local observations are required in order to track the path walked~\cite{Backpack,Localization,NickJournal}.  The process of recovering the 3D trajectory of the system is especially difficult for our backpack hardware, since a human operator can experience six degrees of freedom in movement ($x$, $y$, $z$, $\theta$, $\phi$, $\psi$), whereas the sensors used in the system can only observe a 2D slice of the environment.  As such, multiple laser scanners are mounted at different orientations on the backpack hardware to ensure that full 3D movement can be observed~\cite{NickJournal}.  These incremental changes in pose are refined by a global graph optimization step that constrains the system path whenever an area is revisited~\cite{toro07}.  These revisits are automatically detected and constrained in order to produce an accurate 2D trajectory of the system.  The elevation of the system is computed using a similar technique with a vertically mounted scanner~\cite{Backpack}.

% coordinate systems used:  sensor, backpack, model
The localization procedure is complete, we have full mapping between three coordinate systems for each recorded frame for each hardware sensor.  The internal coordinate frame of the sensor, which is specific to the make and model of the sensor hardware, can be converted to the system common coordinate frame based on the extrinsic calibration of each sensor's rigid position and orientation on the backpack system, which is given by the coordinate frame shown in Figure~\ref{fig:backpack}b.  This rigid affine transformation for a given sensor $s$ is represented by the $3 \times 3$ rotation matrix $R_{s\rightarrow c}$ and the $3 \times 1$ translation vector $T_{s\rightarrow c}$.  The localization procedure provides an output path of the system, so that each pose of the system can be transformed from the system common coordinate frame to the world coordinate frame.  At a given time $t$, the affine transformation from the system common coordinates to the model coordinates is given by $3 \times 3$ rotation matrix $R_{c\rightarrow m}(t)$ and $3 \times 1$ translation vector $T_{c\rightarrow m}(t)$.  With these transformations, any sensor reading can be placed in global coordinates.

% discuss output model frame of localization, and units
The localization output path provides the output model coordinate frame in a locally consistent coordinate frame based on the starting position of the operator during a scan.  This starting position is used as the origin point of the model.  By convention, the heading vector of the first pose is used as the $+X$-direction of the model coordinate frame.  The direction of gravity is used as the $-Z$-direction of the model coordinate frame.  The $Y$-direction of the model coordinate frame is chosen such that the coordinate system is right-handed.  Although this procedure produces a common set of coordinates for the model, we apply an additional step in order to align the model with the global coordinate frame, as discussed in Section~\ref{sec:align_path}.  Once this step is complete, the model uses East-North-Up (ENU) coordinates.  For the rest of this dissertation, all output models are expressed in units of meters.

% show figure of double surfacing.
\begin{figure}
	\centerline{\includegraphics[width=0.7\linewidth]{quals/misregistration_example}}
	\caption{An example of misregistration or ``double surfacing'' in a point cloud.}
	\label{fig:double_surface}
\end{figure}

% noise models of localization output (e.g. double-surfacing)
Since the localization procedure used is performed in an integrative fashion, the expected error in the reported position and orientation of the system at any pose is higher than in competing technologies, such as static scan systems~\cite{NickJournal}.  Static systems often utilize markers or control points within the environment to ensure manual alignment of scans, allowing for positional accuracy of within $1~mm$~\cite{Li97,Karimi00}.  By contrast, the mean positional accuracy of the output poses in the backpack localization procedure is typically $10~cm$.  Additionally, the errors in successive poses are highly correlated.  Typically a local set of scan points captured within a few seconds of one another are highly accurate, yet if the same features in the environment are scanned twice with a long interval between scans, then the scans are likely to be mis-matched.  This behavior leads to the presence of ``double-surfacing'' in the output scans, which can be observed in the generated point cloud of the environment.  Figure~\ref{fig:double_surface} shows an example of such misregistration.  The chair in the depicted point cloud is represented by two copies, slightly offset from one another.  Note that the point cloud shown is generated using a static scanning system, which can also suffer from misregistration issues.

In order to provide consistent output geometry, the modeling techniques described in this dissertation are designed to prevent such ``double-surfacing'' from occurring in the output meshes.  This trait is accomplished by performing volumetric analysis of the input scans, as discussed later in this document. 

\section{Aligning Model Coordinates}
\label{sec:align_path}

% show color pointcloud compared to camera image
\begin{figure}
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{align_path/compass_readings}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{align_path/aligned_path}}
		\centerline{(b)}
	\end{minipage}

	\caption[Aligning model coordinate system with North.]{An example of aligning model coordinates to magnetic north:  (a) individual compass estimates of North (red) are compiled from each pose of the path (blue) and represented in world coordinates; (b) the model coordinates are transformed so that the mean estimate of magnetic North across all readings is oriented in the $+Y$-direction.}
	\label{fig:align_path}
\end{figure}

As discussed in Section~\ref{sec:localization}, the system path generated by the localization procedure is initially oriented using the heading vector of the first pose.  In order to keep the orientation of model coordinate frames consistent between separate scans of the same environment, and to ensure that output geometry is easily recognizable, we perform an additional orientation step that aligns the path to the East-North-Up (ENU) coordinate frame.

One of the on-board sensors of the backpack hardware system is a 3D magnetometer, which is built into the Inertial Measurement Unit (IMU).  Individual readings from a 3D compass are typically noisy when indoors, due to nearby ferrous building elements that interfere with hard and soft iron calibration~\cite{Caruso00,Guo08}.  As such, these readings are not reliable during the localization process.  Once the final, consistent path is generated we are able to generate a least-squares estimate of compass north and reorient the path so that North is aligned with the $+Y$-direction.

First, the 3D magnetometer reading is recorded at each pose during the data acquisition.  As shown in the example in Figure~\ref{fig:align_path}a, these vector readings represent the magnetic field at that location.  The direction of each reading is used as a noisy estimate of the direction of magnetic South.  The magnetic field flows South, so the reverse direction, as shown in Figure~\ref{fig:align_path}a, is an estimate of North.  Let $\vec{m}_i \in \mathbb{R}^3$ be the magnetometer reading at pose $i$, in the coordinate frame of the magnetometer sensor $\texttt{mag}$.  The rotation required to orient the model coordinate frame to ENU coordinates is given by:

\begin{equation}
	\label{eq:align_path}
	\texttt{argmin}_{\theta \in [0,2\pi]} \; \sum \limits_{i} 
		\; \left| \left( R_{z}(\theta) \, R_{c\rightarrow m} (t_i) \,
			R_{\texttt{mag} \rightarrow c} \, m_i \right) - \left[ 
				\begin{array}{c} 0 \\ -1 \\ 0 \end{array} 
					\right] \right|^2
\end{equation}

where $t_i$ is the timestamp of pose $i$, and $R_{z}(\theta)$ is the rotation matrix:

\begin{equation}
	\label{eq:rotation_matrix}
	R_{z}(\theta) = \left[ \begin{array}{ccc}
				\texttt{cos}(\theta) & -\texttt{sin}(\theta) & 0 \\
				\texttt{sin}(\theta) & \texttt{cos}(\theta) & 0 \\
				0 & 0 & 1 \end{array} \right]
\end{equation}

This optimization rigidly rotates the model coordinate frame about the $z$-axis in order to best align all magnetometer readings with the direction we wish to denote south:  the $-Y$-direction.  An example result of this procedure is shown in Figure~\ref{fig:align_path}b.  This procedure ensures that all output models are aligned with magnetic North, in ENU coordinates.  While not applied in the current configuration, it is possible to also align the model to true North if given the latitude and longitude coordinates of the scanning location, by performing a look-up of magnetic declination~\cite{MagDec}.

\section{Point Cloud Generation}
\label{sec:pointcloud}

When a finalized path and coordinate frame is produced, we can immediately generate a point cloud of the raw scans taken in the environment.  This point cloud allows visualization of observed building geometry.  Each point can be computed independently by transforming its position from its sensor frame to the model coordinate frame.  An example point cloud of an indoor environment is shown in Figure~\ref{fig:pointcloud_nocolor}.  This scene shows a corner of a room, with a lamp to the left of a TV on a table.

% show example pointcloud
\begin{figure}
	\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_nocolor}}
	\caption{An example point cloud of an indoor building environment.}
	\label{fig:pointcloud_nocolor}
\end{figure}

We also have the capability to export point clouds in color.  The coloration is taken from camera imagery taken temporally close to when the range points were acquired.  For a given point $\vec{x}$ taken at time $t$, the point color is determined by querying all available cameras for any images within $\Delta t$ seconds of time $t$.  Typically, we use a window size of $\Delta t = 2$~seconds.  For each reported camera, we project the position $\vec{x}$ onto the camera image plane, and determine the pixel color at that position.  This projection is demonstrated in Figure~\ref{fig:pointcloud_color_diagram}.  In addition to color, we also compute a quality measure $q_x = (\vec{x} - \vec{c})^T \vec{n}$, where $\vec{c}$ is the camera position and $\vec{n}$ is the optical axis of the camera.  The quality $q_x$ is $1.0$ if $\vec{x}$ projects into the center of the image, and decreases as the projected location moves away from the center of image.  For all images within the time window $[t - \Delta t, t + \Delta t]$, the pixel color associated with the highest quality meaure is used to color the point.

The resulting point cloud is colored based on all available imagery in the dataset.  Figure~\ref{fig:pointcloud_color}a shows a camera image of an example seen and Figure~\ref{fig:pointcloud_color}b shows the colored point cloud of the same environment.

% figure showing projection of points onto images
\begin{figure}
	\centerline{\includegraphics[width=0.5\linewidth]{pointcloud/gradlounge/pointcloud_color_diagram}}
	\caption{Determining the color of point $x$ based on camera imagery.}
	\label{fig:pointcloud_color_diagram}
\end{figure}

% show color pointcloud compared to camera image
\begin{figure}
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/camera}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_color}}
		\centerline{(b)}
	\end{minipage}

	\caption{An example point cloud colored by nearby camera imagery.}
	\label{fig:pointcloud_color}
\end{figure}

% partitioning pointclouds into separate levels
\section{Partitioning Point Cloud by Building Levels}
\label{sec:pointcloud_level_split}

Many methods described in this dissertation require explicit detection and partitioning of the different levels, or stories, in a scanned building.  Since our hardware system is human-mounted, the operator can walk up and down stairs during the data acquistion process, facilitating scans of multiple stories at once.  

Some localization systems that rely on 2D grid maps of wall samples are capable of detecting when the operator moves from one level to another and automatically partition their output grid maps accordingly~\cite{MITBackpack}.  We do not just want the elevation of each level, but the vertical extent as well.  In order to detect and separate building levels, we identify the primary floor and ceiling surfaces for each level.  This identification can be done either in the point cloud domain or after a full model has been generated.  The latter method is discussed in Chapter~\ref{ch:better_floorplans}.  Here, we discuss how level splitting can be accomplished via point clouds, which offers a faster if less accurate partitioning.

A histogram approach can be used to separate the point cloud by levels~\cite{Turner12,Turner14Journal}. Figure~\ref{fig:heighthist}a shows an example point cloud, colored by height, which contains multiple levels.  By computing a histogram along the vertical-axis of the point-cloud, it is possible to find heights with high point density, which indicates the presence of a large horizontal surface.  An example of this process is shown in Figure~\ref{fig:heighthist}b, where peaks in the histogram correspond to the floors and ceilings of each scanned level in the building.  Points scanned from above, in a downward direction, are used to populate a histogram to estimate the position of each floor, and the histogram used to estimate the position of each ceiling is populated by points scanned from below.  The local maxima of these two histograms show locations of likely candidates for floor and ceiling positions, which are used to estimate the number of scanned levels and the vertical extent of each level.  The candidate floor and ceiling heights are pruned by first taking the lowest floor maxima as the elevation of the first level's floor.  The first level's ceiling elevation is determined by the most populated ceiling maxima position that resides below the next maximal floor elevation.  This process is repeated for all levels, which allows for detection of both number of levels and their range extents.  Once levels are separated, they can be processed and analyzed separately.  Figure~\ref{fig:heighthist}c shows the final extruded mesh with all three scanned levels.

% show example histogram and partitioning
\begin{figure}

	\centerline{\begin{minipage}[c]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/points_LEFT}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.35\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/hist}}
		\centerline{(b)}\medskip
	\end{minipage}}
	\begin{minipage}[c]{0.9\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/mesh}}
		\centerline{(c)}
	\end{minipage}
	
	\caption[An example point-cloud partitioning by height.]{An example point-cloud partitioning by height: (a) the input point-cloud, showing geometry for three levels; (b) the vertical histogram showing estimates of each building level height; (c) the produced mesh of this building scan using method from Chapter~\ref{ch:floorplan}.}
	\label{fig:heighthist}

\end{figure}



% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
\bibliography{elturner}
\vfill

% that's all folks
\end{document}
