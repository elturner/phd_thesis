%\documentclass[journal]{IEEEtran}
%\documentclass[11pt,draftcls,onecolumn]{IEEEtran}
%\documentclass[10pt,twocolumn,twoside]{report}
\documentclass[12pt,onecolumn,oneside]{book}
\usepackage[margin=1.0in]{geometry}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% *** HEADER/PAGE NUMBERING RELATED PACKAGES ***
\usepackage{blindtext}% adds the blindtext
%\usepackage{showframe}% shows a framed header, type area and footer
\setlength{\headheight}{1.1\baselineskip}
%\setlength{\headsep}{10pt}


% *** GRAPHICS RELATED PACKAGES ***
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{../figures/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpg,.jpeg,.png}

% *** MATH PACKAGES ***
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\interdisplaylinepenalty=2500

% *** PDF, URL AND HYPERLINK PACKAGES ***
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=blue
}

% Force headers and page numbers to always be on the upper right
\usepackage[nouppercase,automark]{scrpage2}
\clearscrheadfoot
\pagestyle{scrheadings}
%See the KOMA-script documentation for more information
\ihead{\rightmark}
%\ihead[]{\headmark}%helpful for two-sided printing
\ofoot[\pagemark]{\pagemark}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% begin document with roman page numbering
\pagenumbering{roman}
\begin{document}

% TODO deal with title and author pages, which have special requirements

\title{Eric's Ph.D. Thesis}

% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Eric~Turner}
%\thanks{This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

% make the title area
\maketitle

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% ABSTRACT
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the abstract uses arabic page numbering and has its own counter
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

% TODO do abstract, which also has special formatting requirements

%\begin{abstract}
This is where the abstract will go.
%\end{abstract}

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% FRONT MATTER:  dedication, table of contents, list of figures, acknowledge
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{roman}
\setcounter{page}{1}

% TODO dedication page

% table of contents
\addcontentsline{toc}{chapter}{Table of Contents}
\tableofcontents{}

% list of figures
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures{}

% list of tables
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listtablename}
\listoftables{}

% TODO acknowledgements

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Introduction
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

% the front matter of the paper uses roman page numbering
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{Introduction}
\label{ch:introduction}

% motivation text
Laser scanning technology is becoming a vital component of building construction and maintenance.  During building construction, laser scanning can be used to record the as-built locations of HVAC and plumbing systems before drywall is installed.  In existing buildings, blueprints are often outdated or missing, especially after several remodelings.  Such scans can be used to generate building models describing the current architecture.  Meshed triangulations allow for the efficient representation of the scanned geometry.  In addition to being useful in the fields of architecture, civil engineering, and construction, these models can be directly applied to virtual walk-throughs of environments, gaming entertainment, augmented reality, indoor navigation, and energy simulation analysis.  These applications rely on the accuracy of a model as well as its compact representation.

% figure with pointcloud and models
\begin{figure}

	% top row
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_photo}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_triangles}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_3d_texture_compressed}}
		\centerline{(c)}\medskip
	\end{minipage}
	
	% bottom row
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_pointcloud_compressed}}
		\centerline{(d)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_triangles}}
		\centerline{(e)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\linewidth}
		\centerline{\includegraphics[height=3.4cm]{quals/joint/coryf2/coryf2_2d_texture_compressed}}
		\centerline{(f)}\medskip
	\end{minipage}
	
	\caption[Models generated with our techniques.]{Models generated with the techniques described in this dissertation:  (a) photograph of scanned area, academic building; (b) surface carving model of this area; (c) surface carving model with texturing; (d) point cloud of captured scans; (e) extruded floor plan model of area; (f) extruded floor plan with texturing.}
	\label{fig:coryf2}

\end{figure}

% applications
Generating an accurate model of indoor environments is an emerging challenge in the fields of architecture and construction for the purposes of verifying as-built compliance to engineering designs~\cite{Bosche10,Xiong13}.  This task is made more challenging by the GPS-deprived nature of these environments~\cite{Liang13}.  Another application that requires an exported mesh to retain as much detail as possible is historical preservation via a virtual reality model~\cite{VillageHeritage,Carving}.  Building energy efficiency simulations can use watertight meshes of the environment to estimate airflow and heat distribution~\cite{EnergyPlus}.  These simulations require simplified meshes as input, since finite element models are difficult to scale.  It is also important to be able to generate an immersive visualization and walk-through of the environment for these applications, so experts can remotely inspect the scanned environment via telepresence, a task that currently requires expensive travel and on-site visits.  Different applications require models of different complexities, both with and without furniture geometry.  The modeling approaches detailed in this report are useful for both types of applications, as shown in the examples in Figure~\ref{fig:coryf2}.  Figure~\ref{fig:coryf2}a is a photograph of the scanned area: the hallways of an academic building, encompassing about 1,000 square meters of scanned area.  Figure~\ref{fig:coryf2}d represents the captured 3D point-cloud of this area.  Figures~\ref{fig:coryf2}b and~\ref{fig:coryf2}c show a high-detail 3D mesh of 2.7 million triangles generated using the algorithm in Chapter~\ref{ch:carving}, with and without texturing, respectively.  Figures~\ref{fig:coryf2}e and~\ref{fig:coryf2}f show a low detail model of 2,644 triangles generated using the approach in Chapter~\ref{ch:floorplan}, with and without texturing.

% brief overview of different modeling techniques presented
One of the primary challenges of indoor modeling is the sheer size of the input point-clouds.  Scans of single floors of buildings result in point-clouds that contain hundreds of millions of points, often larger than the physical memory in a personal computer.  Man-made geometry is typically composed of planar regions and sharp corners, but many conventional surface reconstruction schemes assume a certain degree of smoothness and result in rounded or blobby output if applied to these models~\cite{Powercrust,OctreeSculpting,Carving,ProgressiveMesh,Poisson,Eigencrust}.  In addition to large flat regions, building interiors also contain many small details, such as furniture.  A surface reconstruction scheme must be able to represent the large surfaces in a building with an efficient number of elements and preserve their sharp features.  The fine details of furniture are useful for some applications whereas others require furniture to be removed.  In this report, we discuss existing modeling techniques that remove fine details and those that preserve these details.  The output of both of these modeling techniques can be texture-mapped with captured camera imagery~\cite{Cheng14}.  An example of texture-mapping these two modeling processes is shown in Figs.~\ref{fig:coryf2}c and~\ref{fig:coryf2}f.

\section{Surface Reconstruction}
\label{sec:surf_recon_background}

% background on surface reconstruction techniques
This section describes common techniques for surface reconstruction from input point-cloud data.  These techniques are useful for generating meshes of objects scanned with table-top scanning systems.

% powercrust and the need for noise-reduction
Powercrust is a volumetric surface reconstruction approach that yields watertight models from point-cloud data~\cite{Powercrust}.  It computes the interior median axis of a shape using the voronoi diagram of the input scan points.  By taking the union of the inner polar balls, the elements of the Delaunay Triangulation that form the interior of the shape can be found, and are used to approximate the modeled volume.  While this approach is popular, one weakness is that it is sensitive to noise in the input scans.  Fortunately, other techniques have improved on this methodology to account for noisy input

% Accounting for noise in volumetric scans
An approach to account for the noise in the Powercrust algorithm is to remove sufficiently small polar balls from consideration, which are likely to be due to input noise~\cite{NoisyPowercrust}.  Another approach is to perform spectral clustering on the tetrahedralization of the points~\cite{EigencrustShewchuk}.  This approach, dubbed Eigencrust, allows for substantially improved robustness to noise.  An important consideration, though, is that the noise models tested are randomly positioned outlier points and randomly perturbed input points.  Both of these approaches assume that the randomness is independent and identically distributed from point-to-point.  Such characteristics are not the dominant case when dealing with noise from mis-registration of scans, which is the most common source of noise from mobile scanning systems.

% Isosurface and the reign of SDFs
Another technique used to produce watertight models is to define implicit functions on the scanned volume, and use an isosurface of this volume as the exported mesh.  Once an implicit function is defined, techniques such as Marching Cubes or Dual Contours can be used to mesh the surface~\cite{MarchingCubes,DualContouring}. A popular method for defining this implicit function is to use Signed-Distance Fields~\cite{SignedDistanceFields}.  Another popular approach is Poisson Surface Reconstruction~\cite{Poisson}.  Both of these methods are more robust at mis-registration errors than the previous algorithms.  One downside of their use in the area of modeling man-made objects is that they yield smooth, continuous surfaces.  Such results tend to look overly organic when modeling objects with flat regions or sharp corners.  This issue can be alleviated somewhat by using dual contour isosurface meshing rather than marching cubes, since it preserves sharp edges in the scalar field, but with these methods, the scalar field itself can be overly-smoothed.

% typical hardware set-up for traditional surface reconstruction
An important note is that traditional surface reconstruction techniques, as described above, are written for and tested with common point-cloud test-sets, such as the Stanford Bunny or Dragon.  These models are scanned with precision 3D scanners that yield relatively low noise and minimal mis-registration when compared to scans of larger areas such as buildings.  These approaches are also designed to model isolated objects.  The goal of my thesis is to model buildings, which are scanned at a much larger scale and produce much higher rates of noise and mis-registration.  While watertight volumetric processing is still useful for our applications, different techniques must be applied compared to isolated objects.

\section{Outdoor Building Scanning}
\label{sec:outdoor_scanning}

% TODO discuss general outdoor technology for building scanning
Discuss general outdoor technology for building scanning.

% TODO briefly mention highlights of navteq project
Briefly mention highlights of navteq project.

% TODO use this project as smooth introduction to dealing with large pointclouds
Smooth introduction to dealing with large pointclouds.

\section{Indoor Building Scanning Systems}
\label{sec:indoor_scanning}

% building information models
In the architecture community, there is a continuing push for use of a consolidated Building Information Model file format~\cite{AutodeskBIM}.  Such models need to be semantically-rich for all phases of building development, including architectural, structural, mechanical, electrical, plumbing, etc.  Traditionally, each separate aspect needed to be remodeled from scratch, by hand, which introduced errors into the modeling process.

It is important to distinguish between models that are as-designed compared to as-built.  Each phase of building development will produce a design, but it is still important to compare this design to the as-is nature of the building.  3D scanning of building environments is an important technology to be able to compare the result of construction to what was desired.  With current technology, there is still a fair amount of manual intervention that is  required to convert a 3D scan of a building into a suitable BIM file that can be used for such a comparison.

% competing systems
Traditional industry-standard building scanning uses static scanners.  Such scanners are mounted on tripods, and moved from area to area in the building~\cite{RoomSegmentation,HistWallRecon,BasicPlaneFit}. This scanning process is labor intensive and slow, but results in highly accurate point clouds after stitching.  In order to automate indoor scanning, many mobile systems have been introduced.  Wheeled platforms that carry scanning equipment and are manually pushed through the environment are popular~\cite{Carving, ProbabilisticRobotics}.  Mobility of such systems is limited, since they are unable to traverse rough terrain or stairs easily.  Others have investigated mounting laser range finders on unmanned aerial vehicles~\cite{Quadrotor,QuadrotorMIT}.  Such platforms are agile in that they can scan difficult-to-reach areas.  Such unmanned platforms are limited by short battery life and cannot scan for long durations.

% include system hardware description
% our system and WHAT WE DO DIFFERENT AND WHY IT IS BETTER!!!!!!!!!
In this dissertation, we focus on ambulatory scanning platforms, where the sensor suite is carried by a human operator as the operator moves through the building environment~\cite{Sweep,MITBackpack,VillageHeritage}.  These systems allow for rapid data acquisition and can be actively scanning for several hours at a time.  They use 2D LiDAR scanners due to the cost and weight of full 3D laser range finders.  The captured scans are used both to reconstruct the geometry of the environment and to localize the system in the environment over time.  The datasets shown in this paper were generated by a backpack-mounted system that uses 2D LiDAR scanners to estimate the 3D path of the system over time as well as multiple scanners to generate geometry for the environment~\cite{liu2010indoor,Backpack,Localization,NickJournal}.  This system also has multiple cameras collecting imagery during the data acquisition process, which allows for scanned points to be colored or for generated meshes to be textured with realistic imagery~\cite{Cheng14}. 

% This section describes building meshing techniques
\section{3D Modeling of Building Environments}
\label{sec:building_meshing}

% give a brief overview of the different types of meshing techniques
% Discuss how the below techniques typically use noise-less input from
% static scanners, and do not explicitly detail with misregistration or
% noise.
The primary thesis of this dissertation regards surface reconstruction of scan data of indoor building environments, regardless of what hardware systems are used to collect building scan data.  There exists an emerging field of techniques used to model different aspects of building geometry from captured scans.  These techniques can be classified into three main categories:  Floor-Plan Generation, Simplified 3D Modeling, and Detailed 3D Modeling.  Floor-plan generation focuses on estimating 2D positions of walls in the building.  Simplified 3D modeling similarly focuses on 3D modeling only the permanent features of a building: floors, walls, and ceilings.  Detailed 3D modeling focuses on modeling all aspects of the scanned geometry, including fine details such as furniture or objects observed in the building.  Note that most of the approaches discussed here were developed to be applied to static scans of buildings, which have very low noise and capture high detail. The focus of this dissertation is to develop modeling techniques for mobile scanning systems, which are much more likely to suffer from mis-registration noise or missing geometry.

In this section, we discuss existing techniques to generate each of these types of building models.  This dissertation also describes novel work we contributed, in Chapters~\ref{ch:floorplan},~\ref{ch:carving},~\ref{ch:better_floorplan}, and~\ref{ch:better_carving}.

% This subsubsection describes floor plan generation techniques
\subsection{Floor-Plan Modeling}
\label{ssec:background_floorplan}

Floor-plan modeling techniques are based on the idea of sampling positional information of walls within the environment captured by the scans, then using these wall samples to generate a plan composed of line segments or polygons.  The work of Weiss et al use a cart-based system with a horizontal laser scanner~\cite{Weiss05}.  The output scan points are exactly the sample positions of the walls.  The find lines in this scan map with a Hough Transform, which represent walls.  Okorn et al employ a similar method, but their input scans represent a full 3D point-cloud~\cite{Okorn09}.  The wall sample positions are found by computing a top-down histogram of the input points, and areas of high density are classified as vertical surfaces.  The approaches we discuss in Chapter~\ref{ch:floorplan} employ a similar top-down histogram.  Lastly, Mura et al's paper takes a different approach to estimating wall positions~\cite{Mura13}.  They perform region growing in the 3D point-cloud to fine planar regions, which are projected into 2D and treated as potential wall candidates.  A cell complex is then built to volumetrically identify separate rooms in the model.  This approach is the second publication that performs automatic room partitioning, where the first is our approach as discussed in Chapter~\ref{sec:room_label}~\cite{Turner14}.  There are also methods that take a floor plan as input, and use this information to generate a 3D model of the environment by extruding the defined wall information~\cite{Or05,Lewis98}.  This type of modeling yields aesthetically pleasing results with well-defined floors, walls, and ceilings.

% This subsubsection describes plane fitting for 3D modeling
\subsection{Simplified 3D Modeling}
\label{ssec:background_planefit}

Since building features are almost entirely planar, a popular approach is to explicitly fit planar elements to the input point-clouds.  Sanchez and Zakhor use PCA plane-fitting to find floors, walls, and ceilings explicitly in the point-cloud~\cite{Victors}. Since this method was applied to ambulatory data, it resulted in missing components, holes, and double-surfacing due to mis-registration.  Xiong et al also perform plane-fitting in a similar fashion, but also analyze the computed planes for the locations of windows and doors, which are represented as holes in the surface~\cite{Xiong13}.  Adan et al find planes by first generating a floor-plan, then extrude the floor-plan into a 3D model~\cite{WallFinder}.  One limitation with these methods is that they are not necessarily watertight.  Other approaches have focused to perform volumetric processing to ensure watertightness when computing simplified 3D models.  Xiao et al find horizontal cross-sections of the building, forming a sequence of 2D CSG models that are then stacked together and simplified~\cite{Museums}.  While this approach does lead to aesthetically-pleasing models, it assumes manhattan-world models, which leads to topological errors if an insufficient number of cross-sections are recovered.  Oseau et al use a voxelization approach, with a follow-up graph-cut step to remove small details in the environment, leaving only floors, walls, and ceilings~\cite{Oesau13}.  This optimization step, however, can also cause significant deformations in the final geometry depending on the input parameters. 

% TODO include furukawa image-based floorplans

% This subsubsection describes dense 3D modeling
\subsection{Detailed 3D Modeling}
\label{ssec:background_3dmodeling}

One of the methods used to preserve the detail of furniture in a building scan is to explicitly search and classify for furniture models in the scan.  These techniques attempt to find locations in the input scans that match best with a stored database of known furniture.  The pre-existing model of the recognized piece of furniture is then oriented in the output model.  Nan et al employ this technique, with explicit classification of chairs and tables~\cite{SearchClassifyPointcloud}.  Kim et al also use this method, using a larger library of objects and operating on noisier scans~\cite{Kim12}.  They also discuss how search-classification can allow for change detection across scans of the same area taken at different times.  A major downside of this method is that the classification is only as good as the database.  Objects that are in unexpected orientations or are not in the database are misclassified.  For instance, a sideways chair is misclassified as a table.  One major benefit of this approach is the ability to model the objects independently of the room itself, as discussed later in Chapter~\ref{ch:better_carving}.  There are also methods that capture fine detail of buildings by attempting to be as accurate to the input point-cloud as possible.  Holenstein et al generate a space-carving model that voxelizes the scanned environment, labeling any voxels intersected by a scan-line line to be interior~\cite{Carving}.  The boundary of the interior voxels is meshed with Marching Cubes.  The advantage of this approach is an increased robustness to mis-registration errors, but the downside is that over-carving can result in the loss of fine detail.  Lastly, a popular approach to detailed modeling of environments is Kinect Fusion~\cite{KinectFusion,Kintinuous}.  This method allows for both meshing and tracking, but is limited in that it cannot handle large areas, and the actual meshing approach is a minor extension of Signed-Distance Fields~\cite{SignedDistanceFields}. 

\section{Thesis Organization}
\label{sec:organization}

% TODO organize remaining paper

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Hardware Description
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Hardware Description}
\label{ch:hardware}

The surface reconstruction algorithms described in this dissertation can be applied on any indoor scanning data.  For practicality purposes, we have developed our own scanning system, and all examples shown in this document are generated with our custom hardware (unless otherwise stated).

We developed a human-mounted ambulatory scanning system, worn as a backpack.  As the human operator walks through the building environment, at normal speed, this system collects data from a variety of sensors on-board.  These data products are logged and then processed offline.  Our developed software can accurately estimate the trajectory of the system over time and localize the system~\cite{NickJournal}.  This procedure allows us to rapidly move through a large environment, spending only a few seconds in each room yet capturing full geometry information.

\section{Mechanical Specification}
\label{sec:mechanical}

% this figure shows side-by-side backpacks
\begin{figure}

	\centering
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_1}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_2}}
		\centerline{(b)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.32\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{hardware/gen_3}}
		\centerline{(c)}\medskip
	\end{minipage}	

	\caption[The three backpack scanning systems developed by our lab.]{The three backpack scanning systems developed by our lab:  (a) First generation, weighing 80 pounds; (b) second generation, weighing 32.5 pounds, developed in 2013; (c) third generation, weighing 35 pounds, developed in 2014.}
	\label{fig:all_backpacks}

\end{figure}

% different backpack models
As shown in Figure~\ref{fig:all_backpacks}, our lab has developed three generations of the backpack hardware.  The first generation system, shown in Figure~\ref{fig:all_backpacks}a, was built as a prototype system~\cite{Backpack}.  It contains five Hokuyo laser scanners, two cameras, and an Internal Measurement Unit (IMU).  Since 2012, it has been augmented with WiFi antennas and infrared cameras.  The WiFi antennas are used to record signal-strength of nearby access points, which is useful for indoor localization techniques~\cite{Levchev14}.  The infrared cameras are useful for energy-efficiency analysis of building environments.  These data products are discussed further in Chapter~\ref{ch:applications}.

The second generation backpack, as shown in Figure~\ref{fig:all_backpacks}b, was constructed in 2013.  It contains all sensors available on the first backpack, except for the infrared cameras.  It also has a barometer and a set of 3D magnetometers.  The main modification of the second generation was to reduce the size.  The first generation is about 80 pounds, whereas the second generation is only 32.5 pounds.  The third generation, shown in Figure~\ref{fig:all_backpacks}c, backpack is almost identical to the second generation, but with the addition of infrared cameras.

\section{Sensor Characteristics}
\label{sec:sensor_specs}

% figure of backpack hardware
\begin{figure}[t]
	\centerline{\includegraphics[width=0.5\linewidth]{procarve/hardware/backpack_annotated}}
	\caption[Annotated scanning hardware system.]{Annotated scanning hardware system, worn as a backpack.  The operator walks through the indoor building area as the system scans the surroundings.  Our method combines the LiDAR and inertial measurements in order to form a virtual 3D model for the observed space.}
	\label{fig:backpack}
\end{figure}

% briefly describe the type of scans we use as input
Our geometry reconstruction algorithms use the retrieved laser range data as input.  These scans are taken with time-of-flight laser scanners that capture 3D geometry of the environment.  We use multiple 2D laser scanners mounted at different orientations to capture all observed geometry in the environment.  Since our system is mobile and ambulatory, the resulting point clouds have higher noise than traditional static scanners due to natural variability in human gait.  As such, our approach needs to be robust to motion induced scan-level noise and its associated artifacts.  We use these scans to form watertight models of the indoor building environment and the contained objects.

% sparse but fast
The primary advantage of a backpack system is speed of acquisition.  We use 2D Time-Of-Flight (TOF) laser scanners, which means we have sparser scan information per frame and traverse each area of the environment much faster than when scanning the equivalent area with a Kinect or Google Tango.  Since our system produces much sparser point clouds than Kinect-based scanning, it is not feasible to employ the same averaging techniques that allow for high-quality Kinect scans.  The advantage of these 2D TOF sensors is that they are less noisy, have a longer range, and a wider field of view.  The result is that much larger environments can be covered in significantly less time when using these sensors in a sweeping motion~\cite{Sweep}.  For instance, a backpack-mounted system allows an unskilled operator to rapidly walk through an environment to acquire data.

% the noise characteristics
Our system uses Hokuyo UTM-30LX sensors, whose intrinsic noise characterization is given in~\cite{Pomerleau12}.  Typically this noise contributes on the order of 1 to 2 centimeters to the standard deviation of the positional estimate of scan points.  This uncertainty value increases as the range of the point increases, with accurate measurements stopping at a range of $30$~meters.

% TODO LEFT OFF HERE

% also discuss extrinsic calibration of sensors, and show diagram of system common coordinate frame

% TODO (very) brief overview of data acquisition software

% TODO discuss time synchronization, both during collection and processing

% TODO scalability of system (size on disk, possible sizes for buildings

%---------------------------------------------------------------------------
%---------------------------------------------------------------------------
% CHAPTER:  Preprocessing and Conventions
%---------------------------------------------------------------------------
%---------------------------------------------------------------------------

\chapter{Preprocessing and Conventions}
\label{ch:preprocessing}

This chapter details processing steps that are required before any model generation can occur.  All approaches discussed in this dissertation require a reconstructed path of how the system moved through the environment.  Such a path trajectory is required in order to estimate the position of each scanner at any given time, so that the geometry readings can be placed in global coordinates.  The details of how the localization data are used are described in Sections~\ref{sec:localization} and~\ref{sec:align_path}.

Some methods discussed in this dissertation also require a consolidated 3D point cloud of the scanned environment.  Unlike most geometry products discussed, a point cloud contains no topology information.  Instead, a point cloud is a concatenation of the raw laser range data, transformed into the global coordinate frame.  Specifics on how these values are generated and what conventions are used are discussed in Section~\ref{sec:pointcloud}.

\section{Localization}
\label{sec:localization}

% Nick's localization procedure
Before models of the building geometry can be generated, we must first determine how the operator traversed the environment.  Indoor building environments are typically GPS-denied areas, so local observations are required in order to track the path walked~\cite{Backpack,Localization,NickJournal}.  The process of recovering the 3D trajectory of the system is especially difficult for our backpack hardware, since a human operator can experience six degrees of freedom in movement ($x$, $y$, $z$, $\theta$, $\phi$, $\psi$), whereas the sensors used in the system can only observe a 2D slice of the environment.  As such, multiple laser scanners are mounted at different orientations on the backpack hardware to ensure that full 3D movement can be observed~\cite{NickJournal}.  These incremental changes in pose are refined by a global graph optimization step that constrains the system path whenever an area is revisited~\cite{toro07}.  These revisits are automatically detected and constrained in order to produce an accurate 2D trajectory of the system.  The elevation of the system is computed using a similar technique with a vertically mounted scanner~\cite{Backpack}.

% coordinate systems used:  sensor, backpack, model
The localization procedure is complete, we have full mapping between three coordinate systems for each recorded frame for each hardware sensor.  The internal coordinate frame of the sensor, which is specific to the make and model of the sensor hardware, can be converted to the system common coordinate frame based on the extrinsic calibration of each sensor's rigid position and orientation on the backpack system.  This rigid affine transformation for a given sensor $s$ is represented by the $3 \times 3$ rotation matrix $R_{s\rightarrow c}$ and the $3 \times 1$ translation vector $T_{s\rightarrow c}$.  The localization procedure provides an output path of the system, so that each pose of the system can be transformed from the system common coordinate frame to the world coordinate frame.  At a given time $t$, the affine transformation from the system common coordinates to the model coordinates is given by $3 \times 3$ rotation matrix $R_{c\rightarrow m}(t)$ and $3 \times 1$ translation vector $T_{c\rightarrow m}(t)$.  With these transformations, any sensor reading can be placed in global coordinates.

% discuss output model frame of localization, and units
The localization output path provides the output model coordinate frame in a locally consistent coordinate frame based on the starting position of the operator during a scan.  This starting position is used as the origin point of the model.  By convention, the heading vector of the first pose is used as the $+X$-direction of the model coordinate frame.  The direction of gravity is used as the $-Z$-direction of the model coordinate frame.  The $Y$-direction of the model coordinate frame is chosen such that the coordinate system is right-handed.  Although this procedure produces a common set of coordinates for the model, we apply an additional step in order to align the model with the global coordinate frame, as discussed in Section~\ref{sec:align_path}.  Once this step is complete, the model uses East-North-Up (ENU) coordinates.  For the rest of this dissertation, all output models are expressed in units of meters.

% TODO show figure of backpack coordinate frame and model coordinate frame

% noise models of localization output (e.g. double-surfacing)
Since the localization procedure used is performed in an integrative fashion, the expected error in the reported position and orientation of the system at any pose is higher than in competing technologies, such as static scan systems~\cite{NickJournal}.  Static systems often utilize markers or control points within the environment to ensure manual alignment of scans, allowing for positional accuracy of within $1~mm$~\cite{Li97,Karimi00}.  By contrast, the mean positional accuracy of the output poses in the backpack localization procedure is typically $10~cm$.  Additionally, the errors in successive poses are highly correlated.  Typically a local set of scan points captured within a few seconds of one another are highly accurate, yet if the same features in the environment are scanned twice with a long interval between scans, then the scans are likely to be mis-matched.  This behavior leads to the presence of ``double-surfacing'' in the output scans, which can be observed in the generated point cloud of the environment.

% TODO show figure of double surfacing.

In order to provide consistent output geometry, the modeling techniques described in this dissertation are designed to prevent such ``double-surfacing'' from occurring in the output meshes.  This trait is accomplished by performing volumetric analysis of the input scans, as discussed later in this document. 

\section{Aligning Model Coordinates}
\label{sec:align_path}

As discussed in Section~\ref{sec:localization}, the system path generated by the localization procedure is initially oriented using the heading vector of the first pose.  In order to keep the orientation of model coordinate frames consistent between separate scans of the same environment, and to ensure that output geometry is easily recognizable, we perform an additional orientation step that aligns the path to the East-North-Up (ENU) coordinate frame.

One of the on-board sensors of the backpack hardware system is a 3D magnetometer, which is built into the Inertial Measurement Unit (IMU).  Individual readings from a 3D compass are typically noisy when indoors, due to nearby ferrous building elements that interfere with hard and soft iron calibration~\cite{Caruso00,Guo08}.  As such, these readings are not reliable during the localization process.  Once the final, consistent path is generated we are able to generate a least-squares estimate of compass north and reorient the path so that North is aligned with the $+Y$-direction.

First, the 3D magnetometer reading is recorded at each pose during the data acquisition.  These vector readings represent the magnetic field at that location.  The direction of each reading is used as a noisy estimate of the direction of magnetic South.  Let $\vec{m}_i \in \mathbb{R}^3$ be the magnetometer reading at pose $i$, in the coordinate frame of the magnetometer sensor $\texttt{mag}$.  The rotation required to orient the model coordinate frame to ENU coordinates is given by:

\begin{equation}
	\label{eq:align_path}
	\texttt{argmin}_{\theta \in [0,2\pi]} \; \sum \limits_{i} 
		\; \left| \left( R_{z}(\theta) \, R_{c\rightarrow m} (t_i) \,
			R_{\texttt{mag} \rightarrow c} \, m_i \right) - \left[ 
				\begin{array}{c} 0 \\ -1 \\ 0 \end{array} 
					\right] \right|^2
\end{equation}

where $t_i$ is the timestamp of pose $i$, and $R_{z}(\theta)$ is the rotation matrix:

\begin{equation}
	\label{eq:rotation_matrix}
	R_{z}(\theta) = \left[ \begin{array}{ccc}
				\texttt{cos}(\theta) & -\texttt{sin}(\theta) & 0 \\
				\texttt{sin}(\theta) & \texttt{cos}(\theta) & 0 \\
				0 & 0 & 1 \end{array} \right]
\end{equation}

This optimization rigidly rotates the model coordinate frame about the $z$-axis in order to best align all magnetometer readings with the direction we wish to denote south:  the $-Y$-direction.  This procedure ensures that all output models are aligned with magnetic North, in ENU coordinates.  While not applied in the current configuration, it is possible to also align the model to true North if given the latitude and longitude coordinates of the scanning location, by performing a look-up of magnetic declination~\cite{MagDec}.

% TODO show figure of align_path rotation

\section{Point Cloud Generation}
\label{sec:pointcloud}

When a finalized path and coordinate frame is produced, we can immediately generate a point cloud of the raw scans taken in the environment.  This point cloud allows visualization of observed building geometry.  Each point can be computed independently by transforming its position from its sensor frame to the model coordinate frame.  An example point cloud of an indoor environment is shown in Figure~\ref{fig:pointcloud_nocolor}.  This scene shows a corner of a room, with a lamp to the left of a TV on a table.

% show example pointcloud
\begin{figure}
	\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_nocolor}}
	\caption{An example point cloud of an indoor building environment.}
	\label{fig:pointcloud_nocolor}
\end{figure}

We also have the capability to export point clouds in color.  The coloration is taken from camera imagery taken temporally close to when the range points were acquired.  For a given point $\vec{x}$ taken at time $t$, the point color is determined by querying all available cameras for any images within $\Delta t$ seconds of time $t$.  Typically, we use a window size of $\Delta t = 2$~seconds.  For each reported camera, we project the position $\vec{x}$ onto the camera image plane, and determine the pixel color at that position.  This projection is demonstrated in Figure~\ref{fig:pointcloud_color_diagram}.  In addition to color, we also compute a quality measure $q_x = (\vec{x} - \vec{c})^T \vec{n}$, where $\vec{c}$ is the camera position and $\vec{n}$ is the optical axis of the camera.  The quality $q_x$ is $1.0$ if $\vec{x}$ projects into the center of the image, and decreases as the projected location moves away from the center of image.  For all images within the time window $[t - \Delta t, t + \Delta t]$, the pixel color associated with the highest quality meaure is used to color the point.

The resulting point cloud is colored based on all available imagery in the dataset.  Figure~\ref{fig:pointcloud_color}a shows a camera image of an example seen and Figure~\ref{fig:pointcloud_color}b shows the colored point cloud of the same environment.

% figure showing projection of points onto images
\begin{figure}
	\centerline{\includegraphics[width=0.5\linewidth]{pointcloud/gradlounge/pointcloud_color_diagram}}
	\caption{Determining the color of point $x$ based on camera imagery.}
	\label{fig:pointcloud_color_diagram}
\end{figure}

% show color pointcloud compared to camera image
\begin{figure}
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/camera}}
		\centerline{(a)}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{pointcloud/gradlounge/pointcloud_color}}
		\centerline{(b)}
	\end{minipage}

	\caption{An example point cloud colored by nearby camera imagery.}
	\label{fig:pointcloud_color}
\end{figure}

% partitioning pointclouds into separate levels
\section{Partitioning Point Cloud by Building Levels}
\label{sec:pointcloud_level_split}

Many methods described in this dissertation require explicit detection and partitioning of the different levels, or stories, in a scanned building.  Since our hardware system is human-mounted, the operator can walk up and down stairs during the data acquistion process, facilitating scans of multiple stories at once.  

Some localization systems that rely on 2D grid maps of wall samples are capable of detecting when the operator moves from one level to another and automatically partition their output grid maps accordingly~\cite{MITBackpack}.  We do not just want the elevation of each level, but the vertical extent as well.  In order to detect and separate building levels, we identify the primary floor and ceiling surfaces for each level.  This identification can be done either in the point cloud domain or after a full model has been generated.  The latter method is discussed in Chapter~\ref{ch:better_floorplans}.  Here, we discuss how level splitting can be accomplished via point clouds, which offers a faster if less accurate partitioning.

A histogram approach can be used to separate the point cloud by levels~\cite{Turner12,Turner14Journal}. Figure~\ref{fig:heighthist}a shows an example point cloud, colored by height, which contains multiple levels.  By computing a histogram along the vertical-axis of the point-cloud, it is possible to find heights with high point density, which indicates the presence of a large horizontal surface.  An example of this process is shown in Figure~\ref{fig:heighthist}b, where peaks in the histogram correspond to the floors and ceilings of each scanned level in the building.  Points scanned from above, in a downward direction, are used to populate a histogram to estimate the position of each floor, and the histogram used to estimate the position of each ceiling is populated by points scanned from below.  The local maxima of these two histograms show locations of likely candidates for floor and ceiling positions, which are used to estimate the number of scanned levels and the vertical extent of each level.  The candidate floor and ceiling heights are pruned by first taking the lowest floor maxima as the elevation of the first level's floor.  The first level's ceiling elevation is determined by the most populated ceiling maxima position that resides below the next maximal floor elevation.  This process is repeated for all levels, which allows for detection of both number of levels and their range extents.  Once levels are separated, they can be processed and analyzed separately.  Figure~\ref{fig:heighthist}c shows the final extruded mesh with all three scanned levels.

% show example histogram and partitioning
\begin{figure}

	\centerline{\begin{minipage}[c]{0.5\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/points_LEFT}}
		\centerline{(a)}\medskip
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.35\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/hist}}
		\centerline{(b)}\medskip
	\end{minipage}}
	\begin{minipage}[c]{0.9\linewidth}
		\centerline{\includegraphics[width=1.0\linewidth]{jstsp2014/floorplan/cory3story/mesh}}
		\centerline{(c)}
	\end{minipage}
	
	\caption[An example point-cloud partitioning by height.]{An example point-cloud partitioning by height: (a) the input point-cloud, showing geometry for three levels; (b) the vertical histogram showing estimates of each building level height; (c) the produced mesh of this building scan using method from Chapter~\ref{ch:floorplan}.}
	\label{fig:heighthist}

\end{figure}



% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
\bibliography{elturner}
\vfill

% that's all folks
\end{document}
